{
  "hash": "877b3abff28eb8c9dd5309ef8e9b6a3e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Pr√©-traitements\"\n---\n\n\n\n\n## Introduction\n\nLe document de travail contient deux types d'√©l√©ments : du texte pour expliquer et pr√©senter ce que l'on fait et du code pour r√©aliser les manipulations de donn√©es, les analyses et les graphiques. On commence toujours un document avec un bloc de code de setup, pour lister et charger les packages que l'on va utiliser et les options g√©n√©rales pour l'√©dition du document.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(quanteda)\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(RColorBrewer)\n```\n:::\n\n\n\nDans un premier temps, nous allons tout simplement charger la base de donn√©es de travail puis la d√©crire. Ensuite, nous cr√©erons un corpus, le visualisons. Puis nous effectueront quelques analyses liminaires, avant de voir les pr√©-traitements √† r√©aliser sur le corpus.\n\n## Les donn√©es\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#On charge les donn√©es, stock√©es dans un fichier csv\n\ndata <- read_csv(\"data/data_trustpilot_oiseaux.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 4388 Columns: 7\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (4): auteur, date, month, comments\ndbl (3): id, year, note\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nnames(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"id\"       \"auteur\"   \"date\"     \"month\"    \"year\"     \"note\"     \"comments\"\n```\n\n\n:::\n\n```{.r .cell-code}\nview(data)\ndata\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4,388 x 7\n      id auteur                   date            month    year  note comments  \n   <dbl> <chr>                    <chr>           <chr>   <dbl> <dbl> <chr>     \n 1     1 MAQUET Cyril             07 ao√ªt 2023    ao√ªt     2023     5 \"Comme to~\n 2     2 Mme Laurence  Wolff      08 ao√ªt 2023    ao√ªt     2023     5 \"Le d√©lai~\n 3     3 Une nouvelle cliente     07 ao√ªt 2023    ao√ªt     2023     5 \"Produits~\n 4     4 Patricia ALLAMAN         15 ao√ªt 2023    ao√ªt     2023     5 \"Envoi ra~\n 5     5 VPL                      24 juillet 2023 juillet  2023     5 \"Exp√©diti~\n 6     6 PHILIPPE GODIN           08 ao√ªt 2023    ao√ªt     2023     5 \"site s√©r~\n 7     7 Mme MARIA ADILIA PEREIRA 15 ao√ªt 2023    ao√ªt     2023     1 \"deux sem~\n 8     8 Rachel Mattyssen         31 juillet 2023 juillet  2023     5 \"Tr√®s bie~\n 9     9 Estelle Fay              16 juillet 2023 juillet  2023     5 \"Enfin un~\n10    10 Mme T.                   06 ao√ªt 2023    ao√ªt     2023     4 \"Satisfai~\n# i 4,378 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\n#R√©sum√© des donn√©es\nsummary(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       id          auteur              date              month          \n Min.   :   1   Length:4388        Length:4388        Length:4388       \n 1st Qu.:1098   Class :character   Class :character   Class :character  \n Median :2194   Mode  :character   Mode  :character   Mode  :character  \n Mean   :2194                                                           \n 3rd Qu.:3291                                                           \n Max.   :4388                                                           \n      year           note         comments        \n Min.   :2013   Min.   :1.000   Length:4388       \n 1st Qu.:2018   1st Qu.:5.000   Class :character  \n Median :2020   Median :5.000   Mode  :character  \n Mean   :2019   Mean   :4.641                     \n 3rd Qu.:2021   3rd Qu.:5.000                     \n Max.   :2023   Max.   :5.000                     \n```\n\n\n:::\n:::\n\n\n\n## Premi√®res analyses des donn√©es\n\nAvant de s'int√©resser au contenu des commentaires, explorons la structure des donn√©es. On va regarder la distribution des commentaires et des notes dans le temps, et s'int√©resser √† la longueur des avis clients.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Les ann√©es \ndata$year<-as.factor(data$year)\nsummary(data$year)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 \n 125  209  249  171  324  341  375  735  853  623  383 \n```\n\n\n:::\n\n```{.r .cell-code}\ndata%>%\n  group_by(year)%>%\n  summarise(n=n(), prop=n/nrow(data))%>%\n  ggplot(aes(year,prop))+\n  geom_col(fill=\"green\",show.legend = TRUE)+\n  scale_y_continuous(labels = scales::percent)+\n  theme_light()+\n  labs(title = \"R√©partition des avis dans le temps\", subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\", x=\"ann√©es\", y=NULL)\n```\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/data_explor-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n#Les notes\nsummary(data$note)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   5.000   5.000   4.641   5.000   5.000 \n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(as.factor(data$note))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   1    2    3    4    5 \n 116   62  169  589 3452 \n```\n\n\n:::\n\n```{.r .cell-code}\ndata%>%\n  group_by(note)%>%\n  summarise(n=n(), prop=n/nrow(data))%>%\n  ggplot(aes(note,prop))+\n  geom_col(fill=c(\"red\",\"pink\",\"orange\",\"gold\",\"lightgreen\"))+\n  annotate(\"text\", x=2, y=0.7, label=paste(\"Note moyenne = \",round(mean(data$note),1)))+\n  scale_y_continuous(labels=scales::percent)+\n  theme_light()+\n  labs(title = \"R√©partition des avis en fonction des notes\", subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\", x=\"note\", y=NULL)\n```\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/data_explor-2.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n#Le nombre de caract√®re\ndata$nb_caractere<-nchar(data$comments) #on compte le nombre de caract√®re de chaque commentaire\nsummary(data$nb_caractere)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   12.0    41.0    74.0   104.3   130.0  1571.0 \n```\n\n\n:::\n\n```{.r .cell-code}\nggplot(data, aes(nb_caractere))+\n  geom_boxplot()+\n  scale_y_continuous(NULL, breaks = NULL)+\n  labs(x=NULL,title = \"Nombre de caract√®res des avis\", subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\")+\n  theme_light()+\n  coord_flip()\n```\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/data_explor-3.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n#Le nombre de tokens\n\ndata$nb_token<-ntoken(data$comments) #on compte le nombre de caract√®re de chaque commentaire\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: ntoken.character()/ntype.corpus() was deprecated in quanteda 4.0.0.\ni Please use ntoken(tokens(x)) instead.\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(data$nb_token)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    2.0     7.0    13.0    19.3    24.0   308.0 \n```\n\n\n:::\n\n```{.r .cell-code}\nggplot(data, aes(nb_token))+\n  geom_boxplot()+\n  scale_y_continuous(NULL, breaks = NULL)+\n  labs(x=NULL,title = \"Nombre de tokens des avis\", subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\")+\n  theme_light()+\n  coord_flip()\n```\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/data_explor-4.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n#On va filtrer au-dessus de 100 tokens\ndata_100t<-data%>%filter(nb_token<50)\n  \nggplot(data_100t, aes(nb_token))+\n  geom_boxplot()+\n  scale_y_continuous(NULL, breaks = NULL)+\n  labs(x=NULL,title = \"Nombre de tokens des avis\", subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\")+\n  theme_light()+\n  coord_flip()\n```\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/data_explor-5.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n#Les notes dans le temps\ndata%>%\n  mutate(note=as.factor(note))%>%\n  group_by(year, note)%>%\n  summarise(n=n() ,prop=n/nrow(data))%>%\n  ggplot(aes(year, prop))+\n  geom_col(aes(fill=note), show.legend = FALSE)+\n  scale_fill_discrete(type=c(\"red\",\"pink\",\"orange\",\"gold\",\"lightgreen\"))+\n  scale_y_continuous(labels=scales::percent)+\n  theme_light()+\n  labs(title = \"R√©partition des avis dans le temps\", subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\", x=\"ann√©es\", y=NULL)\n```\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/data_explor-6.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\ndata%>%\n  mutate(note=as.factor(note))%>%\n  group_by(year, note)%>%\n  summarise(n=n())%>%\n  ggplot(aes(x=year, y=n, group=note))+\n  geom_bar(position=\"fill\",stat=\"identity\", aes(fill=note))+\n  scale_fill_discrete(type=c(\"red\",\"pink\",\"orange\",\"gold\",\"lightgreen\"))+\n  scale_y_continuous(labels=scales::percent)+\n  theme_minimal()+\n  labs(title = \"Comparaison de la r√©partition des notes dans le temps\", subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\", x=\"ann√©es\", y=NULL)\n```\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/data_explor-7.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n# Cr√©ation du corpus et premi√®res observations du corpus\n\nTout d'abord, nous transformons le jeu de donn√©es en corpus. La variable qui contient le texte est \"comments\", les autres variables vont devenir des m√©tadonn√©es du corpus, c'est-√†-dire des variables associ√©es √† chaque texte. Cela sera utile par le suite pour faire des analyses comparatives entre les textes suivant diff√©rentes variables (le temps en particulier, mais pas seulement).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Cr√©ation du corpus\ncorpus_oiseaux<-corpus(data, text_field = \"comments\")\ncorpus_oiseaux\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCorpus consisting of 4,388 documents and 8 docvars.\ntext1 :\n\"Comme toujours, super service. Ne changez rien!(sauf peut √™t...\"\n\ntext2 :\n\"Le d√©lai de ma commande super rapide Le d√©lais des ma comman...\"\n\ntext3 :\n\"Produits de qualit√© et √©quipe professionnelle ‚Ä¶ Tr√®s bon acc...\"\n\ntext4 :\n\"Envoi rapide, bien emball√© et conforme √† l'annonce\"\n\ntext5 :\n\"Exp√©dition internationale ! J'ai r√©cemment d√©m√©nag√© en Espag...\"\n\ntext6 :\n\"site s√©rieux bon produit livraisons plus que correct mais pe...\"\n\n[ reached max_ndoc ... 4,382 more documents ]\n```\n\n\n:::\n\n```{.r .cell-code}\ncorpus_oiseaux[\"text600\"] #pour visualiser un texte pr√©cis\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCorpus consisting of 1 document and 8 docvars.\ntext600 :\n\"Commande re√ßu dans les temps pas de surprise. Tr√®s bon site\"\n```\n\n\n:::\n\n```{.r .cell-code}\na<-corpus_oiseaux[\"text30\"]\nrm(a)\n```\n:::\n\n\n\nEnsuite, nous allons extraire de chaque texte les termes qui les composent. Ces termes sont nomm√©s \"token\" (jeton), et comme vous pouvez le voir, ce ne sont pas uniquement des mots, mais tout caract√®re ou suite de caract√®res s√©par√©s des autres par un espace.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Extraction des tokens\ntok<-tokens(corpus_oiseaux)\ntok[\"text600\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTokens consisting of 1 document and 8 docvars.\ntext600 :\n [1] \"Commande\" \"re√ßu\"     \"dans\"     \"les\"      \"temps\"    \"pas\"     \n [7] \"de\"       \"surprise\" \".\"        \"Tr√®s\"     \"bon\"      \"site\"    \n```\n\n\n:::\n:::\n\n\n\nChaque texte est maintenant d√©compos√© en une suite de tokens. Pour voir les termes les plus fr√©quents dans le corpus, ainsi que leur co-occurrences (apparition de deux termes en m√™me temps), il convient de transformer l'objet tok en une matrice termes-documents. En ligne, tous les tokens identifi√©s, en ligne, tous les textes du corpus, et les valeurs correspondent au nombre d'occurrences (d'apparitions) de chaque token dans chaque document. Une particularit√© de cette matrice est qu'elle contient √©norm√©ment de z√©ro.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Transformation en document-term frequency matrix\ndfm<-dfm(tok)\ndfm\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDocument-feature matrix of: 4,388 documents, 5,658 features (99.72% sparse) and 8 docvars.\n       features\ndocs    comme toujours , super service . ne changez rien !\n  text1     1        1 1     1       1 4  1       1    1 1\n  text2     0        1 1     2       0 3  0       0    0 0\n  text3     0        0 0     0       0 2  0       0    0 0\n  text4     0        0 1     0       0 0  0       0    0 0\n  text5     0        0 4     0       0 2  1       0    0 4\n  text6     0        0 0     0       0 0  0       0    0 0\n[ reached max_ndoc ... 4,382 more documents, reached max_nfeat ... 5,648 more features ]\n```\n\n\n:::\n:::\n\n\n\nEnfin, nous pouvons avoir un aper√ßu des termes les plus fr√©quents. Nous les visualisons d'abord sous forme de tableau (les 20 tokens les plus fr√©quents), puis sous la forme d'un nuage de mots, o√π la taille des mots correspond √† leur fr√©quence dans le corpus.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Visualisation des termes les plus fr√©quents\ntextstat_frequency(dfm, n=20) #les 20 premiers termes les plus fr√©quents\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     feature frequency rank docfreq group\n1          .      4777    1    2094   all\n2          ,      2788    2    1576   all\n3         de      2685    3    1646   all\n4         et      2530    4    1915   all\n5       tr√®s      1996    5    1529   all\n6     rapide      1659    6    1569   all\n7         je      1622    7    1221   all\n8  livraison      1412    8    1299   all\n9          !      1349    9     649   all\n10         √†      1312   10    1022   all\n11  commande      1238   11    1030   all\n12        le      1215   12     812   all\n13        la      1215   12     868   all\n14      pour      1049   14     782   all\n15       les      1013   15     754   all\n16      bien       920   16     803   all\n17        en       801   17     618   all\n18      site       793   18     686   all\n19  produits       738   19     661   all\n20        un       729   20     565   all\n```\n\n\n:::\n\n```{.r .cell-code}\ntextplot_wordcloud(dfm) #nuage de mots\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in text.default(x1, y1, word[i], cex = (1 + adjust) * size[i], offset =\n0, : font metrics unknown for Unicode character U+1F44D\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/0_viz-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nPour conclure sur cette premi√®re approche du corpus, nous voyons que nos analyses sont g·∫øn√©es par la pr√©sence de la ponctuation et de plein de petits mots \"vides de sens\" (les articles par exemple). C'est pourquoi nous allons nettoyer le corpus pour avoir une meilleure vision de ce qu'il contient.\n\n# Nettoyage du corpus\n\nLe nettoyage du corpus pour les analyses se fait lors de la transformation en tokens. Nous allons ajouter des options pour supprimer la ponctuation, les chiffres et les stopwords (les mots qui n'apportent pas de sens s√©mantique mais permettent l'articulation du discours).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstopwords(\"fr\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  [1] \"au\"       \"aux\"      \"avec\"     \"ce\"       \"ces\"      \"dans\"    \n  [7] \"de\"       \"des\"      \"du\"       \"elle\"     \"en\"       \"et\"      \n [13] \"eux\"      \"il\"       \"je\"       \"la\"       \"le\"       \"leur\"    \n [19] \"lui\"      \"ma\"       \"mais\"     \"me\"       \"m√™me\"     \"mes\"     \n [25] \"moi\"      \"mon\"      \"ne\"       \"nos\"      \"notre\"    \"nous\"    \n [31] \"on\"       \"ou\"       \"par\"      \"pas\"      \"pour\"     \"qu\"      \n [37] \"que\"      \"qui\"      \"sa\"       \"se\"       \"ses\"      \"son\"     \n [43] \"sur\"      \"ta\"       \"te\"       \"tes\"      \"toi\"      \"ton\"     \n [49] \"tu\"       \"un\"       \"une\"      \"vos\"      \"votre\"    \"vous\"    \n [55] \"c\"        \"d\"        \"j\"        \"l\"        \"√†\"        \"m\"       \n [61] \"n\"        \"s\"        \"t\"        \"y\"        \"√©t√©\"      \"√©t√©e\"    \n [67] \"√©t√©es\"    \"√©t√©s\"     \"√©tant\"    \"suis\"     \"es\"       \"est\"     \n [73] \"sommes\"   \"√™tes\"     \"sont\"     \"serai\"    \"seras\"    \"sera\"    \n [79] \"serons\"   \"serez\"    \"seront\"   \"serais\"   \"serait\"   \"serions\" \n [85] \"seriez\"   \"seraient\" \"√©tais\"    \"√©tait\"    \"√©tions\"   \"√©tiez\"   \n [91] \"√©taient\"  \"fus\"      \"fut\"      \"f√ªmes\"    \"f√ªtes\"    \"furent\"  \n [97] \"sois\"     \"soit\"     \"soyons\"   \"soyez\"    \"soient\"   \"fusse\"   \n[103] \"fusses\"   \"f√ªt\"      \"fussions\" \"fussiez\"  \"fussent\"  \"ayant\"   \n[109] \"eu\"       \"eue\"      \"eues\"     \"eus\"      \"ai\"       \"as\"      \n[115] \"avons\"    \"avez\"     \"ont\"      \"aurai\"    \"auras\"    \"aura\"    \n[121] \"aurons\"   \"aurez\"    \"auront\"   \"aurais\"   \"aurait\"   \"aurions\" \n[127] \"auriez\"   \"auraient\" \"avais\"    \"avait\"    \"avions\"   \"aviez\"   \n[133] \"avaient\"  \"eut\"      \"e√ªmes\"    \"e√ªtes\"    \"eurent\"   \"aie\"     \n[139] \"aies\"     \"ait\"      \"ayons\"    \"ayez\"     \"aient\"    \"eusse\"   \n[145] \"eusses\"   \"e√ªt\"      \"eussions\" \"eussiez\"  \"eussent\"  \"ceci\"    \n[151] \"cela\"     \"cel√†\"     \"cet\"      \"cette\"    \"ici\"      \"ils\"     \n[157] \"les\"      \"leurs\"    \"quel\"     \"quels\"    \"quelle\"   \"quelles\" \n[163] \"sans\"     \"soi\"     \n```\n\n\n:::\n\n```{.r .cell-code}\ntok<-tokens(corpus_oiseaux, remove_punct = TRUE, remove_numbers = TRUE)%>%\n  tokens_remove(stopwords(\"fr\"))\ncorpus_oiseaux[\"text600\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCorpus consisting of 1 document and 8 docvars.\ntext600 :\n\"Commande re√ßu dans les temps pas de surprise. Tr√®s bon site\"\n```\n\n\n:::\n\n```{.r .cell-code}\ntok[\"text600\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTokens consisting of 1 document and 8 docvars.\ntext600 :\n[1] \"Commande\" \"re√ßu\"     \"temps\"    \"surprise\" \"Tr√®s\"     \"bon\"      \"site\"    \n```\n\n\n:::\n:::\n\n\n\nEnsuite, on transforme en dfm et on visualise ce que √ßa donne.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndfm<-dfm(tok)\ndfm\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDocument-feature matrix of: 4,388 documents, 5,430 features (99.81% sparse) and 8 docvars.\n       features\ndocs    comme toujours super service changez rien sauf peut √™tre l√†\n  text1     1        1     1       1       1    1    1    1    1  1\n  text2     0        1     2       0       0    0    0    0    0  1\n  text3     0        0     0       0       0    0    0    0    0  0\n  text4     0        0     0       0       0    0    0    0    0  0\n  text5     0        0     0       0       0    0    0    0    0  0\n  text6     0        0     0       0       0    0    0    0    0  0\n[ reached max_ndoc ... 4,382 more documents, reached max_nfeat ... 5,420 more features ]\n```\n\n\n:::\n\n```{.r .cell-code}\ntextstat_frequency(dfm, n=20)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      feature frequency rank docfreq group\n1        tr√®s      1996    1    1529   all\n2      rapide      1659    2    1569   all\n3   livraison      1412    3    1299   all\n4    commande      1238    4    1030   all\n5        bien       920    5     803   all\n6        site       793    6     686   all\n7    produits       738    7     661   all\n8           a       706    8     566   all\n9     produit       676    9     609   all\n10        bon       639   10     575   all\n11      merci       553   11     529   all\n12 recommande       553   11     534   all\n13    parfait       522   13     474   all\n14    qualit√©       487   14     447   all\n15       j'ai       382   15     289   all\n16    oiseaux       379   16     319   all\n17       tout       375   17     347   all\n18       re√ßu       362   18     332   all\n19      colis       360   19     313   all\n20 rapidement       356   20     343   all\n```\n\n\n:::\n\n```{.r .cell-code}\ntextplot_wordcloud(dfm)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in text.default(x1, y1, word[i], cex = (1 + adjust) * size[i], offset =\n0, : font metrics unknown for Unicode character U+1F44D\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/1_dfm-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\ng<-textstat_frequency(dfm,n=20)\n\n\nggplot(g, aes(x = feature, y=frequency))+\n  geom_col()+\n  coord_flip()\n```\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/1_dfm-2.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nggplot(g, aes(x = reorder(feature, frequency), y=frequency, fill=frequency))+\n  geom_col(show.legend = TRUE)+\n  coord_flip()+\n  theme_light()+\n  scale_fill_distiller(palette = \"Blues\", direction = 1)+\n  labs(title=\"Les mots les plus fr√©quents\",subtitle = \"du corpus Oiseaux Mania\",caption = \"Source : Data TrustPilot\")+\n  xlab(NULL)+\n  ylab(\"Fr√©quence\")\n```\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/1_dfm-3.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\ndisplay.brewer.all()\n```\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/1_dfm-4.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nGlobalement, la commande et la livraison sont TR√àS rapides et les produits sont bons. La surrepr√©sentation de ces termes dans le corpus nous emp√™che de voir les th√©matiques abord√©es de mani√®re moins √©videntes. Nous avons plusieurs solutions qui s'offrent √† nous : filtrer les mots trop fr√©quents du corpus ou nous int√©resser √† une autre mesure de la fr√©quence d'apparition. Nous allons d'abord filtrer le corpus.\n\nOn peut aussi vouloir remplacer des termes par d'autres, comme ici \"produits\" par \"produit\".\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntok<-tokens_replace(tok, \"produits\", \"produit\")\ndfm<-dfm(tok)\ntextstat_frequency(dfm, n=20)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      feature frequency rank docfreq group\n1        tr√®s      1996    1    1529   all\n2      rapide      1659    2    1569   all\n3     produit      1414    3    1243   all\n4   livraison      1412    4    1299   all\n5    commande      1238    5    1030   all\n6        bien       920    6     803   all\n7        site       793    7     686   all\n8           a       706    8     566   all\n9         bon       639    9     575   all\n10      merci       553   10     529   all\n11 recommande       553   10     534   all\n12    parfait       522   12     474   all\n13    qualit√©       487   13     447   all\n14       j'ai       382   14     289   all\n15    oiseaux       379   15     319   all\n16       tout       375   16     347   all\n17       re√ßu       362   17     332   all\n18      colis       360   18     313   all\n19 rapidement       356   19     343   all\n20    service       351   20     325   all\n```\n\n\n:::\n:::\n\n\n\n# Filtrer le corpus des termes trop fr√©quents\n\nNous allons filtrer les mots qui sont pr√©sents plus de 500 fois dans le corpus.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndfm_trim<-dfm_trim(dfm, max_termfreq = 500)\n\ntextstat_frequency(dfm_trim, n=20)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      feature frequency rank docfreq group\n1     qualit√©       487    1     447   all\n2        j'ai       382    2     289   all\n3     oiseaux       379    3     319   all\n4        tout       375    4     347   all\n5        re√ßu       362    5     332   all\n6       colis       360    6     313   all\n7  rapidement       356    7     343   all\n8     service       351    8     325   all\n9        prix       327    9     307   all\n10 satisfaite       327    9     301   all\n11      super       326   11     294   all\n12   conforme       326   11     307   all\n13       rien       317   13     299   all\n14      bonne       293   14     277   all\n15       plus       292   15     254   all\n16       tres       278   16     216   all\n17   toujours       274   17     236   all\n18      mania       256   18     228   all\n19      envoi       239   19     231   all\n20    s√©rieux       232   20     220   all\n```\n\n\n:::\n\n```{.r .cell-code}\ntextplot_wordcloud(dfm_trim, max_words = 100, color = rev(brewer.pal(10, \"Set2\")))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in brewer.pal(10, \"Set2\"): n too large, allowed maximum for palette Set2 is 8\nReturning the palette you asked for with that many colors\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/1_filter-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\ndisplay.brewer.all()\n```\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/1_filter-2.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nUne autre mani√®re de s'y prendre est d'√©liminer directement les termes que l'on ne veut pas voir appara√Ætre.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntextstat_frequency(dfm,n=20)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      feature frequency rank docfreq group\n1        tr√®s      1996    1    1529   all\n2      rapide      1659    2    1569   all\n3     produit      1414    3    1243   all\n4   livraison      1412    4    1299   all\n5    commande      1238    5    1030   all\n6        bien       920    6     803   all\n7        site       793    7     686   all\n8           a       706    8     566   all\n9         bon       639    9     575   all\n10      merci       553   10     529   all\n11 recommande       553   10     534   all\n12    parfait       522   12     474   all\n13    qualit√©       487   13     447   all\n14       j'ai       382   14     289   all\n15    oiseaux       379   15     319   all\n16       tout       375   16     347   all\n17       re√ßu       362   17     332   all\n18      colis       360   18     313   all\n19 rapidement       356   19     343   all\n20    service       351   20     325   all\n```\n\n\n:::\n\n```{.r .cell-code}\nrem<-c(\"tr√®s\",\"rapide\",\"produit\",\"livraison\", \"commande\", \"bien\", \"site\", \"a\", \"bon\", \"merci\", \"recommande\",\"parfait\", \"j'ai\",\"tres\")\n\ndfm_rem<-dfm_remove(dfm, rem)\ntextstat_frequency(dfm_rem, n=20)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      feature frequency rank docfreq group\n1     qualit√©       487    1     447   all\n2     oiseaux       379    2     319   all\n3        tout       375    3     347   all\n4        re√ßu       362    4     332   all\n5       colis       360    5     313   all\n6  rapidement       356    6     343   all\n7     service       351    7     325   all\n8        prix       327    8     307   all\n9  satisfaite       327    8     301   all\n10      super       326   10     294   all\n11   conforme       326   10     307   all\n12       rien       317   12     299   all\n13      bonne       293   13     277   all\n14       plus       292   14     254   all\n15   toujours       274   15     236   all\n16      mania       256   16     228   all\n17      envoi       239   17     231   all\n18    s√©rieux       232   18     220   all\n19      choix       222   19     205   all\n20  satisfait       220   20     205   all\n```\n\n\n:::\n:::\n\n\n\n# La loi de Zipf\n\nV√©rifions la proposition de la loi de Zipf, selon laquelle la fr√©quence d'apparition d'un terme est inversement proportionnel √† son rang.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nzipf<-textstat_frequency(dfm)\nggplot(zipf, aes(rank, frequency))+\n  geom_line(color=\"blue\")+\n  geom_point(color=\"darkgreen\")+\n   scale_x_log10() +\n   scale_y_log10()+\n  theme_light()+\n  labs(title = \"Observation de la loi de Zipf\",x=\"log (rang)\",y=\"log (fr√©quence)\")\n```\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/zipf_law-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n# Mesures de fr√©quence\n\nOn s'est pour l'instant int√©ress√© uniquement aux termes les plus fr√©quents dans un corpus. On a vu comment √©liminer les termes trop fr√©quents pour qu'ils nous apportent de l'information. Pour l'analyse de topics, il nous faut prendre un autre angle d'attaque : afin de d√©tecter les sujets abord√©s dans un corpus, on ne peut se contenter d'observer les mots les plus fr√©quents, il faut s'int√©resser aux termes dont la fr√©quence dans l'ensemble du corpus est faible, mais qui contribuent fortement √† diff√©rencier les √©l√©ments du corpus entre eux (les documents). On utilise pour cela une mesure de fr√©quence pond√©r√©e : la *tf-idf* pour *term frequency - inverse document frequency* qui permet d'accorder plus de poids aux termes les plus discriminants du corpus. $tf-idf= \\frac{occurrence\\ du\\ mot\\ dans\\ le\\ document }{nombre\\ de\\ mots\\ dans \\ le \\ document}* log (\\frac{nombre\\ de\\ documents\\ dans\\ le\\ corpus} {nombre\\ de \\ documents\\ dans\\ lequel\\ le\\ mot\\ apparait})$\n\n## Pond√©ration tf-idf\n\nOn commence par reprendre nos manipulations pr√©c√©dentes : cr√©ation de corpus, √©limination des stopwords, constitution de bi- ou tri- grammes. On applique ensuite la pond√©ration tf-idf.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndfmtfidf<-dfm_tfidf(dfm)\n\ndfmtfidf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDocument-feature matrix of: 4,388 documents, 5,429 features (99.81% sparse) and 8 docvars.\n       features\ndocs       comme toujours    super  service  changez     rien     sauf     peut\n  text1 1.406738 1.269355 1.173919 1.130383 3.040207 1.166595 2.320047 1.926263\n  text2 0        1.269355 2.347839 0        0        0        0        0       \n  text3 0        0        0        0        0        0        0        0       \n  text4 0        0        0        0        0        0        0        0       \n  text5 0        0        0        0        0        0        0        0       \n  text6 0        0        0        0        0        0        0        0       \n       features\ndocs        √™tre       l√†\n  text1 1.961025 2.320047\n  text2 0        2.320047\n  text3 0        0       \n  text4 0        0       \n  text5 0        0       \n  text6 0        0       \n[ reached max_ndoc ... 4,382 more documents, reached max_nfeat ... 5,419 more features ]\n```\n\n\n:::\n\n```{.r .cell-code}\n#Repr√©sentations graphiques\ntextplot_wordcloud(dfm, max_words = 200)\n```\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/tf-idf-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\ntextplot_wordcloud(dfmtfidf, max_words = 200)\n```\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/tf-idf-2.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n#On filtre les mots trop fr√©quents\n\ndfm_trim<-dfm_trim(dfm, max_termfreq = 500)\n\ndfmtfidf_trim<-dfm_tfidf(dfm_trim)\n\ntextplot_wordcloud(dfm_trim, max_words = 200)\n```\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/tf-idf-3.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\ntextplot_wordcloud(dfmtfidf_trim, max_words = 200, color = brewer.pal(6, \"Set2\"))\n```\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/tf-idf-4.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\ndisplay.brewer.all()\n```\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/tf-idf-5.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n# Comprendre le sens des termes\n\nOn peut visualiser un ou plusieurs termes dans leur contexte, afin d'avoir une meilleure compr√©hension de leur sens. Pour cela on utilise la fonction \"kwic\" pour key word in context, √† partir de l'objet tokens :\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(kwic(tok,\"livraison\",window = 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nKeyword-in-context with 6 matches.                                                     \n  [text2, 12] revanche n‚Äôarrive cliquer | livraison |\n  [text3, 13]    depuis conseil jusqu'√† | livraison |\n  [text5, 36]  perfection puisque d√©lai | livraison |\n  [text8, 14]    a enti√®re satisfaction | livraison |\n  [text14, 1]                           | Livraison |\n [text15, 21]         trouve peut frais | livraison |\n                            \n adresse toujours l√†        \n produit recommande vivement\n Espagne depuis France      \n aussi a rapide             \n rapide soign√©e Bons        \n √©lev√©                      \n```\n\n\n:::\n\n```{.r .cell-code}\n# kwic(tok,\"livraison\",window = 3)\n\nhead(kwic(tok, c(\"commande\", \"recommande\"),window = 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nKeyword-in-context with 6 matches.                                                                            \n  [text2, 2]                     d√©lai |  commande  | super rapide d√©lais   \n  [text2, 6]       super rapide d√©lais |  commande  | super rapide revanche \n [text3, 15] jusqu'√† livraison produit | recommande | vivement oiseaux Mania\n [text5, 14]   Food regardant derni√®re |  commande  | j'avais faite France  \n  [text7, 5]       semaines j'ai pass√© |  commande  | toujours livr√© √©toile \n  [text8, 9]       j'ai n√©anmoins fait |  commande  | difficult√© a enti√®re  \n```\n\n\n:::\n\n```{.r .cell-code}\nhead(kwic(tok,\"perroquet\",window = 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nKeyword-in-context with 6 matches.                                                      \n  [text5, 51]    d'avoir trouv√© solution | perroquet |\n  [text18, 8]        comp√©tent sers site | perroquet |\n [text22, 18]         convient tout fait | perroquet |\n  [text34, 6] treats granul√©s compress√©s | Perroquet |\n [text37, 10]        choix produit petit | perroquet |\n  [text39, 2]               Alimentation | perroquet |\n                              \n afin qu'il puisse            \n  üòä Excellent rapide          \n tr√®s heureux aussi           \n Gris Gabon tr√®s              \n n'h√©siterai refaire commandes\n commande depuis quelques     \n```\n\n\n:::\n\n```{.r .cell-code}\nhead(kwic(tok,\"prix\",window=10))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nKeyword-in-context with 6 matches.             \n  [text14, 6]\n [text15, 15]\n [text26, 23]\n [text28, 12]\n [text31, 77]\n  [text32, 7]\n                                                                                     \n                                                Livraison rapide soign√©e Bons produit\n            rapide satisfait commander cher oiseaux mania car Besan√ßon trouve produit\n beaucoup choix graines friandises accessoires commande a livr√©e rapidement encombres\n      harnais re√ßus d√©plore juste l‚Äôemballage spartiate arriv√© moiti√© d√©chir√© rapport\n                           flacons 57gr chacun √ßa fait peut ch√®re Mondial relay offre\n                                           Livraison rapide Bons produit bien emball√©\n                                                          \n | prix | raisonnables                                    \n | Prix | tr√®s attractif trouve peut frais livraison √©lev√©\n | Prix | int√©ressant                                     \n | prix | vente c‚Äôest peu l√©ger surtout harnais           \n | prix | bien plus raisonnable Bien cordialement         \n | prix | raisonnables                                    \n```\n\n\n:::\n:::\n",
    "supporting": [
      "chapter2_files\\figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}