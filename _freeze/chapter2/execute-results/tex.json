{
  "hash": "877b3abff28eb8c9dd5309ef8e9b6a3e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Pré-traitements\"\n---\n\n\n\n\n## Introduction\n\nLe document de travail contient deux types d'éléments : du texte pour expliquer et présenter ce que l'on fait et du code pour réaliser les manipulations de données, les analyses et les graphiques. On commence toujours un document avec un bloc de code de setup, pour lister et charger les packages que l'on va utiliser et les options générales pour l'édition du document.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(quanteda)\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(RColorBrewer)\n```\n:::\n\n\n\nDans un premier temps, nous allons tout simplement charger la base de données de travail puis la décrire. Ensuite, nous créerons un corpus, le visualisons. Puis nous effectueront quelques analyses liminaires, avant de voir les pré-traitements à réaliser sur le corpus.\n\n## Les données\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#On charge les données, stockées dans un fichier csv\n\ndata <- read_csv(\"data/data_trustpilot_oiseaux.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 4388 Columns: 7\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (4): auteur, date, month, comments\ndbl (3): id, year, note\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nnames(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"id\"       \"auteur\"   \"date\"     \"month\"    \"year\"     \"note\"     \"comments\"\n```\n\n\n:::\n\n```{.r .cell-code}\nview(data)\ndata\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4,388 x 7\n      id auteur                   date            month    year  note comments  \n   <dbl> <chr>                    <chr>           <chr>   <dbl> <dbl> <chr>     \n 1     1 MAQUET Cyril             07 août 2023    août     2023     5 \"Comme to~\n 2     2 Mme Laurence  Wolff      08 août 2023    août     2023     5 \"Le délai~\n 3     3 Une nouvelle cliente     07 août 2023    août     2023     5 \"Produits~\n 4     4 Patricia ALLAMAN         15 août 2023    août     2023     5 \"Envoi ra~\n 5     5 VPL                      24 juillet 2023 juillet  2023     5 \"Expéditi~\n 6     6 PHILIPPE GODIN           08 août 2023    août     2023     5 \"site sér~\n 7     7 Mme MARIA ADILIA PEREIRA 15 août 2023    août     2023     1 \"deux sem~\n 8     8 Rachel Mattyssen         31 juillet 2023 juillet  2023     5 \"Très bie~\n 9     9 Estelle Fay              16 juillet 2023 juillet  2023     5 \"Enfin un~\n10    10 Mme T.                   06 août 2023    août     2023     4 \"Satisfai~\n# i 4,378 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\n#Résumé des données\nsummary(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       id          auteur              date              month          \n Min.   :   1   Length:4388        Length:4388        Length:4388       \n 1st Qu.:1098   Class :character   Class :character   Class :character  \n Median :2194   Mode  :character   Mode  :character   Mode  :character  \n Mean   :2194                                                           \n 3rd Qu.:3291                                                           \n Max.   :4388                                                           \n      year           note         comments        \n Min.   :2013   Min.   :1.000   Length:4388       \n 1st Qu.:2018   1st Qu.:5.000   Class :character  \n Median :2020   Median :5.000   Mode  :character  \n Mean   :2019   Mean   :4.641                     \n 3rd Qu.:2021   3rd Qu.:5.000                     \n Max.   :2023   Max.   :5.000                     \n```\n\n\n:::\n:::\n\n\n\n## Premières analyses des données\n\nAvant de s'intéresser au contenu des commentaires, explorons la structure des données. On va regarder la distribution des commentaires et des notes dans le temps, et s'intéresser à la longueur des avis clients.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Les années \ndata$year<-as.factor(data$year)\nsummary(data$year)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 \n 125  209  249  171  324  341  375  735  853  623  383 \n```\n\n\n:::\n\n```{.r .cell-code}\ndata%>%\n  group_by(year)%>%\n  summarise(n=n(), prop=n/nrow(data))%>%\n  ggplot(aes(year,prop))+\n  geom_col(fill=\"green\",show.legend = TRUE)+\n  scale_y_continuous(labels = scales::percent)+\n  theme_light()+\n  labs(title = \"Répartition des avis dans le temps\", subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\", x=\"années\", y=NULL)\n```\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/data_explor-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n#Les notes\nsummary(data$note)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   5.000   5.000   4.641   5.000   5.000 \n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(as.factor(data$note))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   1    2    3    4    5 \n 116   62  169  589 3452 \n```\n\n\n:::\n\n```{.r .cell-code}\ndata%>%\n  group_by(note)%>%\n  summarise(n=n(), prop=n/nrow(data))%>%\n  ggplot(aes(note,prop))+\n  geom_col(fill=c(\"red\",\"pink\",\"orange\",\"gold\",\"lightgreen\"))+\n  annotate(\"text\", x=2, y=0.7, label=paste(\"Note moyenne = \",round(mean(data$note),1)))+\n  scale_y_continuous(labels=scales::percent)+\n  theme_light()+\n  labs(title = \"Répartition des avis en fonction des notes\", subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\", x=\"note\", y=NULL)\n```\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/data_explor-2.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n#Le nombre de caractère\ndata$nb_caractere<-nchar(data$comments) #on compte le nombre de caractère de chaque commentaire\nsummary(data$nb_caractere)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   12.0    41.0    74.0   104.3   130.0  1571.0 \n```\n\n\n:::\n\n```{.r .cell-code}\nggplot(data, aes(nb_caractere))+\n  geom_boxplot()+\n  scale_y_continuous(NULL, breaks = NULL)+\n  labs(x=NULL,title = \"Nombre de caractères des avis\", subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\")+\n  theme_light()+\n  coord_flip()\n```\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/data_explor-3.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n#Le nombre de tokens\n\ndata$nb_token<-ntoken(data$comments) #on compte le nombre de caractère de chaque commentaire\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: ntoken.character()/ntype.corpus() was deprecated in quanteda 4.0.0.\ni Please use ntoken(tokens(x)) instead.\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(data$nb_token)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    2.0     7.0    13.0    19.3    24.0   308.0 \n```\n\n\n:::\n\n```{.r .cell-code}\nggplot(data, aes(nb_token))+\n  geom_boxplot()+\n  scale_y_continuous(NULL, breaks = NULL)+\n  labs(x=NULL,title = \"Nombre de tokens des avis\", subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\")+\n  theme_light()+\n  coord_flip()\n```\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/data_explor-4.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n#On va filtrer au-dessus de 100 tokens\ndata_100t<-data%>%filter(nb_token<50)\n  \nggplot(data_100t, aes(nb_token))+\n  geom_boxplot()+\n  scale_y_continuous(NULL, breaks = NULL)+\n  labs(x=NULL,title = \"Nombre de tokens des avis\", subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\")+\n  theme_light()+\n  coord_flip()\n```\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/data_explor-5.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n#Les notes dans le temps\ndata%>%\n  mutate(note=as.factor(note))%>%\n  group_by(year, note)%>%\n  summarise(n=n() ,prop=n/nrow(data))%>%\n  ggplot(aes(year, prop))+\n  geom_col(aes(fill=note), show.legend = FALSE)+\n  scale_fill_discrete(type=c(\"red\",\"pink\",\"orange\",\"gold\",\"lightgreen\"))+\n  scale_y_continuous(labels=scales::percent)+\n  theme_light()+\n  labs(title = \"Répartition des avis dans le temps\", subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\", x=\"années\", y=NULL)\n```\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/data_explor-6.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\ndata%>%\n  mutate(note=as.factor(note))%>%\n  group_by(year, note)%>%\n  summarise(n=n())%>%\n  ggplot(aes(x=year, y=n, group=note))+\n  geom_bar(position=\"fill\",stat=\"identity\", aes(fill=note))+\n  scale_fill_discrete(type=c(\"red\",\"pink\",\"orange\",\"gold\",\"lightgreen\"))+\n  scale_y_continuous(labels=scales::percent)+\n  theme_minimal()+\n  labs(title = \"Comparaison de la répartition des notes dans le temps\", subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\", x=\"années\", y=NULL)\n```\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/data_explor-7.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n# Création du corpus et premières observations du corpus\n\nTout d'abord, nous transformons le jeu de données en corpus. La variable qui contient le texte est \"comments\", les autres variables vont devenir des métadonnées du corpus, c'est-à-dire des variables associées à chaque texte. Cela sera utile par le suite pour faire des analyses comparatives entre les textes suivant différentes variables (le temps en particulier, mais pas seulement).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Création du corpus\ncorpus_oiseaux<-corpus(data, text_field = \"comments\")\ncorpus_oiseaux\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCorpus consisting of 4,388 documents and 8 docvars.\ntext1 :\n\"Comme toujours, super service. Ne changez rien!(sauf peut êt...\"\n\ntext2 :\n\"Le délai de ma commande super rapide Le délais des ma comman...\"\n\ntext3 :\n\"Produits de qualité et équipe professionnelle … Très bon acc...\"\n\ntext4 :\n\"Envoi rapide, bien emballé et conforme à l'annonce\"\n\ntext5 :\n\"Expédition internationale ! J'ai récemment déménagé en Espag...\"\n\ntext6 :\n\"site sérieux bon produit livraisons plus que correct mais pe...\"\n\n[ reached max_ndoc ... 4,382 more documents ]\n```\n\n\n:::\n\n```{.r .cell-code}\ncorpus_oiseaux[\"text600\"] #pour visualiser un texte précis\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCorpus consisting of 1 document and 8 docvars.\ntext600 :\n\"Commande reçu dans les temps pas de surprise. Très bon site\"\n```\n\n\n:::\n\n```{.r .cell-code}\na<-corpus_oiseaux[\"text30\"]\nrm(a)\n```\n:::\n\n\n\nEnsuite, nous allons extraire de chaque texte les termes qui les composent. Ces termes sont nommés \"token\" (jeton), et comme vous pouvez le voir, ce ne sont pas uniquement des mots, mais tout caractère ou suite de caractères séparés des autres par un espace.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Extraction des tokens\ntok<-tokens(corpus_oiseaux)\ntok[\"text600\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTokens consisting of 1 document and 8 docvars.\ntext600 :\n [1] \"Commande\" \"reçu\"     \"dans\"     \"les\"      \"temps\"    \"pas\"     \n [7] \"de\"       \"surprise\" \".\"        \"Très\"     \"bon\"      \"site\"    \n```\n\n\n:::\n:::\n\n\n\nChaque texte est maintenant décomposé en une suite de tokens. Pour voir les termes les plus fréquents dans le corpus, ainsi que leur co-occurrences (apparition de deux termes en même temps), il convient de transformer l'objet tok en une matrice termes-documents. En ligne, tous les tokens identifiés, en ligne, tous les textes du corpus, et les valeurs correspondent au nombre d'occurrences (d'apparitions) de chaque token dans chaque document. Une particularité de cette matrice est qu'elle contient énormément de zéro.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Transformation en document-term frequency matrix\ndfm<-dfm(tok)\ndfm\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDocument-feature matrix of: 4,388 documents, 5,658 features (99.72% sparse) and 8 docvars.\n       features\ndocs    comme toujours , super service . ne changez rien !\n  text1     1        1 1     1       1 4  1       1    1 1\n  text2     0        1 1     2       0 3  0       0    0 0\n  text3     0        0 0     0       0 2  0       0    0 0\n  text4     0        0 1     0       0 0  0       0    0 0\n  text5     0        0 4     0       0 2  1       0    0 4\n  text6     0        0 0     0       0 0  0       0    0 0\n[ reached max_ndoc ... 4,382 more documents, reached max_nfeat ... 5,648 more features ]\n```\n\n\n:::\n:::\n\n\n\nEnfin, nous pouvons avoir un aperçu des termes les plus fréquents. Nous les visualisons d'abord sous forme de tableau (les 20 tokens les plus fréquents), puis sous la forme d'un nuage de mots, où la taille des mots correspond à leur fréquence dans le corpus.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Visualisation des termes les plus fréquents\ntextstat_frequency(dfm, n=20) #les 20 premiers termes les plus fréquents\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     feature frequency rank docfreq group\n1          .      4777    1    2094   all\n2          ,      2788    2    1576   all\n3         de      2685    3    1646   all\n4         et      2530    4    1915   all\n5       très      1996    5    1529   all\n6     rapide      1659    6    1569   all\n7         je      1622    7    1221   all\n8  livraison      1412    8    1299   all\n9          !      1349    9     649   all\n10         à      1312   10    1022   all\n11  commande      1238   11    1030   all\n12        le      1215   12     812   all\n13        la      1215   12     868   all\n14      pour      1049   14     782   all\n15       les      1013   15     754   all\n16      bien       920   16     803   all\n17        en       801   17     618   all\n18      site       793   18     686   all\n19  produits       738   19     661   all\n20        un       729   20     565   all\n```\n\n\n:::\n\n```{.r .cell-code}\ntextplot_wordcloud(dfm) #nuage de mots\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in text.default(x1, y1, word[i], cex = (1 + adjust) * size[i], offset =\n0, : font metrics unknown for Unicode character U+1F44D\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/0_viz-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nPour conclure sur cette première approche du corpus, nous voyons que nos analyses sont gếnées par la présence de la ponctuation et de plein de petits mots \"vides de sens\" (les articles par exemple). C'est pourquoi nous allons nettoyer le corpus pour avoir une meilleure vision de ce qu'il contient.\n\n# Nettoyage du corpus\n\nLe nettoyage du corpus pour les analyses se fait lors de la transformation en tokens. Nous allons ajouter des options pour supprimer la ponctuation, les chiffres et les stopwords (les mots qui n'apportent pas de sens sémantique mais permettent l'articulation du discours).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstopwords(\"fr\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  [1] \"au\"       \"aux\"      \"avec\"     \"ce\"       \"ces\"      \"dans\"    \n  [7] \"de\"       \"des\"      \"du\"       \"elle\"     \"en\"       \"et\"      \n [13] \"eux\"      \"il\"       \"je\"       \"la\"       \"le\"       \"leur\"    \n [19] \"lui\"      \"ma\"       \"mais\"     \"me\"       \"même\"     \"mes\"     \n [25] \"moi\"      \"mon\"      \"ne\"       \"nos\"      \"notre\"    \"nous\"    \n [31] \"on\"       \"ou\"       \"par\"      \"pas\"      \"pour\"     \"qu\"      \n [37] \"que\"      \"qui\"      \"sa\"       \"se\"       \"ses\"      \"son\"     \n [43] \"sur\"      \"ta\"       \"te\"       \"tes\"      \"toi\"      \"ton\"     \n [49] \"tu\"       \"un\"       \"une\"      \"vos\"      \"votre\"    \"vous\"    \n [55] \"c\"        \"d\"        \"j\"        \"l\"        \"à\"        \"m\"       \n [61] \"n\"        \"s\"        \"t\"        \"y\"        \"été\"      \"étée\"    \n [67] \"étées\"    \"étés\"     \"étant\"    \"suis\"     \"es\"       \"est\"     \n [73] \"sommes\"   \"êtes\"     \"sont\"     \"serai\"    \"seras\"    \"sera\"    \n [79] \"serons\"   \"serez\"    \"seront\"   \"serais\"   \"serait\"   \"serions\" \n [85] \"seriez\"   \"seraient\" \"étais\"    \"était\"    \"étions\"   \"étiez\"   \n [91] \"étaient\"  \"fus\"      \"fut\"      \"fûmes\"    \"fûtes\"    \"furent\"  \n [97] \"sois\"     \"soit\"     \"soyons\"   \"soyez\"    \"soient\"   \"fusse\"   \n[103] \"fusses\"   \"fût\"      \"fussions\" \"fussiez\"  \"fussent\"  \"ayant\"   \n[109] \"eu\"       \"eue\"      \"eues\"     \"eus\"      \"ai\"       \"as\"      \n[115] \"avons\"    \"avez\"     \"ont\"      \"aurai\"    \"auras\"    \"aura\"    \n[121] \"aurons\"   \"aurez\"    \"auront\"   \"aurais\"   \"aurait\"   \"aurions\" \n[127] \"auriez\"   \"auraient\" \"avais\"    \"avait\"    \"avions\"   \"aviez\"   \n[133] \"avaient\"  \"eut\"      \"eûmes\"    \"eûtes\"    \"eurent\"   \"aie\"     \n[139] \"aies\"     \"ait\"      \"ayons\"    \"ayez\"     \"aient\"    \"eusse\"   \n[145] \"eusses\"   \"eût\"      \"eussions\" \"eussiez\"  \"eussent\"  \"ceci\"    \n[151] \"cela\"     \"celà\"     \"cet\"      \"cette\"    \"ici\"      \"ils\"     \n[157] \"les\"      \"leurs\"    \"quel\"     \"quels\"    \"quelle\"   \"quelles\" \n[163] \"sans\"     \"soi\"     \n```\n\n\n:::\n\n```{.r .cell-code}\ntok<-tokens(corpus_oiseaux, remove_punct = TRUE, remove_numbers = TRUE)%>%\n  tokens_remove(stopwords(\"fr\"))\ncorpus_oiseaux[\"text600\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCorpus consisting of 1 document and 8 docvars.\ntext600 :\n\"Commande reçu dans les temps pas de surprise. Très bon site\"\n```\n\n\n:::\n\n```{.r .cell-code}\ntok[\"text600\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTokens consisting of 1 document and 8 docvars.\ntext600 :\n[1] \"Commande\" \"reçu\"     \"temps\"    \"surprise\" \"Très\"     \"bon\"      \"site\"    \n```\n\n\n:::\n:::\n\n\n\nEnsuite, on transforme en dfm et on visualise ce que ça donne.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndfm<-dfm(tok)\ndfm\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDocument-feature matrix of: 4,388 documents, 5,430 features (99.81% sparse) and 8 docvars.\n       features\ndocs    comme toujours super service changez rien sauf peut être là\n  text1     1        1     1       1       1    1    1    1    1  1\n  text2     0        1     2       0       0    0    0    0    0  1\n  text3     0        0     0       0       0    0    0    0    0  0\n  text4     0        0     0       0       0    0    0    0    0  0\n  text5     0        0     0       0       0    0    0    0    0  0\n  text6     0        0     0       0       0    0    0    0    0  0\n[ reached max_ndoc ... 4,382 more documents, reached max_nfeat ... 5,420 more features ]\n```\n\n\n:::\n\n```{.r .cell-code}\ntextstat_frequency(dfm, n=20)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      feature frequency rank docfreq group\n1        très      1996    1    1529   all\n2      rapide      1659    2    1569   all\n3   livraison      1412    3    1299   all\n4    commande      1238    4    1030   all\n5        bien       920    5     803   all\n6        site       793    6     686   all\n7    produits       738    7     661   all\n8           a       706    8     566   all\n9     produit       676    9     609   all\n10        bon       639   10     575   all\n11      merci       553   11     529   all\n12 recommande       553   11     534   all\n13    parfait       522   13     474   all\n14    qualité       487   14     447   all\n15       j'ai       382   15     289   all\n16    oiseaux       379   16     319   all\n17       tout       375   17     347   all\n18       reçu       362   18     332   all\n19      colis       360   19     313   all\n20 rapidement       356   20     343   all\n```\n\n\n:::\n\n```{.r .cell-code}\ntextplot_wordcloud(dfm)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in text.default(x1, y1, word[i], cex = (1 + adjust) * size[i], offset =\n0, : font metrics unknown for Unicode character U+1F44D\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/1_dfm-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\ng<-textstat_frequency(dfm,n=20)\n\n\nggplot(g, aes(x = feature, y=frequency))+\n  geom_col()+\n  coord_flip()\n```\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/1_dfm-2.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nggplot(g, aes(x = reorder(feature, frequency), y=frequency, fill=frequency))+\n  geom_col(show.legend = TRUE)+\n  coord_flip()+\n  theme_light()+\n  scale_fill_distiller(palette = \"Blues\", direction = 1)+\n  labs(title=\"Les mots les plus fréquents\",subtitle = \"du corpus Oiseaux Mania\",caption = \"Source : Data TrustPilot\")+\n  xlab(NULL)+\n  ylab(\"Fréquence\")\n```\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/1_dfm-3.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\ndisplay.brewer.all()\n```\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/1_dfm-4.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nGlobalement, la commande et la livraison sont TRÈS rapides et les produits sont bons. La surreprésentation de ces termes dans le corpus nous empêche de voir les thématiques abordées de manière moins évidentes. Nous avons plusieurs solutions qui s'offrent à nous : filtrer les mots trop fréquents du corpus ou nous intéresser à une autre mesure de la fréquence d'apparition. Nous allons d'abord filtrer le corpus.\n\nOn peut aussi vouloir remplacer des termes par d'autres, comme ici \"produits\" par \"produit\".\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntok<-tokens_replace(tok, \"produits\", \"produit\")\ndfm<-dfm(tok)\ntextstat_frequency(dfm, n=20)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      feature frequency rank docfreq group\n1        très      1996    1    1529   all\n2      rapide      1659    2    1569   all\n3     produit      1414    3    1243   all\n4   livraison      1412    4    1299   all\n5    commande      1238    5    1030   all\n6        bien       920    6     803   all\n7        site       793    7     686   all\n8           a       706    8     566   all\n9         bon       639    9     575   all\n10      merci       553   10     529   all\n11 recommande       553   10     534   all\n12    parfait       522   12     474   all\n13    qualité       487   13     447   all\n14       j'ai       382   14     289   all\n15    oiseaux       379   15     319   all\n16       tout       375   16     347   all\n17       reçu       362   17     332   all\n18      colis       360   18     313   all\n19 rapidement       356   19     343   all\n20    service       351   20     325   all\n```\n\n\n:::\n:::\n\n\n\n# Filtrer le corpus des termes trop fréquents\n\nNous allons filtrer les mots qui sont présents plus de 500 fois dans le corpus.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndfm_trim<-dfm_trim(dfm, max_termfreq = 500)\n\ntextstat_frequency(dfm_trim, n=20)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      feature frequency rank docfreq group\n1     qualité       487    1     447   all\n2        j'ai       382    2     289   all\n3     oiseaux       379    3     319   all\n4        tout       375    4     347   all\n5        reçu       362    5     332   all\n6       colis       360    6     313   all\n7  rapidement       356    7     343   all\n8     service       351    8     325   all\n9        prix       327    9     307   all\n10 satisfaite       327    9     301   all\n11      super       326   11     294   all\n12   conforme       326   11     307   all\n13       rien       317   13     299   all\n14      bonne       293   14     277   all\n15       plus       292   15     254   all\n16       tres       278   16     216   all\n17   toujours       274   17     236   all\n18      mania       256   18     228   all\n19      envoi       239   19     231   all\n20    sérieux       232   20     220   all\n```\n\n\n:::\n\n```{.r .cell-code}\ntextplot_wordcloud(dfm_trim, max_words = 100, color = rev(brewer.pal(10, \"Set2\")))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in brewer.pal(10, \"Set2\"): n too large, allowed maximum for palette Set2 is 8\nReturning the palette you asked for with that many colors\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/1_filter-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\ndisplay.brewer.all()\n```\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/1_filter-2.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nUne autre manière de s'y prendre est d'éliminer directement les termes que l'on ne veut pas voir apparaître.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntextstat_frequency(dfm,n=20)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      feature frequency rank docfreq group\n1        très      1996    1    1529   all\n2      rapide      1659    2    1569   all\n3     produit      1414    3    1243   all\n4   livraison      1412    4    1299   all\n5    commande      1238    5    1030   all\n6        bien       920    6     803   all\n7        site       793    7     686   all\n8           a       706    8     566   all\n9         bon       639    9     575   all\n10      merci       553   10     529   all\n11 recommande       553   10     534   all\n12    parfait       522   12     474   all\n13    qualité       487   13     447   all\n14       j'ai       382   14     289   all\n15    oiseaux       379   15     319   all\n16       tout       375   16     347   all\n17       reçu       362   17     332   all\n18      colis       360   18     313   all\n19 rapidement       356   19     343   all\n20    service       351   20     325   all\n```\n\n\n:::\n\n```{.r .cell-code}\nrem<-c(\"très\",\"rapide\",\"produit\",\"livraison\", \"commande\", \"bien\", \"site\", \"a\", \"bon\", \"merci\", \"recommande\",\"parfait\", \"j'ai\",\"tres\")\n\ndfm_rem<-dfm_remove(dfm, rem)\ntextstat_frequency(dfm_rem, n=20)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      feature frequency rank docfreq group\n1     qualité       487    1     447   all\n2     oiseaux       379    2     319   all\n3        tout       375    3     347   all\n4        reçu       362    4     332   all\n5       colis       360    5     313   all\n6  rapidement       356    6     343   all\n7     service       351    7     325   all\n8        prix       327    8     307   all\n9  satisfaite       327    8     301   all\n10      super       326   10     294   all\n11   conforme       326   10     307   all\n12       rien       317   12     299   all\n13      bonne       293   13     277   all\n14       plus       292   14     254   all\n15   toujours       274   15     236   all\n16      mania       256   16     228   all\n17      envoi       239   17     231   all\n18    sérieux       232   18     220   all\n19      choix       222   19     205   all\n20  satisfait       220   20     205   all\n```\n\n\n:::\n:::\n\n\n\n# La loi de Zipf\n\nVérifions la proposition de la loi de Zipf, selon laquelle la fréquence d'apparition d'un terme est inversement proportionnel à son rang.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nzipf<-textstat_frequency(dfm)\nggplot(zipf, aes(rank, frequency))+\n  geom_line(color=\"blue\")+\n  geom_point(color=\"darkgreen\")+\n   scale_x_log10() +\n   scale_y_log10()+\n  theme_light()+\n  labs(title = \"Observation de la loi de Zipf\",x=\"log (rang)\",y=\"log (fréquence)\")\n```\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/zipf_law-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n# Mesures de fréquence\n\nOn s'est pour l'instant intéressé uniquement aux termes les plus fréquents dans un corpus. On a vu comment éliminer les termes trop fréquents pour qu'ils nous apportent de l'information. Pour l'analyse de topics, il nous faut prendre un autre angle d'attaque : afin de détecter les sujets abordés dans un corpus, on ne peut se contenter d'observer les mots les plus fréquents, il faut s'intéresser aux termes dont la fréquence dans l'ensemble du corpus est faible, mais qui contribuent fortement à différencier les éléments du corpus entre eux (les documents). On utilise pour cela une mesure de fréquence pondérée : la *tf-idf* pour *term frequency - inverse document frequency* qui permet d'accorder plus de poids aux termes les plus discriminants du corpus. $tf-idf= \\frac{occurrence\\ du\\ mot\\ dans\\ le\\ document }{nombre\\ de\\ mots\\ dans \\ le \\ document}* log (\\frac{nombre\\ de\\ documents\\ dans\\ le\\ corpus} {nombre\\ de \\ documents\\ dans\\ lequel\\ le\\ mot\\ apparait})$\n\n## Pondération tf-idf\n\nOn commence par reprendre nos manipulations précédentes : création de corpus, élimination des stopwords, constitution de bi- ou tri- grammes. On applique ensuite la pondération tf-idf.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndfmtfidf<-dfm_tfidf(dfm)\n\ndfmtfidf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDocument-feature matrix of: 4,388 documents, 5,429 features (99.81% sparse) and 8 docvars.\n       features\ndocs       comme toujours    super  service  changez     rien     sauf     peut\n  text1 1.406738 1.269355 1.173919 1.130383 3.040207 1.166595 2.320047 1.926263\n  text2 0        1.269355 2.347839 0        0        0        0        0       \n  text3 0        0        0        0        0        0        0        0       \n  text4 0        0        0        0        0        0        0        0       \n  text5 0        0        0        0        0        0        0        0       \n  text6 0        0        0        0        0        0        0        0       \n       features\ndocs        être       là\n  text1 1.961025 2.320047\n  text2 0        2.320047\n  text3 0        0       \n  text4 0        0       \n  text5 0        0       \n  text6 0        0       \n[ reached max_ndoc ... 4,382 more documents, reached max_nfeat ... 5,419 more features ]\n```\n\n\n:::\n\n```{.r .cell-code}\n#Représentations graphiques\ntextplot_wordcloud(dfm, max_words = 200)\n```\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/tf-idf-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\ntextplot_wordcloud(dfmtfidf, max_words = 200)\n```\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/tf-idf-2.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n#On filtre les mots trop fréquents\n\ndfm_trim<-dfm_trim(dfm, max_termfreq = 500)\n\ndfmtfidf_trim<-dfm_tfidf(dfm_trim)\n\ntextplot_wordcloud(dfm_trim, max_words = 200)\n```\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/tf-idf-3.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\ntextplot_wordcloud(dfmtfidf_trim, max_words = 200, color = brewer.pal(6, \"Set2\"))\n```\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/tf-idf-4.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\ndisplay.brewer.all()\n```\n\n::: {.cell-output-display}\n![](chapter2_files/figure-pdf/tf-idf-5.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n# Comprendre le sens des termes\n\nOn peut visualiser un ou plusieurs termes dans leur contexte, afin d'avoir une meilleure compréhension de leur sens. Pour cela on utilise la fonction \"kwic\" pour key word in context, à partir de l'objet tokens :\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(kwic(tok,\"livraison\",window = 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nKeyword-in-context with 6 matches.                                                     \n  [text2, 12] revanche n’arrive cliquer | livraison |\n  [text3, 13]    depuis conseil jusqu'à | livraison |\n  [text5, 36]  perfection puisque délai | livraison |\n  [text8, 14]    a entière satisfaction | livraison |\n  [text14, 1]                           | Livraison |\n [text15, 21]         trouve peut frais | livraison |\n                            \n adresse toujours là        \n produit recommande vivement\n Espagne depuis France      \n aussi a rapide             \n rapide soignée Bons        \n élevé                      \n```\n\n\n:::\n\n```{.r .cell-code}\n# kwic(tok,\"livraison\",window = 3)\n\nhead(kwic(tok, c(\"commande\", \"recommande\"),window = 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nKeyword-in-context with 6 matches.                                                                            \n  [text2, 2]                     délai |  commande  | super rapide délais   \n  [text2, 6]       super rapide délais |  commande  | super rapide revanche \n [text3, 15] jusqu'à livraison produit | recommande | vivement oiseaux Mania\n [text5, 14]   Food regardant dernière |  commande  | j'avais faite France  \n  [text7, 5]       semaines j'ai passé |  commande  | toujours livré étoile \n  [text8, 9]       j'ai néanmoins fait |  commande  | difficulté a entière  \n```\n\n\n:::\n\n```{.r .cell-code}\nhead(kwic(tok,\"perroquet\",window = 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nKeyword-in-context with 6 matches.                                                      \n  [text5, 51]    d'avoir trouvé solution | perroquet |\n  [text18, 8]        compétent sers site | perroquet |\n [text22, 18]         convient tout fait | perroquet |\n  [text34, 6] treats granulés compressés | Perroquet |\n [text37, 10]        choix produit petit | perroquet |\n  [text39, 2]               Alimentation | perroquet |\n                              \n afin qu'il puisse            \n  😊 Excellent rapide          \n très heureux aussi           \n Gris Gabon très              \n n'hésiterai refaire commandes\n commande depuis quelques     \n```\n\n\n:::\n\n```{.r .cell-code}\nhead(kwic(tok,\"prix\",window=10))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nKeyword-in-context with 6 matches.             \n  [text14, 6]\n [text15, 15]\n [text26, 23]\n [text28, 12]\n [text31, 77]\n  [text32, 7]\n                                                                                     \n                                                Livraison rapide soignée Bons produit\n            rapide satisfait commander cher oiseaux mania car Besançon trouve produit\n beaucoup choix graines friandises accessoires commande a livrée rapidement encombres\n      harnais reçus déplore juste l’emballage spartiate arrivé moitié déchiré rapport\n                           flacons 57gr chacun ça fait peut chère Mondial relay offre\n                                           Livraison rapide Bons produit bien emballé\n                                                          \n | prix | raisonnables                                    \n | Prix | très attractif trouve peut frais livraison élevé\n | Prix | intéressant                                     \n | prix | vente c’est peu léger surtout harnais           \n | prix | bien plus raisonnable Bien cordialement         \n | prix | raisonnables                                    \n```\n\n\n:::\n:::\n",
    "supporting": [
      "chapter2_files\\figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}