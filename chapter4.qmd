---
title: "Annotations et dépendances syntaxiques"
---

```{r setup, include=TRUE,output=FALSE}
library(readxl)
library(tidyverse)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
library(RColorBrewer)
display.brewer.all()
```

## Les données

```{r data, warning=FALSE}
data <- read_csv("data/data_trustpilot_oiseaux.csv")

data$nb_caractere<-nchar(data$comments) #on compte le nombre de caractère de chaque commentaire
summary(data$nb_caractere)
mean(data$nb_caractere)
median(data$nb_caractere)
round(mean(data$nb_caractere),1)


moy<-round(mean(na.omit(data$nb_caractere)), 1)

ggplot(data)+
  geom_boxplot(aes(nb_caractere))+
  geom_text(aes(x=500, y=0.2,label=paste("Moyenne :",moy)))+
  coord_flip()+
  scale_y_continuous(NULL, breaks = NULL)+
  theme_minimal()

```

## Les traitements préliminaires

On reprend ce qu'on a fait au cours dernier, sans éliminer les termes trop fréquents :

```{r corpus}
corpus_oiseaux<-corpus(data, text_field = "comments")

tok<-tokens(corpus_oiseaux, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE)%>%
  tokens_remove(stopwords("fr"))

dfm<-dfm(tok)

```

## Co-occurrences

On va maintenant constituer des bi-grammes basés sur de nombreuses co-occurrences entre les termes :

```{r cooc}
# textstat_collocations(tok)

head(textstat_collocations(tok), 5)
tail(textstat_collocations(tok),10)

colloc<-textstat_collocations(tok, min_count = 10, tolower = TRUE)
head(colloc,10)
tail(colloc,10)

tok_cooc<-tokens_compound(tok, pattern = colloc[colloc$z>6.97,],join = TRUE)

tok["text400"]
tok_cooc["text400"]

```

Analyse de fréquence et représentation graphique :

```{r cooc_viz, warning=FALSE}
dfm_cooc<-dfm(tok_cooc)

dfm_cooc2<-dfm_trim(dfm_cooc, max_termfreq = 170)

head(textstat_frequency(dfm_cooc2),20)

textplot_wordcloud(dfm_cooc2, max_words = 200, color = brewer.pal(6, "Set2"))

tok_cooc<-tokens_replace(tok_cooc, c("très_rapidement","très_satisfait"), c("très_rapide","très_satisfaite"))

dfm_cooc<-dfm(tok_cooc)
textstat_frequency(dfm_cooc, n=25)

dfm_cooc2<-dfm_trim(dfm_cooc, max_termfreq = 175)

textplot_wordcloud(dfm_cooc2, max_words = 100, color = brewer.pal(6, "Set2"))
```


## Annotations

Pour cette partie, on repart du jeu de données brut.

### Détecter les langues

Dans le cas d'un corpus composé de plusieurs langues (par exemple, un corpus extrait de twitter), il peut être intéressant de filtrer le corpus à partir de la langue. On utilise un algorithme, qui peut être long à exécuter selon la taille du corpus, et qui est plutôt performant : cld3. Il repose sur un réseau de neurones développé par [Google](https://github.com/ropensci/cld3/blob/master/README.md)

```{r langue}
library(cld3)

data$langue<-detect_language(data$comments)
# data$langue

data_fr<-data%>%filter(langue=="fr")

```

### POS

```{r ann_POS}
library(cleanNLP)

# cnlp_init_udpipe(model_name = "french")
# 
# annotate<-cnlp_annotate(data$comments, verbose = 100)
# ann_token<-annotate$token
# write_csv2(ann_token, "annotation_oiseaux.csv")
# write_rds(ann_token,"annotation_oiseaux.rds")

ann_token<-read_rds("data/annotation_oiseaux.rds")

head(ann_token%>%filter(upos=="ADJ"|upos=="NOUN"|upos=="VERB"),15)

ann_token%>%filter(upos=="ADJ"|upos=="NOUN"|upos=="VERB")

g<-ann_token%>%group_by(upos)%>%
  summarise(n=n())%>%
  filter(!is.na(upos))

ggplot(g)+
  geom_col(aes(reorder(upos,n),n, fill=n), show.legend = FALSE)+
  scale_fill_fermenter(palette = "PuRd", direction = 1)+
  coord_flip()+
  labs(title = "Fréquence des UPOS", subtitle = "Corpus Oiseaux Mania", caption = "Data TrustPilot", x=NULL, y=NULL)+
  theme_dark()
```

Maintenant, on va s'intéresser à des catégories grammaticales spécifiques :

```{r ann_viz, message=FALSE}

vocab1<-ann_token%>%
  filter(upos=="NOUN")%>%
  summarise(freq=n(),.by=lemma)%>%
  filter(freq>55)

ggplot(vocab1,aes(x=reorder(lemma,freq),y=freq))+
  geom_bar(stat="identity",fill="lightgreen")+
  coord_flip()+
  theme_minimal()+
  labs(title = "Noms communs les plus fréquents",subtitle = "Corpus Oiseaux Mania", caption="Data : TrustPilot",x="Noms commun",y="Fréquence")

vocab1bis<-ann_token%>%
  filter(upos=="ADJ")%>%
  summarise(freq=n(),.by=lemma)%>%
  filter(freq>55)

ggplot(vocab1bis,aes(x=reorder(lemma,freq),y=freq))+
  geom_bar(stat="identity",fill="darkblue")+
  coord_flip()+
  theme_minimal()+
  labs(title = "Adjectifs les plus fréquents",subtitle = "Corpus Oiseaux Mania", caption="Data : TrustPilot",x="Adjectifs",y="Fréquence")


vocab2<-ann_token%>%
  filter(upos=="NOUN" | upos=="VERB" | upos=="ADJ")%>%
  summarise(freq=n(),.by=c(lemma,upos))%>%
  filter(freq>30)%>%
  mutate(angle= 90 * sample(c(0, 1), n(), replace = TRUE, prob = c(75, 25)))

library(ggwordcloud)
ggplot(vocab2)+
  geom_text_wordcloud_area(aes(label=lemma, size=freq, color=freq, angle=angle))+
  scale_size_area(max_size = 24)+
  scale_color_fermenter(palette = "Set2")+
  theme_minimal()

ggplot(vocab2)+
  geom_text_wordcloud_area(aes(label=lemma, size=freq, color=upos, angle=angle))+
  scale_size_area(max_size = 24)+
  scale_color_manual(values=c("ADJ"="orange","NOUN"="lightgreen","VERB"="purple"))+
  theme_minimal()

```

## Les dépendances syntaxiques

Quels sont les mots associés aux termes cibles ?

```{r dep_synt}

#on met à niveau la racine
ann_racine<- ann_token%>%
  left_join(ann_token,by= c("doc_id"="doc_id", "sid"="sid", "tid_source"="tid"), suffix=c("", "_source"))
#on filtre les relation nominales puis celle qui concerne les termes cibles
foo<-ann_racine %>%
  filter(relation == "amod"|relation =="acl"|relation =="nmod"|relation =="appos") %>%
  select(qual = lemma, source = lemma_source)%>%
  filter(source=="commande"|source=="livraison"|source=="produit"|source=="prix")%>% 
  group_by(source,qual)%>%
  summarise(n=n())
# On remet en forme les données
foo1<-foo%>%
  pivot_wider(names_from = source, values_from = n)%>%
  mutate(across(everything(), ~replace_na(.x,0)))%>%
  mutate(sum=rowSums(.[,2:5]))%>%
  filter(sum>15)%>%
  select(-sum)%>%
  pivot_longer(!qual, names_to = "source", values_to = "n")



ggplot(foo1,aes(x=reorder(qual,n), y=n, group=source))+
  geom_bar(stat="identity",aes(fill=source),position=position_dodge())+
  coord_flip()+
  scale_fill_brewer(palette="Spectral",direction = -1)+
  theme_minimal()+ 
  labs( title="Analyse des dépendances nominales", subtitle = "les termes du site et du service",caption = "Data : TrustPilot sur Oiseaux Mania", x="tokens dépendants", y="Fréquence", fill="Termes")+
  facet_wrap(~source, ncol = 4)
```
