---
title: "Pré-traitements"
---


## Introduction

Le document de travail contient deux types d'éléments : du texte pour expliquer et présenter ce que l'on fait et du code pour réaliser les manipulations de données, les analyses et les graphiques. On commence toujours un document avec un bloc de code de setup, pour lister et charger les packages que l'on va utiliser et les options générales pour l'édition du document.

```{r setup, include=TRUE,output=FALSE}
library(readxl)
library(tidyverse)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
library(RColorBrewer)
```

Dans un premier temps, nous allons tout simplement charger la base de données de travail puis la décrire. Ensuite, nous créerons un corpus, le visualisons. Puis nous effectueront quelques analyses liminaires, avant de voir les pré-traitements à réaliser sur le corpus.

## Les données

```{r data}
#On charge les données, stockées dans un fichier csv

data <- read_csv("data/data_trustpilot_oiseaux.csv")
names(data)
view(data)
data

#Résumé des données
summary(data)

```

## Premières analyses des données

Avant de s'intéresser au contenu des commentaires, explorons la structure des données. On va regarder la distribution des commentaires et des notes dans le temps, et s'intéresser à la longueur des avis clients.

```{r data_explor, message=FALSE}
#Les années 
data$year<-as.factor(data$year)
summary(data$year)

data%>%
  group_by(year)%>%
  summarise(n=n(), prop=n/nrow(data))%>%
  ggplot(aes(year,prop))+
  geom_col(fill="green",show.legend = TRUE)+
  scale_y_continuous(labels = scales::percent)+
  theme_light()+
  labs(title = "Répartition des avis dans le temps", subtitle = "Corpus Oiseaux Mania", caption="Data : TrustPilot", x="années", y=NULL)

#Les notes
summary(data$note)
summary(as.factor(data$note))

data%>%
  group_by(note)%>%
  summarise(n=n(), prop=n/nrow(data))%>%
  ggplot(aes(note,prop))+
  geom_col(fill=c("red","pink","orange","gold","lightgreen"))+
  annotate("text", x=2, y=0.7, label=paste("Note moyenne = ",round(mean(data$note),1)))+
  scale_y_continuous(labels=scales::percent)+
  theme_light()+
  labs(title = "Répartition des avis en fonction des notes", subtitle = "Corpus Oiseaux Mania", caption="Data : TrustPilot", x="note", y=NULL)


#Le nombre de caractère
data$nb_caractere<-nchar(data$comments) #on compte le nombre de caractère de chaque commentaire
summary(data$nb_caractere)

ggplot(data, aes(nb_caractere))+
  geom_boxplot()+
  scale_y_continuous(NULL, breaks = NULL)+
  labs(x=NULL,title = "Nombre de caractères des avis", subtitle = "Corpus Oiseaux Mania", caption="Data : TrustPilot")+
  theme_light()+
  coord_flip()

#Le nombre de tokens

data$nb_token<-ntoken(data$comments) #on compte le nombre de caractère de chaque commentaire
summary(data$nb_token)

ggplot(data, aes(nb_token))+
  geom_boxplot()+
  scale_y_continuous(NULL, breaks = NULL)+
  labs(x=NULL,title = "Nombre de tokens des avis", subtitle = "Corpus Oiseaux Mania", caption="Data : TrustPilot")+
  theme_light()+
  coord_flip()

#On va filtrer au-dessus de 100 tokens
data_100t<-data%>%filter(nb_token<50)
  
ggplot(data_100t, aes(nb_token))+
  geom_boxplot()+
  scale_y_continuous(NULL, breaks = NULL)+
  labs(x=NULL,title = "Nombre de tokens des avis", subtitle = "Corpus Oiseaux Mania", caption="Data : TrustPilot")+
  theme_light()+
  coord_flip()

#Les notes dans le temps
data%>%
  mutate(note=as.factor(note))%>%
  group_by(year, note)%>%
  summarise(n=n() ,prop=n/nrow(data))%>%
  ggplot(aes(year, prop))+
  geom_col(aes(fill=note), show.legend = FALSE)+
  scale_fill_discrete(type=c("red","pink","orange","gold","lightgreen"))+
  scale_y_continuous(labels=scales::percent)+
  theme_light()+
  labs(title = "Répartition des avis dans le temps", subtitle = "Corpus Oiseaux Mania", caption="Data : TrustPilot", x="années", y=NULL)


data%>%
  mutate(note=as.factor(note))%>%
  group_by(year, note)%>%
  summarise(n=n())%>%
  ggplot(aes(x=year, y=n, group=note))+
  geom_bar(position="fill",stat="identity", aes(fill=note))+
  scale_fill_discrete(type=c("red","pink","orange","gold","lightgreen"))+
  scale_y_continuous(labels=scales::percent)+
  theme_minimal()+
  labs(title = "Comparaison de la répartition des notes dans le temps", subtitle = "Corpus Oiseaux Mania", caption="Data : TrustPilot", x="années", y=NULL)


```

# Création du corpus et premières observations du corpus

Tout d'abord, nous transformons le jeu de données en corpus. La variable qui contient le texte est "comments", les autres variables vont devenir des métadonnées du corpus, c'est-à-dire des variables associées à chaque texte. Cela sera utile par le suite pour faire des analyses comparatives entre les textes suivant différentes variables (le temps en particulier, mais pas seulement).

```{r 0_corpus}
#Création du corpus
corpus_oiseaux<-corpus(data, text_field = "comments")
corpus_oiseaux
corpus_oiseaux["text600"] #pour visualiser un texte précis

a<-corpus_oiseaux["text30"]
rm(a)

```

Ensuite, nous allons extraire de chaque texte les termes qui les composent. Ces termes sont nommés "token" (jeton), et comme vous pouvez le voir, ce ne sont pas uniquement des mots, mais tout caractère ou suite de caractères séparés des autres par un espace.

```{r 0_tokens}
#Extraction des tokens
tok<-tokens(corpus_oiseaux)
tok["text600"]
```

Chaque texte est maintenant décomposé en une suite de tokens. Pour voir les termes les plus fréquents dans le corpus, ainsi que leur co-occurrences (apparition de deux termes en même temps), il convient de transformer l'objet tok en une matrice termes-documents. En ligne, tous les tokens identifiés, en ligne, tous les textes du corpus, et les valeurs correspondent au nombre d'occurrences (d'apparitions) de chaque token dans chaque document. Une particularité de cette matrice est qu'elle contient énormément de zéro.

```{r 0_dfm}
#Transformation en document-term frequency matrix
dfm<-dfm(tok)
dfm
```

Enfin, nous pouvons avoir un aperçu des termes les plus fréquents. Nous les visualisons d'abord sous forme de tableau (les 20 tokens les plus fréquents), puis sous la forme d'un nuage de mots, où la taille des mots correspond à leur fréquence dans le corpus.

```{r 0_viz}
#Visualisation des termes les plus fréquents
textstat_frequency(dfm, n=20) #les 20 premiers termes les plus fréquents
textplot_wordcloud(dfm) #nuage de mots
```

Pour conclure sur cette première approche du corpus, nous voyons que nos analyses sont gếnées par la présence de la ponctuation et de plein de petits mots "vides de sens" (les articles par exemple). C'est pourquoi nous allons nettoyer le corpus pour avoir une meilleure vision de ce qu'il contient.

# Nettoyage du corpus

Le nettoyage du corpus pour les analyses se fait lors de la transformation en tokens. Nous allons ajouter des options pour supprimer la ponctuation, les chiffres et les stopwords (les mots qui n'apportent pas de sens sémantique mais permettent l'articulation du discours).

```{r 1_tok}

stopwords("fr")

tok<-tokens(corpus_oiseaux, remove_punct = TRUE, remove_numbers = TRUE)%>%
  tokens_remove(stopwords("fr"))
corpus_oiseaux["text600"]
tok["text600"]
```

Ensuite, on transforme en dfm et on visualise ce que ça donne.

```{r 1_dfm}
dfm<-dfm(tok)
dfm
textstat_frequency(dfm, n=20)
textplot_wordcloud(dfm)

g<-textstat_frequency(dfm,n=20)


ggplot(g, aes(x = feature, y=frequency))+
  geom_col()+
  coord_flip()


ggplot(g, aes(x = reorder(feature, frequency), y=frequency, fill=frequency))+
  geom_col(show.legend = TRUE)+
  coord_flip()+
  theme_light()+
  scale_fill_distiller(palette = "Blues", direction = 1)+
  labs(title="Les mots les plus fréquents",subtitle = "du corpus Oiseaux Mania",caption = "Source : Data TrustPilot")+
  xlab(NULL)+
  ylab("Fréquence")

display.brewer.all()

```

Globalement, la commande et la livraison sont TRÈS rapides et les produits sont bons. La surreprésentation de ces termes dans le corpus nous empêche de voir les thématiques abordées de manière moins évidentes. Nous avons plusieurs solutions qui s'offrent à nous : filtrer les mots trop fréquents du corpus ou nous intéresser à une autre mesure de la fréquence d'apparition. Nous allons d'abord filtrer le corpus.

On peut aussi vouloir remplacer des termes par d'autres, comme ici "produits" par "produit".

```{r tok_replace}

tok<-tokens_replace(tok, "produits", "produit")
dfm<-dfm(tok)
textstat_frequency(dfm, n=20)

```

# Filtrer le corpus des termes trop fréquents

Nous allons filtrer les mots qui sont présents plus de 500 fois dans le corpus.

```{r 1_filter}

dfm_trim<-dfm_trim(dfm, max_termfreq = 500)

textstat_frequency(dfm_trim, n=20)
textplot_wordcloud(dfm_trim, max_words = 100, color = rev(brewer.pal(10, "Set2")))

display.brewer.all()
```

Une autre manière de s'y prendre est d'éliminer directement les termes que l'on ne veut pas voir apparaître.

```{r 1_filter2}
textstat_frequency(dfm,n=20)
rem<-c("très","rapide","produit","livraison", "commande", "bien", "site", "a", "bon", "merci", "recommande","parfait", "j'ai","tres")

dfm_rem<-dfm_remove(dfm, rem)
textstat_frequency(dfm_rem, n=20)

```

# La loi de Zipf

Vérifions la proposition de la loi de Zipf, selon laquelle la fréquence d'apparition d'un terme est inversement proportionnel à son rang.

```{r zipf_law}
zipf<-textstat_frequency(dfm)
ggplot(zipf, aes(rank, frequency))+
  geom_line(color="blue")+
  geom_point(color="darkgreen")+
   scale_x_log10() +
   scale_y_log10()+
  theme_light()+
  labs(title = "Observation de la loi de Zipf",x="log (rang)",y="log (fréquence)")
```

# Mesures de fréquence

On s'est pour l'instant intéressé uniquement aux termes les plus fréquents dans un corpus. On a vu comment éliminer les termes trop fréquents pour qu'ils nous apportent de l'information. Pour l'analyse de topics, il nous faut prendre un autre angle d'attaque : afin de détecter les sujets abordés dans un corpus, on ne peut se contenter d'observer les mots les plus fréquents, il faut s'intéresser aux termes dont la fréquence dans l'ensemble du corpus est faible, mais qui contribuent fortement à différencier les éléments du corpus entre eux (les documents). On utilise pour cela une mesure de fréquence pondérée : la *tf-idf* pour *term frequency - inverse document frequency* qui permet d'accorder plus de poids aux termes les plus discriminants du corpus. $tf-idf= \frac{occurrence\ du\ mot\ dans\ le\ document }{nombre\ de\ mots\ dans \ le \ document}* log (\frac{nombre\ de\ documents\ dans\ le\ corpus} {nombre\ de \ documents\ dans\ lequel\ le\ mot\ apparait})$

## Pondération tf-idf

On commence par reprendre nos manipulations précédentes : création de corpus, élimination des stopwords, constitution de bi- ou tri- grammes. On applique ensuite la pondération tf-idf.

```{r tf-idf, warning=FALSE}

dfmtfidf<-dfm_tfidf(dfm)

dfmtfidf


#Représentations graphiques
textplot_wordcloud(dfm, max_words = 200)
textplot_wordcloud(dfmtfidf, max_words = 200)


#On filtre les mots trop fréquents

dfm_trim<-dfm_trim(dfm, max_termfreq = 500)

dfmtfidf_trim<-dfm_tfidf(dfm_trim)

textplot_wordcloud(dfm_trim, max_words = 200)
textplot_wordcloud(dfmtfidf_trim, max_words = 200, color = brewer.pal(6, "Set2"))

display.brewer.all()

```

# Comprendre le sens des termes

On peut visualiser un ou plusieurs termes dans leur contexte, afin d'avoir une meilleure compréhension de leur sens. Pour cela on utilise la fonction "kwic" pour key word in context, à partir de l'objet tokens :

```{r 1_kwic}
head(kwic(tok,"livraison",window = 3))
# kwic(tok,"livraison",window = 3)

head(kwic(tok, c("commande", "recommande"),window = 3))
head(kwic(tok,"perroquet",window = 3))

head(kwic(tok,"prix",window=10))

```
