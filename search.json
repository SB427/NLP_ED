[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NLP avec R pour les sciences sociales",
    "section": "",
    "text": "Introduction\nCeci est le support pour le cours ‚ÄúNLP avec R pour les sciences sociales‚Äù r√©alis√© aupr√®s des doctorants de l‚Äô√©cole doctorale SHS de l‚ÄôUPJV.\nL‚Äôobjectif est de donner aux apprenants des m√©thodes et outils pour traiter de larges corpus de texte pour r√©pondre √† leur probl√©matique de recherche.\nL‚Äôenvironnement de traitement des donn√©es est R et Rstudio.\nIci, une pr√©sentation en guise d‚Äôintroduction.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#ressources-en-ligne",
    "href": "index.html#ressources-en-ligne",
    "title": "NLP avec R pour les sciences sociales",
    "section": "Ressources en ligne",
    "text": "Ressources en ligne\nR et RStudio\n\nhttps://www.r-project.org/\nhttps://rstudio.com/products/rstudio/\n\nDes ressources en ligne\n\nIntroduction √† R et au tidyverse\nR for data science\nHands-On Programming with R\nText mining with R\nTutoriels Quanteda\nLes techniques du NLP pour la recherche en sciences de gestion\nNLP avec r et en fran√ßais - un Manuel synth√©tique\nQuarto pour communiquer\nLe s√©minaire du Coll√®ge de France : Apprendre les langues aux machines - B. Sagot",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "1¬† Prise en main",
    "section": "",
    "text": "1.1 Introduction\nLe document de travail contient deux types d‚Äô√©l√©ments : du texte pour expliquer et pr√©senter ce que l‚Äôon fait et du code pour r√©aliser les manipulations de donn√©es, les analyses et les graphiques. On commence toujours un document avec un bloc de code de setup, pour lister et charger les packages que l‚Äôon va utiliser et les options g√©n√©rales pour l‚Äô√©dition du document.\nLa publication des documents peut se faire en diff√©rents formats : html, word, pdf (via latex), pr√©sentation html (via revealjs), pr√©sentation powerpoint, pr√©sentation beamer(via latex), ‚Ä¶ On se r√©f√©rera au site de Quarto pour plus de d√©tails.\nIci, on commence simplement avec quelques manipulations pour comprendre l‚Äôenvironnement de travail, puis on verra comment charger des donn√©es sous diff√©rents formats.",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Prise en main</span>"
    ]
  },
  {
    "objectID": "chapter1.html#premi√®res-manipulations",
    "href": "chapter1.html#premi√®res-manipulations",
    "title": "1¬† Prise en main",
    "section": "1.2 Premi√®res manipulations",
    "text": "1.2 Premi√®res manipulations\n\nCr√©er un document script (.R) : pour simplement √©diter du code\nCr√©er un document quarto (.qmd) : pour mixer du code et du texte\nCommenter du code : #vous permet d‚Äô√©crire un commentaire dans le code\nAfficher de l‚Äôaide sur une fonction : F1 ou lancer la ligne ?‚Äònom de la fonction‚Äô",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Prise en main</span>"
    ]
  },
  {
    "objectID": "chapter1.html#charger-des-donn√©es",
    "href": "chapter1.html#charger-des-donn√©es",
    "title": "1¬† Prise en main",
    "section": "1.3 Charger des donn√©es",
    "text": "1.3 Charger des donn√©es\n\n1.3.1 Un tableau de donn√©es\nFichier .csv, .xlsx, .rds\n\ndata&lt;-read.csv(\"le/chemin/de/mon/fichier.csv\")\n\nlibrary(readxl)\ndata&lt;-read_xlsx(\"le/chemin/de/mon/fichier.xlsx\")\n\nlibrary(readr)\ndata&lt;- read_rds(\"le/chemin/de/mon/fichier.rds\")\n\n\n\n1.3.2 Une collection de fichier textes\nUn dossier avec plusieurs fichier .txt ou .docx ou .pdf\n\nlibrary(readtext)\n#Exemple de nom de document : \"int1_2024_dirigeant.txt\"\ndata&lt;-readtext(\"le/chemin/de/mes/fichiers/*.txt\",\n         docvarsfrom = \"filenames\", \n         docvarnames = c(\"int\", \"ann√©e\", \"type\"),\n         dvsep = \"_\")\n\ndata&lt;-readtext(\"le/chemin/de/mes/fichiers/*.docx\")\n\ndata&lt;-readtext(\"le/chemin/de/mes/fichiers/*.pdf\")\n\nPour les probl√®mes de mise en forme, on consultera la vignette du package readtext.\nUne autre solution pour les fichiers pdf, permettant d‚Äôenlever tous les √©l√©ments de mise en forme :\n\nlibrary(tm)\n#on r√©cup√®re les noms des fichiers √† lire depuis les dossiers\nfiles &lt;- list.files( pattern = \"pdf$\", recursive = T, include.dirs = T)\n\n#on lit les fichiers, sans la mise en forme\ncorp&lt;-Corpus(URISource(files),\n               readerControl = list(reader = readPDF, text=(\"-layout\")))\n#on enl√®ve les sauts de page et autres mises en forme √† partir d'espace\ncorp &lt;- tm_map(corp, stripWhitespace)\n\nUn autre outil pour les pdf : le package pdftools.\n\n\n1.3.3 Reconnaissance Optique des Caract√®res (OCR)\nPour √ßa, on utilise le package tesseract :\nExemple avec cette image : \n\nlibrary(tesseract)\ntesseract_download(\"fra\") #pour t√©l√©charger le mod√®le de langage\n\ntext &lt;- tesseract::ocr(\"N1_avril1909b.jpeg\", engine = \"fra\")\n\ncat(text) #pour afficher le texte avec sa mise en page",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Prise en main</span>"
    ]
  },
  {
    "objectID": "chapter2.html",
    "href": "chapter2.html",
    "title": "2¬† Pr√©-traitements",
    "section": "",
    "text": "2.1 Introduction\nLe document de travail contient deux types d‚Äô√©l√©ments : du texte pour expliquer et pr√©senter ce que l‚Äôon fait et du code pour r√©aliser les manipulations de donn√©es, les analyses et les graphiques. On commence toujours un document avec un bloc de code de setup, pour lister et charger les packages que l‚Äôon va utiliser et les options g√©n√©rales pour l‚Äô√©dition du document.\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(quanteda)\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(RColorBrewer)\nDans un premier temps, nous allons tout simplement charger la base de donn√©es de travail puis la d√©crire. Ensuite, nous cr√©erons un corpus, le visualisons. Puis nous effectueront quelques analyses liminaires, avant de voir les pr√©-traitements √† r√©aliser sur le corpus.",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Pr√©-traitements</span>"
    ]
  },
  {
    "objectID": "chapter2.html#introduction",
    "href": "chapter2.html#introduction",
    "title": "2¬† Pr√©-traitements",
    "section": "",
    "text": "2.1.1 Les donn√©es\n\n#On charge les donn√©es, stock√©es dans un fichier csv\n\ndata &lt;- read_csv(\"data/data_trustpilot_oiseaux.csv\")\n\nRows: 4388 Columns: 7\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (4): auteur, date, month, comments\ndbl (3): id, year, note\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnames(data)\n\n[1] \"id\"       \"auteur\"   \"date\"     \"month\"    \"year\"     \"note\"     \"comments\"\n\nview(data)\ndata\n\n# A tibble: 4,388 √ó 7\n      id auteur                   date            month    year  note comments  \n   &lt;dbl&gt; &lt;chr&gt;                    &lt;chr&gt;           &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;     \n 1     1 MAQUET Cyril             07 ao√ªt 2023    ao√ªt     2023     5 \"Comme to‚Ä¶\n 2     2 Mme Laurence  Wolff      08 ao√ªt 2023    ao√ªt     2023     5 \"Le d√©lai‚Ä¶\n 3     3 Une nouvelle cliente     07 ao√ªt 2023    ao√ªt     2023     5 \"Produits‚Ä¶\n 4     4 Patricia ALLAMAN         15 ao√ªt 2023    ao√ªt     2023     5 \"Envoi ra‚Ä¶\n 5     5 VPL                      24 juillet 2023 juillet  2023     5 \"Exp√©diti‚Ä¶\n 6     6 PHILIPPE GODIN           08 ao√ªt 2023    ao√ªt     2023     5 \"site s√©r‚Ä¶\n 7     7 Mme MARIA ADILIA PEREIRA 15 ao√ªt 2023    ao√ªt     2023     1 \"deux sem‚Ä¶\n 8     8 Rachel Mattyssen         31 juillet 2023 juillet  2023     5 \"Tr√®s bie‚Ä¶\n 9     9 Estelle Fay              16 juillet 2023 juillet  2023     5 \"Enfin un‚Ä¶\n10    10 Mme T.                   06 ao√ªt 2023    ao√ªt     2023     4 \"Satisfai‚Ä¶\n# ‚Ñπ 4,378 more rows\n\n#R√©sum√© des donn√©es\nsummary(data)\n\n       id          auteur              date              month          \n Min.   :   1   Length:4388        Length:4388        Length:4388       \n 1st Qu.:1098   Class :character   Class :character   Class :character  \n Median :2194   Mode  :character   Mode  :character   Mode  :character  \n Mean   :2194                                                           \n 3rd Qu.:3291                                                           \n Max.   :4388                                                           \n      year           note         comments        \n Min.   :2013   Min.   :1.000   Length:4388       \n 1st Qu.:2018   1st Qu.:5.000   Class :character  \n Median :2020   Median :5.000   Mode  :character  \n Mean   :2019   Mean   :4.641                     \n 3rd Qu.:2021   3rd Qu.:5.000                     \n Max.   :2023   Max.   :5.000                     \n\n\n\n\n2.1.2 Premi√®res analyses/visualisations des donn√©es\nAvant de s‚Äôint√©resser au contenu des commentaires, explorons la structure des donn√©es. On va regarder la distribution des commentaires et des notes dans le temps, et s‚Äôint√©resser √† la longueur des avis clients.\n\n#Les ann√©es \ndata$year&lt;-as.factor(data$year)\nsummary(data$year)\n\n2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 \n 125  209  249  171  324  341  375  735  853  623  383 \n\ndata%&gt;%\n  group_by(year)%&gt;%\n  summarise(n=n(), prop=n/nrow(data))%&gt;%\n  ggplot(aes(year,prop))+\n  geom_col(fill=\"green\",show.legend = TRUE)+\n  scale_y_continuous(labels = scales::percent)+\n  theme_light()+\n  labs(title = \"R√©partition des avis dans le temps\", subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\", x=\"ann√©es\", y=NULL)\n\n\n\n\n\n\n\n#Les notes\nsummary(data$note)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   5.000   5.000   4.641   5.000   5.000 \n\nsummary(as.factor(data$note))\n\n   1    2    3    4    5 \n 116   62  169  589 3452 \n\ndata%&gt;%\n  group_by(note)%&gt;%\n  summarise(n=n(), prop=n/nrow(data))%&gt;%\n  ggplot(aes(note,prop))+\n  geom_col(fill=c(\"red\",\"pink\",\"orange\",\"gold\",\"lightgreen\"))+\n  annotate(\"text\", x=2, y=0.7, label=paste(\"Note moyenne = \",round(mean(data$note),1)))+\n  scale_y_continuous(labels=scales::percent)+\n  theme_light()+\n  labs(title = \"R√©partition des avis en fonction des notes\", subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\", x=\"note\", y=NULL)\n\n\n\n\n\n\n\n#Le nombre de caract√®re\ndata$nb_caractere&lt;-nchar(data$comments) #on compte le nombre de caract√®re de chaque commentaire\nsummary(data$nb_caractere)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   12.0    41.0    74.0   104.3   130.0  1571.0 \n\nggplot(data, aes(nb_caractere))+\n  geom_boxplot()+\n  scale_y_continuous(NULL, breaks = NULL)+\n  labs(x=NULL,title = \"Nombre de caract√®res des avis\", subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\")+\n  theme_light()+\n  coord_flip()\n\n\n\n\n\n\n\n#Le nombre de tokens\n\ndata$nb_token&lt;-ntoken(data$comments) #on compte le nombre de caract√®re de chaque commentaire\n\nWarning: ntoken.character()/ntype.corpus() was deprecated in quanteda 4.0.0.\n‚Ñπ Please use ntoken(tokens(x)) instead.\n\nsummary(data$nb_token)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    2.0     7.0    13.0    19.3    24.0   308.0 \n\nggplot(data, aes(nb_token))+\n  geom_boxplot()+\n  scale_y_continuous(NULL, breaks = NULL)+\n  labs(x=NULL,title = \"Nombre de tokens des avis\", subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\")+\n  theme_light()+\n  coord_flip()\n\n\n\n\n\n\n\n#On va filtrer au-dessus de 100 tokens\ndata_100t&lt;-data%&gt;%filter(nb_token&lt;50)\n  \nggplot(data_100t, aes(nb_token))+\n  geom_boxplot()+\n  scale_y_continuous(NULL, breaks = NULL)+\n  labs(x=NULL,title = \"Nombre de tokens des avis\", subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\")+\n  theme_light()+\n  coord_flip()\n\n\n\n\n\n\n\n#Les notes dans le temps\ndata%&gt;%\n  mutate(note=as.factor(note))%&gt;%\n  group_by(year, note)%&gt;%\n  summarise(n=n() ,prop=n/nrow(data))%&gt;%\n  ggplot(aes(year, prop))+\n  geom_col(aes(fill=note), show.legend = FALSE)+\n  scale_fill_discrete(type=c(\"red\",\"pink\",\"orange\",\"gold\",\"lightgreen\"))+\n  scale_y_continuous(labels=scales::percent)+\n  theme_light()+\n  labs(title = \"R√©partition des avis dans le temps\", subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\", x=\"ann√©es\", y=NULL)\n\n\n\n\n\n\n\ndata%&gt;%\n  mutate(note=as.factor(note))%&gt;%\n  group_by(year, note)%&gt;%\n  summarise(n=n())%&gt;%\n  ggplot(aes(x=year, y=n, group=note))+\n  geom_bar(position=\"fill\",stat=\"identity\", aes(fill=note))+\n  scale_fill_discrete(type=c(\"red\",\"pink\",\"orange\",\"gold\",\"lightgreen\"))+\n  scale_y_continuous(labels=scales::percent)+\n  theme_minimal()+\n  labs(title = \"Comparaison de la r√©partition des notes dans le temps\", subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\", x=\"ann√©es\", y=NULL)",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Pr√©-traitements</span>"
    ]
  },
  {
    "objectID": "chapter2.html#cr√©ation-du-corpus-et-premi√®res-observations-du-corpus",
    "href": "chapter2.html#cr√©ation-du-corpus-et-premi√®res-observations-du-corpus",
    "title": "2¬† Pr√©-traitements",
    "section": "2.2 Cr√©ation du corpus et premi√®res observations du corpus",
    "text": "2.2 Cr√©ation du corpus et premi√®res observations du corpus\nTout d‚Äôabord, nous transformons le jeu de donn√©es en corpus. La variable qui contient le texte est ‚Äúcomments‚Äù, les autres variables vont devenir des m√©tadonn√©es du corpus, c‚Äôest-√†-dire des variables associ√©es √† chaque texte. Cela sera utile par le suite pour faire des analyses comparatives entre les textes suivant diff√©rentes variables (le temps en particulier, mais pas seulement).\n\n#Cr√©ation du corpus\ncorpus_oiseaux&lt;-corpus(data, text_field = \"comments\")\ncorpus_oiseaux\n\nCorpus consisting of 4,388 documents and 8 docvars.\ntext1 :\n\"Comme toujours, super service. Ne changez rien!(sauf peut √™t...\"\n\ntext2 :\n\"Le d√©lai de ma commande super rapide Le d√©lais des ma comman...\"\n\ntext3 :\n\"Produits de qualit√© et √©quipe professionnelle ‚Ä¶ Tr√®s bon acc...\"\n\ntext4 :\n\"Envoi rapide, bien emball√© et conforme √† l'annonce\"\n\ntext5 :\n\"Exp√©dition internationale ! J'ai r√©cemment d√©m√©nag√© en Espag...\"\n\ntext6 :\n\"site s√©rieux bon produit livraisons plus que correct mais pe...\"\n\n[ reached max_ndoc ... 4,382 more documents ]\n\ncorpus_oiseaux[\"text600\"] #pour visualiser un texte pr√©cis\n\nCorpus consisting of 1 document and 8 docvars.\ntext600 :\n\"Commande re√ßu dans les temps pas de surprise. Tr√®s bon site\"\n\na&lt;-corpus_oiseaux[\"text30\"]\nrm(a)\n\nEnsuite, nous allons extraire de chaque texte les termes qui les composent. Ces termes sont nomm√©s ‚Äútoken‚Äù (jeton), et comme vous pouvez le voir, ce ne sont pas uniquement des mots, mais tout caract√®re ou suite de caract√®res s√©par√©s des autres par un espace.\n\n#Extraction des tokens\ntok&lt;-tokens(corpus_oiseaux)\ntok[\"text600\"]\n\nTokens consisting of 1 document and 8 docvars.\ntext600 :\n [1] \"Commande\" \"re√ßu\"     \"dans\"     \"les\"      \"temps\"    \"pas\"     \n [7] \"de\"       \"surprise\" \".\"        \"Tr√®s\"     \"bon\"      \"site\"    \n\n\nChaque texte est maintenant d√©compos√© en une suite de tokens. Pour voir les termes les plus fr√©quents dans le corpus, ainsi que leur co-occurrences (apparition de deux termes en m√™me temps), il convient de transformer l‚Äôobjet tok en une matrice termes-documents. En ligne, tous les tokens identifi√©s, en ligne, tous les textes du corpus, et les valeurs correspondent au nombre d‚Äôoccurrences (d‚Äôapparitions) de chaque token dans chaque document. Une particularit√© de cette matrice est qu‚Äôelle contient √©norm√©ment de z√©ro.\n\n#Transformation en document-term frequency matrix\ndfm&lt;-dfm(tok)\ndfm\n\nDocument-feature matrix of: 4,388 documents, 5,658 features (99.72% sparse) and 8 docvars.\n       features\ndocs    comme toujours , super service . ne changez rien !\n  text1     1        1 1     1       1 4  1       1    1 1\n  text2     0        1 1     2       0 3  0       0    0 0\n  text3     0        0 0     0       0 2  0       0    0 0\n  text4     0        0 1     0       0 0  0       0    0 0\n  text5     0        0 4     0       0 2  1       0    0 4\n  text6     0        0 0     0       0 0  0       0    0 0\n[ reached max_ndoc ... 4,382 more documents, reached max_nfeat ... 5,648 more features ]\n\n\nEnfin, nous pouvons avoir un aper√ßu des termes les plus fr√©quents. Nous les visualisons d‚Äôabord sous forme de tableau (les 20 tokens les plus fr√©quents), puis sous la forme d‚Äôun nuage de mots, o√π la taille des mots correspond √† leur fr√©quence dans le corpus.\n\n#Visualisation des termes les plus fr√©quents\ntextstat_frequency(dfm, n=20) #les 20 premiers termes les plus fr√©quents\n\n     feature frequency rank docfreq group\n1          .      4777    1    2094   all\n2          ,      2788    2    1576   all\n3         de      2685    3    1646   all\n4         et      2530    4    1915   all\n5       tr√®s      1996    5    1529   all\n6     rapide      1659    6    1569   all\n7         je      1622    7    1221   all\n8  livraison      1412    8    1299   all\n9          !      1349    9     649   all\n10         √†      1312   10    1022   all\n11  commande      1238   11    1030   all\n12        le      1215   12     812   all\n13        la      1215   12     868   all\n14      pour      1049   14     782   all\n15       les      1013   15     754   all\n16      bien       920   16     803   all\n17        en       801   17     618   all\n18      site       793   18     686   all\n19  produits       738   19     661   all\n20        un       729   20     565   all\n\ntextplot_wordcloud(dfm) #nuage de mots\n\n\n\n\n\n\n\n\nPour conclure sur cette premi√®re approche du corpus, nous voyons que nos analyses sont g·∫øn√©es par la pr√©sence de la ponctuation et de plein de petits mots ‚Äúvides de sens‚Äù (les articles par exemple). C‚Äôest pourquoi nous allons nettoyer le corpus pour avoir une meilleure vision de ce qu‚Äôil contient.",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Pr√©-traitements</span>"
    ]
  },
  {
    "objectID": "chapter2.html#nettoyage-du-corpus",
    "href": "chapter2.html#nettoyage-du-corpus",
    "title": "2¬† Pr√©-traitements",
    "section": "2.3 Nettoyage du corpus",
    "text": "2.3 Nettoyage du corpus\nLe nettoyage du corpus pour les analyses se fait lors de la transformation en tokens. Nous allons ajouter des options pour supprimer la ponctuation, les chiffres et les stopwords (les mots qui n‚Äôapportent pas de sens s√©mantique mais permettent l‚Äôarticulation du discours).\n\nstopwords(\"fr\")\n\n  [1] \"au\"       \"aux\"      \"avec\"     \"ce\"       \"ces\"      \"dans\"    \n  [7] \"de\"       \"des\"      \"du\"       \"elle\"     \"en\"       \"et\"      \n [13] \"eux\"      \"il\"       \"je\"       \"la\"       \"le\"       \"leur\"    \n [19] \"lui\"      \"ma\"       \"mais\"     \"me\"       \"m√™me\"     \"mes\"     \n [25] \"moi\"      \"mon\"      \"ne\"       \"nos\"      \"notre\"    \"nous\"    \n [31] \"on\"       \"ou\"       \"par\"      \"pas\"      \"pour\"     \"qu\"      \n [37] \"que\"      \"qui\"      \"sa\"       \"se\"       \"ses\"      \"son\"     \n [43] \"sur\"      \"ta\"       \"te\"       \"tes\"      \"toi\"      \"ton\"     \n [49] \"tu\"       \"un\"       \"une\"      \"vos\"      \"votre\"    \"vous\"    \n [55] \"c\"        \"d\"        \"j\"        \"l\"        \"√†\"        \"m\"       \n [61] \"n\"        \"s\"        \"t\"        \"y\"        \"√©t√©\"      \"√©t√©e\"    \n [67] \"√©t√©es\"    \"√©t√©s\"     \"√©tant\"    \"suis\"     \"es\"       \"est\"     \n [73] \"sommes\"   \"√™tes\"     \"sont\"     \"serai\"    \"seras\"    \"sera\"    \n [79] \"serons\"   \"serez\"    \"seront\"   \"serais\"   \"serait\"   \"serions\" \n [85] \"seriez\"   \"seraient\" \"√©tais\"    \"√©tait\"    \"√©tions\"   \"√©tiez\"   \n [91] \"√©taient\"  \"fus\"      \"fut\"      \"f√ªmes\"    \"f√ªtes\"    \"furent\"  \n [97] \"sois\"     \"soit\"     \"soyons\"   \"soyez\"    \"soient\"   \"fusse\"   \n[103] \"fusses\"   \"f√ªt\"      \"fussions\" \"fussiez\"  \"fussent\"  \"ayant\"   \n[109] \"eu\"       \"eue\"      \"eues\"     \"eus\"      \"ai\"       \"as\"      \n[115] \"avons\"    \"avez\"     \"ont\"      \"aurai\"    \"auras\"    \"aura\"    \n[121] \"aurons\"   \"aurez\"    \"auront\"   \"aurais\"   \"aurait\"   \"aurions\" \n[127] \"auriez\"   \"auraient\" \"avais\"    \"avait\"    \"avions\"   \"aviez\"   \n[133] \"avaient\"  \"eut\"      \"e√ªmes\"    \"e√ªtes\"    \"eurent\"   \"aie\"     \n[139] \"aies\"     \"ait\"      \"ayons\"    \"ayez\"     \"aient\"    \"eusse\"   \n[145] \"eusses\"   \"e√ªt\"      \"eussions\" \"eussiez\"  \"eussent\"  \"ceci\"    \n[151] \"cela\"     \"cel√†\"     \"cet\"      \"cette\"    \"ici\"      \"ils\"     \n[157] \"les\"      \"leurs\"    \"quel\"     \"quels\"    \"quelle\"   \"quelles\" \n[163] \"sans\"     \"soi\"     \n\ntok&lt;-tokens(corpus_oiseaux, remove_punct = TRUE, remove_numbers = TRUE)%&gt;%\n  tokens_remove(stopwords(\"fr\"))\ncorpus_oiseaux[\"text600\"]\n\nCorpus consisting of 1 document and 8 docvars.\ntext600 :\n\"Commande re√ßu dans les temps pas de surprise. Tr√®s bon site\"\n\ntok[\"text600\"]\n\nTokens consisting of 1 document and 8 docvars.\ntext600 :\n[1] \"Commande\" \"re√ßu\"     \"temps\"    \"surprise\" \"Tr√®s\"     \"bon\"      \"site\"    \n\n\nEnsuite, on transforme en dfm et on visualise ce que √ßa donne.\n\ndfm&lt;-dfm(tok)\ndfm\n\nDocument-feature matrix of: 4,388 documents, 5,430 features (99.81% sparse) and 8 docvars.\n       features\ndocs    comme toujours super service changez rien sauf peut √™tre l√†\n  text1     1        1     1       1       1    1    1    1    1  1\n  text2     0        1     2       0       0    0    0    0    0  1\n  text3     0        0     0       0       0    0    0    0    0  0\n  text4     0        0     0       0       0    0    0    0    0  0\n  text5     0        0     0       0       0    0    0    0    0  0\n  text6     0        0     0       0       0    0    0    0    0  0\n[ reached max_ndoc ... 4,382 more documents, reached max_nfeat ... 5,420 more features ]\n\ntextstat_frequency(dfm, n=20)\n\n      feature frequency rank docfreq group\n1        tr√®s      1996    1    1529   all\n2      rapide      1659    2    1569   all\n3   livraison      1412    3    1299   all\n4    commande      1238    4    1030   all\n5        bien       920    5     803   all\n6        site       793    6     686   all\n7    produits       738    7     661   all\n8           a       706    8     566   all\n9     produit       676    9     609   all\n10        bon       639   10     575   all\n11      merci       553   11     529   all\n12 recommande       553   11     534   all\n13    parfait       522   13     474   all\n14    qualit√©       487   14     447   all\n15       j'ai       382   15     289   all\n16    oiseaux       379   16     319   all\n17       tout       375   17     347   all\n18       re√ßu       362   18     332   all\n19      colis       360   19     313   all\n20 rapidement       356   20     343   all\n\ntextplot_wordcloud(dfm)\n\n\n\n\n\n\n\ng&lt;-textstat_frequency(dfm,n=20)\n\n\nggplot(g, aes(x = feature, y=frequency))+\n  geom_col()+\n  coord_flip()\n\n\n\n\n\n\n\nggplot(g, aes(x = reorder(feature, frequency), y=frequency, fill=frequency))+\n  geom_col(show.legend = TRUE)+\n  coord_flip()+\n  theme_light()+\n  scale_fill_distiller(palette = \"Blues\", direction = 1)+\n  labs(title=\"Les mots les plus fr√©quents\",subtitle = \"du corpus Oiseaux Mania\",caption = \"Source : Data TrustPilot\")+\n  xlab(NULL)+\n  ylab(\"Fr√©quence\")\n\n\n\n\n\n\n\ndisplay.brewer.all()\n\n\n\n\n\n\n\n\nGlobalement, la commande et la livraison sont TR√àS rapides et les produits sont bons. La surrepr√©sentation de ces termes dans le corpus nous emp√™che de voir les th√©matiques abord√©es de mani√®re moins √©videntes. Nous avons plusieurs solutions qui s‚Äôoffrent √† nous : filtrer les mots trop fr√©quents du corpus ou nous int√©resser √† une autre mesure de la fr√©quence d‚Äôapparition. Nous allons d‚Äôabord filtrer le corpus.\nOn peut aussi vouloir remplacer des termes par d‚Äôautres, comme ici ‚Äúproduits‚Äù par ‚Äúproduit‚Äù.\n\ntok&lt;-tokens_replace(tok, \"produits\", \"produit\")\ndfm&lt;-dfm(tok)\ntextstat_frequency(dfm, n=20)\n\n      feature frequency rank docfreq group\n1        tr√®s      1996    1    1529   all\n2      rapide      1659    2    1569   all\n3     produit      1414    3    1243   all\n4   livraison      1412    4    1299   all\n5    commande      1238    5    1030   all\n6        bien       920    6     803   all\n7        site       793    7     686   all\n8           a       706    8     566   all\n9         bon       639    9     575   all\n10      merci       553   10     529   all\n11 recommande       553   10     534   all\n12    parfait       522   12     474   all\n13    qualit√©       487   13     447   all\n14       j'ai       382   14     289   all\n15    oiseaux       379   15     319   all\n16       tout       375   16     347   all\n17       re√ßu       362   17     332   all\n18      colis       360   18     313   all\n19 rapidement       356   19     343   all\n20    service       351   20     325   all",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Pr√©-traitements</span>"
    ]
  },
  {
    "objectID": "chapter2.html#filtrer-le-corpus-des-termes-trop-fr√©quents",
    "href": "chapter2.html#filtrer-le-corpus-des-termes-trop-fr√©quents",
    "title": "2¬† Pr√©-traitements",
    "section": "2.4 Filtrer le corpus des termes trop fr√©quents",
    "text": "2.4 Filtrer le corpus des termes trop fr√©quents\nNous allons filtrer les mots qui sont pr√©sents plus de 500 fois dans le corpus.\n\ndfm_trim&lt;-dfm_trim(dfm, max_termfreq = 500)\n\ntextstat_frequency(dfm_trim, n=20)\n\n      feature frequency rank docfreq group\n1     qualit√©       487    1     447   all\n2        j'ai       382    2     289   all\n3     oiseaux       379    3     319   all\n4        tout       375    4     347   all\n5        re√ßu       362    5     332   all\n6       colis       360    6     313   all\n7  rapidement       356    7     343   all\n8     service       351    8     325   all\n9        prix       327    9     307   all\n10 satisfaite       327    9     301   all\n11      super       326   11     294   all\n12   conforme       326   11     307   all\n13       rien       317   13     299   all\n14      bonne       293   14     277   all\n15       plus       292   15     254   all\n16       tres       278   16     216   all\n17   toujours       274   17     236   all\n18      mania       256   18     228   all\n19      envoi       239   19     231   all\n20    s√©rieux       232   20     220   all\n\ntextplot_wordcloud(dfm_trim, max_words = 100, color = rev(brewer.pal(10, \"Set2\")))\n\nWarning in brewer.pal(10, \"Set2\"): n too large, allowed maximum for palette Set2 is 8\nReturning the palette you asked for with that many colors\n\n\n\n\n\n\n\n\ndisplay.brewer.all()\n\n\n\n\n\n\n\n\nUne autre mani√®re de s‚Äôy prendre est d‚Äô√©liminer directement les termes que l‚Äôon ne veut pas voir appara√Ætre.\n\ntextstat_frequency(dfm,n=20)\n\n      feature frequency rank docfreq group\n1        tr√®s      1996    1    1529   all\n2      rapide      1659    2    1569   all\n3     produit      1414    3    1243   all\n4   livraison      1412    4    1299   all\n5    commande      1238    5    1030   all\n6        bien       920    6     803   all\n7        site       793    7     686   all\n8           a       706    8     566   all\n9         bon       639    9     575   all\n10      merci       553   10     529   all\n11 recommande       553   10     534   all\n12    parfait       522   12     474   all\n13    qualit√©       487   13     447   all\n14       j'ai       382   14     289   all\n15    oiseaux       379   15     319   all\n16       tout       375   16     347   all\n17       re√ßu       362   17     332   all\n18      colis       360   18     313   all\n19 rapidement       356   19     343   all\n20    service       351   20     325   all\n\nrem&lt;-c(\"tr√®s\",\"rapide\",\"produit\",\"livraison\", \"commande\", \"bien\", \"site\", \"a\", \"bon\", \"merci\", \"recommande\",\"parfait\", \"j'ai\",\"tres\")\n\ndfm_rem&lt;-dfm_remove(dfm, rem)\ntextstat_frequency(dfm_rem, n=20)\n\n      feature frequency rank docfreq group\n1     qualit√©       487    1     447   all\n2     oiseaux       379    2     319   all\n3        tout       375    3     347   all\n4        re√ßu       362    4     332   all\n5       colis       360    5     313   all\n6  rapidement       356    6     343   all\n7     service       351    7     325   all\n8        prix       327    8     307   all\n9  satisfaite       327    8     301   all\n10      super       326   10     294   all\n11   conforme       326   10     307   all\n12       rien       317   12     299   all\n13      bonne       293   13     277   all\n14       plus       292   14     254   all\n15   toujours       274   15     236   all\n16      mania       256   16     228   all\n17      envoi       239   17     231   all\n18    s√©rieux       232   18     220   all\n19      choix       222   19     205   all\n20  satisfait       220   20     205   all",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Pr√©-traitements</span>"
    ]
  },
  {
    "objectID": "chapter2.html#la-loi-de-zipf",
    "href": "chapter2.html#la-loi-de-zipf",
    "title": "2¬† Pr√©-traitements",
    "section": "2.5 La loi de Zipf",
    "text": "2.5 La loi de Zipf\nV√©rifions la proposition de la loi de Zipf, selon laquelle la fr√©quence d‚Äôapparition d‚Äôun terme est inversement proportionnel √† son rang.\n\nzipf&lt;-textstat_frequency(dfm)\nggplot(zipf, aes(rank, frequency))+\n  geom_line(color=\"blue\")+\n  geom_point(color=\"darkgreen\")+\n   scale_x_log10() +\n   scale_y_log10()+\n  theme_light()+\n  labs(title = \"Observation de la loi de Zipf\",x=\"log (rang)\",y=\"log (fr√©quence)\")",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Pr√©-traitements</span>"
    ]
  },
  {
    "objectID": "chapter2.html#mesures-de-fr√©quence",
    "href": "chapter2.html#mesures-de-fr√©quence",
    "title": "2¬† Pr√©-traitements",
    "section": "2.6 Mesures de fr√©quence",
    "text": "2.6 Mesures de fr√©quence\nOn s‚Äôest pour l‚Äôinstant int√©ress√© uniquement aux termes les plus fr√©quents dans un corpus. On a vu comment √©liminer les termes trop fr√©quents pour qu‚Äôils nous apportent de l‚Äôinformation. Pour l‚Äôanalyse de topics, il nous faut prendre un autre angle d‚Äôattaque : afin de d√©tecter les sujets abord√©s dans un corpus, on ne peut se contenter d‚Äôobserver les mots les plus fr√©quents, il faut s‚Äôint√©resser aux termes dont la fr√©quence dans l‚Äôensemble du corpus est faible, mais qui contribuent fortement √† diff√©rencier les √©l√©ments du corpus entre eux (les documents). On utilise pour cela une mesure de fr√©quence pond√©r√©e : la tf-idf pour term frequency - inverse document frequency qui permet d‚Äôaccorder plus de poids aux termes les plus discriminants du corpus. \\(tf-idf= \\frac{occurrence\\ du\\ mot\\ dans\\ le\\ document }{nombre\\ de\\ mots\\ dans \\ le \\ document}* log (\\frac{nombre\\ de\\ documents\\ dans\\ le\\ corpus} {nombre\\ de \\ documents\\ dans\\ lequel\\ le\\ mot\\ apparait})\\)\n\n2.6.1 Pond√©ration tf-idf\nOn commence par reprendre nos manipulations pr√©c√©dentes : cr√©ation de corpus, √©limination des stopwords, constitution de bi- ou tri- grammes. On applique ensuite la pond√©ration tf-idf.\n\ndfmtfidf&lt;-dfm_tfidf(dfm)\n\ndfmtfidf\n\nDocument-feature matrix of: 4,388 documents, 5,429 features (99.81% sparse) and 8 docvars.\n       features\ndocs       comme toujours    super  service  changez     rien     sauf     peut\n  text1 1.406738 1.269355 1.173919 1.130383 3.040207 1.166595 2.320047 1.926263\n  text2 0        1.269355 2.347839 0        0        0        0        0       \n  text3 0        0        0        0        0        0        0        0       \n  text4 0        0        0        0        0        0        0        0       \n  text5 0        0        0        0        0        0        0        0       \n  text6 0        0        0        0        0        0        0        0       \n       features\ndocs        √™tre       l√†\n  text1 1.961025 2.320047\n  text2 0        2.320047\n  text3 0        0       \n  text4 0        0       \n  text5 0        0       \n  text6 0        0       \n[ reached max_ndoc ... 4,382 more documents, reached max_nfeat ... 5,419 more features ]\n\n#Repr√©sentations graphiques\ntextplot_wordcloud(dfm, max_words = 200)\n\n\n\n\n\n\n\ntextplot_wordcloud(dfmtfidf, max_words = 200)\n\n\n\n\n\n\n\n#On filtre les mots trop fr√©quents\n\ndfm_trim&lt;-dfm_trim(dfm, max_termfreq = 500)\n\ndfmtfidf_trim&lt;-dfm_tfidf(dfm_trim)\n\ntextplot_wordcloud(dfm_trim, max_words = 200)\n\n\n\n\n\n\n\ntextplot_wordcloud(dfmtfidf_trim, max_words = 200, color = brewer.pal(6, \"Set2\"))\n\n\n\n\n\n\n\ndisplay.brewer.all()",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Pr√©-traitements</span>"
    ]
  },
  {
    "objectID": "chapter2.html#comprendre-le-sens-des-termes",
    "href": "chapter2.html#comprendre-le-sens-des-termes",
    "title": "2¬† Pr√©-traitements",
    "section": "2.7 Comprendre le sens des termes",
    "text": "2.7 Comprendre le sens des termes\nOn peut visualiser un ou plusieurs termes dans leur contexte, afin d‚Äôavoir une meilleure compr√©hension de leur sens. Pour cela on utilise la fonction ‚Äúkwic‚Äù pour key word in context, √† partir de l‚Äôobjet tokens :\n\nhead(kwic(tok,\"livraison\",window = 3))\n\nKeyword-in-context with 6 matches.                                                     \n  [text2, 12] revanche n‚Äôarrive cliquer | livraison |\n  [text3, 13]    depuis conseil jusqu'√† | livraison |\n  [text5, 36]  perfection puisque d√©lai | livraison |\n  [text8, 14]    a enti√®re satisfaction | livraison |\n  [text14, 1]                           | Livraison |\n [text15, 21]         trouve peut frais | livraison |\n                            \n adresse toujours l√†        \n produit recommande vivement\n Espagne depuis France      \n aussi a rapide             \n rapide soign√©e Bons        \n √©lev√©                      \n\n# kwic(tok,\"livraison\",window = 3)\n\nhead(kwic(tok, c(\"commande\", \"recommande\"),window = 3))\n\nKeyword-in-context with 6 matches.                                                                            \n  [text2, 2]                     d√©lai |  commande  | super rapide d√©lais   \n  [text2, 6]       super rapide d√©lais |  commande  | super rapide revanche \n [text3, 15] jusqu'√† livraison produit | recommande | vivement oiseaux Mania\n [text5, 14]   Food regardant derni√®re |  commande  | j'avais faite France  \n  [text7, 5]       semaines j'ai pass√© |  commande  | toujours livr√© √©toile \n  [text8, 9]       j'ai n√©anmoins fait |  commande  | difficult√© a enti√®re  \n\nhead(kwic(tok,\"perroquet\",window = 3))\n\nKeyword-in-context with 6 matches.                                                      \n  [text5, 51]    d'avoir trouv√© solution | perroquet |\n  [text18, 8]        comp√©tent sers site | perroquet |\n [text22, 18]         convient tout fait | perroquet |\n  [text34, 6] treats granul√©s compress√©s | Perroquet |\n [text37, 10]        choix produit petit | perroquet |\n  [text39, 2]               Alimentation | perroquet |\n                              \n afin qu'il puisse            \n  üòä Excellent rapide          \n tr√®s heureux aussi           \n Gris Gabon tr√®s              \n n'h√©siterai refaire commandes\n commande depuis quelques     \n\nhead(kwic(tok,\"prix\",window=10))\n\nKeyword-in-context with 6 matches.             \n  [text14, 6]\n [text15, 15]\n [text26, 23]\n [text28, 12]\n [text31, 77]\n  [text32, 7]\n                                                                                     \n                                                Livraison rapide soign√©e Bons produit\n            rapide satisfait commander cher oiseaux mania car Besan√ßon trouve produit\n beaucoup choix graines friandises accessoires commande a livr√©e rapidement encombres\n      harnais re√ßus d√©plore juste l‚Äôemballage spartiate arriv√© moiti√© d√©chir√© rapport\n                           flacons 57gr chacun √ßa fait peut ch√®re Mondial relay offre\n                                           Livraison rapide Bons produit bien emball√©\n                                                          \n | prix | raisonnables                                    \n | Prix | tr√®s attractif trouve peut frais livraison √©lev√©\n | Prix | int√©ressant                                     \n | prix | vente c‚Äôest peu l√©ger surtout harnais           \n | prix | bien plus raisonnable Bien cordialement         \n | prix | raisonnables",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Pr√©-traitements</span>"
    ]
  },
  {
    "objectID": "chapter3.html",
    "href": "chapter3.html",
    "title": "3¬† Analyse du sentiment",
    "section": "",
    "text": "3.1 Les donn√©es\ndata &lt;- read_csv(\"data/data_trustpilot_oiseaux.csv\")",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Analyse du sentiment</span>"
    ]
  },
  {
    "objectID": "chapter3.html#sentiment-analysis",
    "href": "chapter3.html#sentiment-analysis",
    "title": "3¬† Analyse du sentiment",
    "section": "3.2 Sentiment analysis",
    "text": "3.2 Sentiment analysis\nOn va r√©aliser une analyse du sentiment du corpus. Pour cela, on utilise le dictionnaire des sentiments et √©motions NRC, car il est disponible dans 40 langues, dont le fran√ßais. Il existe d‚Äôautres dictionnaires de sentiments (positif vs n√©gatif), par exemple AFINN ou BING, ainsi que des dictionnaires th√©matiques (LIWC par exemple), mais ils sont en anglais ou payants, donc utilisables pour des corpus en anglais ou lorsqu‚Äôon dispose d‚Äôun budget. Les derni√®res √©volutions du traitement en langage naturel des IA (transformers et autres, dont ChatGPT est un exemple), permettent d‚Äôautres approches tr√®s pertinentes, en utilisant le machine learning, mais cela va au-del√† des objectifs de ce cours.\nIci, on utilise le dictionnaire NRC √† travers le package syuzhet. La fonction get_nrc_sentiment prend en entr√©e un vecteur de type caract√®re.\n\nlibrary(syuzhet)\n\n# d&lt;-get_nrc_sentiment(data$comments, language = \"french\")\n# write_rds(d, \"sentiment_trustpilot_oiseaux.rds\")\n\nd&lt;-read_rds(\"data/sentiment_trustpilot_oiseaux.rds\")\nsummary(d, digits=1)\n\n     anger      anticipation     disgust         fear          joy     \n Min.   :0.0   Min.   : 0.0   Min.   :0.0   Min.   :0.0   Min.   :0.0  \n 1st Qu.:0.0   1st Qu.: 0.0   1st Qu.:0.0   1st Qu.:0.0   1st Qu.:0.0  \n Median :0.0   Median : 1.0   Median :0.0   Median :0.0   Median :0.0  \n Mean   :0.1   Mean   : 0.9   Mean   :0.1   Mean   :0.1   Mean   :0.6  \n 3rd Qu.:0.0   3rd Qu.: 1.0   3rd Qu.:0.0   3rd Qu.:0.0   3rd Qu.:1.0  \n Max.   :7.0   Max.   :10.0   Max.   :7.0   Max.   :6.0   Max.   :6.0  \n    sadness       surprise       trust        negative       positive \n Min.   :0.0   Min.   :0.0   Min.   :0.0   Min.   : 0.0   Min.   : 0  \n 1st Qu.:0.0   1st Qu.:0.0   1st Qu.:0.0   1st Qu.: 0.0   1st Qu.: 1  \n Median :0.0   Median :1.0   Median :1.0   Median : 0.0   Median : 2  \n Mean   :0.2   Mean   :0.7   Mean   :0.9   Mean   : 0.4   Mean   : 2  \n 3rd Qu.:0.0   3rd Qu.:1.0   3rd Qu.:1.0   3rd Qu.: 0.0   3rd Qu.: 3  \n Max.   :7.0   Max.   :5.0   Max.   :8.0   Max.   :12.0   Max.   :14  \n\ndata&lt;-cbind(data,d)\n\ndata[600,8:17]\n\n    anger anticipation disgust fear joy sadness surprise trust negative\n600     0            1       0    1   1       0        1     1        0\n    positive\n600        1\n\ndata[600,\"comments\"]\n\n[1] \"Commande re√ßu dans les temps pas de surprise. Tr√®s bon site\"\n\n\nLe dictionnaire comprend 10 variables, 8 √©motions et 2 sentiments. Pour repr√©senter les donn√©es, nous avons besoin de les transformer.\n\ne&lt;-d%&gt;%\n  pivot_longer(everything(),names_to = \"sentiment\", values_to = \"nb\")\n\nggplot(e, aes(sentiment, nb))+\n  geom_col(aes(fill=sentiment),show.legend = FALSE)+\n  theme_minimal()+\n  coord_flip()\n\n\n\n\n\n\n\n\n\n3.2.1 Les sentiments\nInt√©ressons-nous d‚Äôabord aux sentiments :\n\nsent&lt;-d%&gt;%\n  select(positive,negative)%&gt;%\n  pivot_longer(everything(), names_to = \"sentiment\", values_to = \"nb\")%&gt;%\n  summarise(nb=sum(nb), .by = sentiment)%&gt;%\n  mutate(prop=nb/sum(nb))\n\nggplot(data=sent,  aes(x=sentiment, y=prop)) + \n  geom_bar(stat=\"identity\", aes(fill=sentiment), show.legend = FALSE)+\n  scale_y_continuous(labels=scales::percent) +\n  labs(title = \"R√©partition des sentiments dans le corpus Oiseaux Mania\",caption = \"Donn√©es TrustPilot\",x=\"Sentiments\",y=NULL)+\n  scale_fill_manual(values=c(\"red\", \"lightgreen\"))+\n  theme_light()\n\n\n\n\n\n\n\n\nLe corpus est tr√®s largement positif, ce qui n‚Äôest pas √©tonnant. On peut aussi cr√©er d‚Äôautres indicateurs, comme la valence (diff√©rence positif-n√©gatif) ou l‚Äôexpressivit√© (somme de positif+n√©gatif).\nExercice : personnalisez le graphique ci-dessous pour la variable d‚Äôexpressivit√©.\n\ndata&lt;-data%&gt;%\n  mutate(nbcar=nchar(comments),\n         valence = positive-negative)\n\nggplot(data = data, aes(x = valence, y= nbcar))+\n  geom_point()+\n  geom_smooth()+\n  labs(title=\"Indicateur de valence\", subtitle = \"en fonction du nombre de caract√®res\", caption=\"Donn√©es TrustPilot\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\nSolution :\n\n\nSolution\ndata&lt;-data%&gt;%\n  mutate(nbcar=nchar(comments),\n         expressivit√© = positive+negative)\n\nggplot(data = data, aes(x = expressivit√©, y= nbcar))+\n  geom_point()+\n  geom_smooth()+\n  labs(title=\"Indicateur d'expressivit√©\", subtitle = \"en fonction du nombre de caract√®res\", caption=\"Donn√©es TrustPilot\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n3.2.2 Les √©motions\nRegardons maintenant ce qu‚Äôil en est de la r√©partition des √©motions :\n\n#On cr√©e d'abord une palette pour les √©motions\nemocol&lt;-c(\"yellow\",\"chartreuse\",\"olivedrab3\",\"green4\",\"royalblue3\",\"purple3\",\"red3\",\"orangered1\") \n\n\nemo&lt;-d%&gt;%\n  select(-positive, -negative)%&gt;% #On r√©cup√®re les √©motions\n  pivot_longer(everything(), names_to = \"emotion\", values_to = \"nb\")#On transforme le tableau\n \nemo2&lt;-emo%&gt;%\n  summarise(nb=sum(nb), .by = emotion)%&gt;%\n  mutate(prop=nb/sum(nb),\n          emotion=factor(emotion, ordered = TRUE,levels = c(\"joy\",\"trust\",\"fear\",\"surprise\",\"sadness\",\"disgust\",\"anger\",\"anticipation\")))\n\n#On cr√©e un graphique circulaire\nggplot(data=emo2,  aes(x=emotion, y=prop, colour=emotion)) + \n  geom_bar(stat=\"identity\", aes(fill=emotion), show.legend = FALSE)+ \n  scale_y_continuous(labels=scales::percent)+\n  labs(title=\"Distribution des √©motions \\n dans le corpus Oiseaux Mania\", caption=\"Donn√©es TrustPilot\", x=\"Emotions\", y=NULL) +\n  coord_polar()+ \n  scale_color_manual(values=emocol)+ scale_fill_manual(values=emocol)+\n  theme_minimal()\n\n\n\n\n\n\n\n#On regarde la r√©partition des √©motions dans le corpus :\n\nggplot(emo, aes(x=emotion, y=nb))+\n  geom_violin(aes(fill=emotion), alpha=0.7,adjust = 2)+\n  theme_minimal()+ scale_fill_manual(values=emocol)+\n  scale_x_discrete(labels=NULL)\n\n\n\n\n\n\n\nggplot(emo, aes(x=emotion, y=nb))+\n  geom_boxplot(aes(fill=emotion,), alpha=0.7,adjust = 2, show.legend = FALSE)+\n  theme_minimal()+ scale_fill_manual(values=emocol)",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Analyse du sentiment</span>"
    ]
  },
  {
    "objectID": "chapter3.html#√©volution-du-corpus-dans-le-temps",
    "href": "chapter3.html#√©volution-du-corpus-dans-le-temps",
    "title": "3¬† Analyse du sentiment",
    "section": "3.3 √âvolution du corpus dans le temps",
    "text": "3.3 √âvolution du corpus dans le temps\nOn va regarder comment les sentiments √©voluent dans le temps. On doit tout d‚Äôabord cr√©er une variable temporel dans notre jeu de donn√©es. Nous en avons d√©j√† une, qui indique la date et l‚Äôheure √† laquelle le commentaire a √©t√© post√©. Nous allons la transformer pour regrouper les commentaires en fonction de l‚Äôann√©e (on peut le faire pour les jours, les mois, les minutes, ‚Ä¶).\n\ndata%&gt;%group_by(year)%&gt;%summarise('nb com'=n())\n\n# A tibble: 11 √ó 2\n    year `nb com`\n   &lt;dbl&gt;    &lt;int&gt;\n 1  2013      125\n 2  2014      209\n 3  2015      249\n 4  2016      171\n 5  2017      324\n 6  2018      341\n 7  2019      375\n 8  2020      735\n 9  2021      853\n10  2022      623\n11  2023      383\n\n\nRegardons maintenant comment √©volue les sentiments dans le temps :\n\n#les √©motions\n##mise en forme des donn√©es\ngen_sent&lt;-data%&gt;%\n  mutate(year=as.factor(year))%&gt;%\n  group_by(year)%&gt;%\n  summarise(across(7:14,~mean(.x, na.rm = T)))%&gt;%\n  na.exclude()%&gt;%\n  pivot_longer(-year, names_to = \"emotion\", values_to = \"mean\")%&gt;%\n  mutate(emotion=factor(emotion, ordered = TRUE,levels = c(\"joy\",\"trust\",\"fear\",\"surprise\",\"sadness\",\"disgust\",\"anger\",\"anticipation\")))\n\n##graphique\nggplot(gen_sent,aes(x=year, y=mean,group=emotion)) +\n  geom_line(aes(color=emotion), linewidth=0.5) + \n  theme_minimal()+\n  scale_color_manual(values = emocol)\n\n\n\n\n\n\n\n#les sentiments\n##mise en forme des donn√©es\ngen_sent2&lt;-data%&gt;%\n  mutate(year=as.factor(year))%&gt;%\n  group_by(year)%&gt;%\n  summarise(across(c(positive, negative),~mean(.x, na.rm = T)))%&gt;%\n  na.exclude()%&gt;%\n  pivot_longer(-year, names_to = \"sentiment\",values_to = \"mean\")\n\n##graphique\nggplot(gen_sent2,aes(x=year, y=mean,group=sentiment)) +\n  geom_line(aes(color=sentiment), linewidth=0.5) + \n  theme_minimal()+\n  scale_color_manual(values = c(\"red\",\"lightgreen\"))\n\n\n\n\n\n\n\n\nMaintenant, on va s‚Äôint√©resser aux mots.",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Analyse du sentiment</span>"
    ]
  },
  {
    "objectID": "chapter3.html#nuage-de-mots-compar√©s",
    "href": "chapter3.html#nuage-de-mots-compar√©s",
    "title": "3¬† Analyse du sentiment",
    "section": "3.4 Nuage de mots compar√©s",
    "text": "3.4 Nuage de mots compar√©s\n\n3.4.1 En fonction des ann√©es\nOn refait les manipulations pr√©liminaires :\n\ndata&lt;-data%&gt;%\n  mutate(year2=case_when(year&lt;2017~\"2013-2016\",\n                         year %in% c(2017:2019)~\"2017-2019\",\n                         .default=as.character(year)))\n\ncorpus_oiseaux&lt;-corpus(data, text_field = \"comments\")\n\ntok&lt;-tokens(corpus_oiseaux, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE)%&gt;%\n  tokens_remove(stopwords(\"fr\"))\n\ndfm&lt;-dfm(tok)\n\nComparons les mots en fonction des ann√©es :\n\ndfmgp&lt;-dfm_group(dfm, groups = year2)\ndfmgp\n\nDocument-feature matrix of: 6 documents, 5,392 features (66.69% sparse) and 1 docvar.\n           features\ndocs        comme toujours super service changez rien sauf peut √™tre l√†\n  2013-2016    38       26    93      58       0   71    6   17    9  8\n  2017-2019    32       61    74      78       0   76    5   13   24  4\n  2020         27       38    42      65       0   51    0    5    6  1\n  2021         35       49    49      64       2   62    8   11   11  2\n  2022         36       49    46      59       0   36    0    4    3  3\n  2023         16       51    22      27       2   21    3    6    3  4\n[ reached max_nfeat ... 5,382 more features ]\n\n#On peut aussi passer la fonction directement en transformant en dfm avec l'option groups : dfm(tok, groups=\"year\")\n\ntextplot_wordcloud(dfmgp, comparison=TRUE, max_words = 200)\n\n\n\n\n\n\n\n\n\n\n3.4.2 En fonction des sentiments\nPour comparer en fonction des sentiments, il faut acc√©der au dictionnaire NRC (en fran√ßais) (il y a des fonctions simplifi√©es pour les dictionnaires en anglais) :\n\ndic_nrc&lt;-read_xlsx(\"NRCfr.xlsx\")%&gt;%\n  pivot_longer(-word,names_to = \"sentiment\", values_to=\"value\")%&gt;%\n  filter(value==1, word!=\"NO TRANSLATION\")%&gt;%\n  select(-value)\n\nsent_term&lt;-convert(dfm,to=\"data.frame\")%&gt;%\n  select(-doc_id)%&gt;%\n  pivot_longer(everything(), names_to=\"word\", values_to=\"value\")%&gt;%\n  filter(value!=0)%&gt;%\n  summarise(value=sum(value), .by=word)%&gt;%\n  inner_join(dic_nrc)%&gt;%\n  slice_max(n=10, by=sentiment, order_by = value, with_ties=F)\n\n\nggplot(sent_term, aes(reorder(word, value), value, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~sentiment, scales = \"free_y\") +\n  labs(y = \"Contribution to sentiment\",\n       x = NULL) +\n  coord_flip()+\n  theme_minimal()",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Analyse du sentiment</span>"
    ]
  },
  {
    "objectID": "chapter4.html",
    "href": "chapter4.html",
    "title": "4¬† Annotations et d√©pendances syntaxiques",
    "section": "",
    "text": "4.1 Les donn√©es\ndata &lt;- read_csv(\"data/data_trustpilot_oiseaux.csv\")\n\nRows: 4388 Columns: 7\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (4): auteur, date, month, comments\ndbl (3): id, year, note\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata$nb_caractere&lt;-nchar(data$comments) #on compte le nombre de caract√®re de chaque commentaire\nsummary(data$nb_caractere)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   12.0    41.0    74.0   104.3   130.0  1571.0 \n\nmean(data$nb_caractere)\n\n[1] 104.2974\n\nmedian(data$nb_caractere)\n\n[1] 74\n\nround(mean(data$nb_caractere),1)\n\n[1] 104.3\n\nmoy&lt;-round(mean(na.omit(data$nb_caractere)), 1)\n\nggplot(data)+\n  geom_boxplot(aes(nb_caractere))+\n  geom_text(aes(x=500, y=0.2,label=paste(\"Moyenne :\",moy)))+\n  coord_flip()+\n  scale_y_continuous(NULL, breaks = NULL)+\n  theme_minimal()",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Annotations et d√©pendances syntaxiques</span>"
    ]
  },
  {
    "objectID": "chapter4.html#les-traitements-pr√©liminaires",
    "href": "chapter4.html#les-traitements-pr√©liminaires",
    "title": "4¬† Annotations et d√©pendances syntaxiques",
    "section": "4.2 Les traitements pr√©liminaires",
    "text": "4.2 Les traitements pr√©liminaires\nOn reprend ce qu‚Äôon a fait au cours dernier, sans √©liminer les termes trop fr√©quents :\n\ncorpus_oiseaux&lt;-corpus(data, text_field = \"comments\")\n\ntok&lt;-tokens(corpus_oiseaux, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE)%&gt;%\n  tokens_remove(stopwords(\"fr\"))\n\ndfm&lt;-dfm(tok)",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Annotations et d√©pendances syntaxiques</span>"
    ]
  },
  {
    "objectID": "chapter4.html#co-occurrences",
    "href": "chapter4.html#co-occurrences",
    "title": "4¬† Annotations et d√©pendances syntaxiques",
    "section": "4.3 Co-occurrences",
    "text": "4.3 Co-occurrences\nOn va maintenant constituer des bi-grammes bas√©s sur de nombreuses co-occurrences entre les termes :\n\n# textstat_collocations(tok)\n\nhead(textstat_collocations(tok), 5)\n\n       collocation count count_nested length   lambda        z\n1 livraison rapide   612            0      2 3.587755 56.29325\n2    oiseaux mania   194            0      2 7.111407 38.50542\n3         tr√®s bon   291            0      2 3.325458 37.28183\n4        tr√®s bien   332            0      2 2.763473 37.07693\n5    bonne qualit√©   116            0      2 4.440036 33.65412\n\ntail(textstat_collocations(tok),10)\n\n            collocation count count_nested length    lambda         z\n4046          bien bien     3            0      2 -1.601302 -2.983435\n4047 livraison produits     4            0      2 -1.539254 -3.248433\n4048        site rapide     7            0      2 -1.205217 -3.274181\n4049  commande commande     8            0      2 -1.154362 -3.338450\n4050        bien rapide    10            0      2 -1.111673 -3.567851\n4051    produits rapide     2            0      2 -2.293296 -3.616176\n4052 livraison commande     9            0      2 -1.233460 -3.770310\n4053             tr√®s a     2            0      2 -2.581564 -4.071675\n4054      rapide rapide     4            0      2 -2.401230 -5.077424\n4055          tr√®s tr√®s    22            0      2 -1.110720 -5.196198\n\ncolloc&lt;-textstat_collocations(tok, min_count = 10, tolower = TRUE)\nhead(colloc,10)\n\n        collocation count count_nested length   lambda        z\n1  livraison rapide   612            0      2 3.587755 56.29325\n2     oiseaux mania   194            0      2 7.111407 38.50542\n3          tr√®s bon   291            0      2 3.325458 37.28183\n4         tr√®s bien   332            0      2 2.763473 37.07693\n5     bonne qualit√©   116            0      2 4.440036 33.65412\n6         rien dire    93            0      2 5.576935 32.54402\n7       bon produit   146            0      2 3.412030 32.15574\n8      envoi rapide   137            0      2 3.741715 27.61231\n9  produit conforme    94            0      2 3.613662 27.52989\n10  tr√®s satisfaite   154            0      2 3.288031 27.11619\n\ntail(colloc,10)\n\n          collocation count count_nested length     lambda         z\n356   livraison merci    10            0      2 -0.4024193 -1.285159\n357 commande produits    11            0      2 -0.3922655 -1.311659\n358       rapide site    13            0      2 -0.4424632 -1.601325\n359  rapide livraison    23            0      2 -0.3478837 -1.653038\n360     bien commande    11            0      2 -0.5705858 -1.910948\n361          rapide a    10            0      2 -0.6915725 -2.214619\n362          a rapide    10            0      2 -0.8863541 -2.840453\n363   commande rapide    21            0      2 -0.6532274 -2.978690\n364       bien rapide    10            0      2 -1.1116732 -3.567851\n365         tr√®s tr√®s    22            0      2 -1.1107203 -5.196198\n\ntok_cooc&lt;-tokens_compound(tok, pattern = colloc[colloc$z&gt;6.97,],join = TRUE)\n\ntok[\"text400\"]\n\nTokens consisting of 1 document and 7 docvars.\ntext400 :\n[1] \"Excellent\" \"service\"   \"livraison\" \"rapide\"   \n\ntok_cooc[\"text400\"]\n\nTokens consisting of 1 document and 7 docvars.\ntext400 :\n[1] \"Excellent_service\" \"livraison_rapide\" \n\n\nAnalyse de fr√©quence et repr√©sentation graphique :\n\ndfm_cooc&lt;-dfm(tok_cooc)\n\ndfm_cooc2&lt;-dfm_trim(dfm_cooc, max_termfreq = 170)\n\nhead(textstat_frequency(dfm_cooc2),20)\n\n           feature frequency rank docfreq group\n1             prix       169    1     160   all\n2          service       165    2     154   all\n3        perroquet       165    2     150   all\n4          oiseaux       165    2     145   all\n5              top       163    5     149   all\n6          qualit√©       160    6     153   all\n7    oiseaux_mania       142    7     129   all\n8       rapidement       139    8     137   all\n9             tres       127    9     107   all\n10        articles       124   10     115   all\n11        rapidit√©       124   10     117   all\n12            re√ßu       122   12     114   all\n13         s√©rieux       121   13     114   all\n14             car       120   14     111   all\n15           temps       120   14     115   all\n16       satisfait       115   16     108   all\n17        probl√®me       114   17     102   all\n18           comme       114   17     108   all\n19 tr√®s_satisfaite       111   19     107   all\n20         graines       107   20      87   all\n\ntextplot_wordcloud(dfm_cooc2, max_words = 200, color = brewer.pal(6, \"Set2\"))\n\n\n\n\n\n\n\ntok_cooc&lt;-tokens_replace(tok_cooc, c(\"tr√®s_rapidement\",\"tr√®s_satisfait\"), c(\"tr√®s_rapide\",\"tr√®s_satisfaite\"))\n\ndfm_cooc&lt;-dfm(tok_cooc)\ntextstat_frequency(dfm_cooc, n=25)\n\n            feature frequency rank docfreq group\n1          commande       715    1     620   all\n2                 a       502    2     412   all\n3         livraison       474    3     432   all\n4              site       401    4     359   all\n5  livraison_rapide       389    5     381   all\n6             merci       380    6     368   all\n7           parfait       361    7     334   all\n8              tr√®s       349    8     298   all\n9           produit       331    9     294   all\n10           rapide       328   10     310   all\n11         produits       321   11     292   all\n12       recommande       320   12     313   all\n13             plus       292   13     254   all\n14             bien       280   14     251   all\n15        tr√®s_bien       256   15     236   all\n16            super       252   16     232   all\n17             tout       245   17     224   all\n18             j'ai       234   18     189   all\n19            colis       207   19     176   all\n20  tr√®s_satisfaite       202   20     195   all\n21            c'est       196   21     168   all\n22         toujours       188   22     163   all\n23      tr√®s_rapide       174   23     168   all\n24             prix       169   24     160   all\n25          service       165   25     154   all\n\ndfm_cooc2&lt;-dfm_trim(dfm_cooc, max_termfreq = 175)\n\ntextplot_wordcloud(dfm_cooc2, max_words = 100, color = brewer.pal(6, \"Set2\"))",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Annotations et d√©pendances syntaxiques</span>"
    ]
  },
  {
    "objectID": "chapter4.html#annotations",
    "href": "chapter4.html#annotations",
    "title": "4¬† Annotations et d√©pendances syntaxiques",
    "section": "4.4 Annotations",
    "text": "4.4 Annotations\nPour cette partie, on repart du jeu de donn√©es brut.\n\n4.4.1 D√©tecter les langues\nDans le cas d‚Äôun corpus compos√© de plusieurs langues (par exemple, un corpus extrait de twitter), il peut √™tre int√©ressant de filtrer le corpus √† partir de la langue. On utilise un algorithme, qui peut √™tre long √† ex√©cuter selon la taille du corpus, et qui est plut√¥t performant : cld3. Il repose sur un r√©seau de neurones d√©velopp√© par Google\n\nlibrary(cld3)\n\ndata$langue&lt;-detect_language(data$comments)\n# data$langue\n\ndata_fr&lt;-data%&gt;%filter(langue==\"fr\")\n\n\n\n4.4.2 POS\n\nlibrary(cleanNLP)\n\n# cnlp_init_udpipe(model_name = \"french\")\n# \n# annotate&lt;-cnlp_annotate(data$comments, verbose = 100)\n# ann_token&lt;-annotate$token\n# write_csv2(ann_token, \"annotation_oiseaux.csv\")\n# write_rds(ann_token,\"annotation_oiseaux.rds\")\n\nann_token&lt;-read_rds(\"data/annotation_oiseaux.rds\")\n\nhead(ann_token%&gt;%filter(upos==\"ADJ\"|upos==\"NOUN\"|upos==\"VERB\"),15)\n\n# A tibble: 15 √ó 11\n   doc_id   sid tid   token    token_with_ws lemma  upos  xpos  feats tid_source\n    &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;         &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     \n 1      1     1 4     super    \"super \"      super  ADJ   &lt;NA&gt;  Gend‚Ä¶ 5         \n 2      1     1 5     service  \"service\"     servi‚Ä¶ NOUN  &lt;NA&gt;  Gend‚Ä¶ 0         \n 3      1     2 2     changez  \"changez \"    chang‚Ä¶ VERB  &lt;NA&gt;  Mood‚Ä¶ 0         \n 4      1     2 5     peut     \"peut \"       pouvo‚Ä¶ VERB  &lt;NA&gt;  Mood‚Ä¶ 2         \n 5      1     2 8     cagnotte \"cagnotte \"   cagnot ADJ   &lt;NA&gt;  Gend‚Ä¶ 9         \n 6      1     2 9     fid√©lit√© \"fid√©lit√© \"   fid√©l‚Ä¶ NOUN  &lt;NA&gt;  Gend‚Ä¶ 5         \n 7      1     2 11    est      \"est\"         √™tre   VERB  &lt;NA&gt;  Mood‚Ä¶ 9         \n 8      1     2 15    dirais   \"dirais \"     dir    VERB  &lt;NA&gt;  Mood‚Ä¶ 11        \n 9      1     2 16    inutile  \"inutile \"    inuti‚Ä¶ ADJ   &lt;NA&gt;  Gend‚Ä¶ 15        \n10      2     1 2     d√©lai    \"d√©lai \"      d√©lai  NOUN  &lt;NA&gt;  Gend‚Ä¶ 0         \n11      2     1 5     commande \"commande \"   comma‚Ä¶ NOUN  &lt;NA&gt;  Gend‚Ä¶ 2         \n12      2     1 7     rapide   \"rapide \"     rapide ADJ   &lt;NA&gt;  Gend‚Ä¶ 5         \n13      2     2 2     d√©lais   \"d√©lais \"     d√©lais NOUN  &lt;NA&gt;  Gend‚Ä¶ 14        \n14      2     2 6     commande \"commande \"   comma‚Ä¶ NOUN  &lt;NA&gt;  Gend‚Ä¶ 2         \n15      2     2 8     rapide   \"rapide\"      rapide ADJ   &lt;NA&gt;  Gend‚Ä¶ 6         \n# ‚Ñπ 1 more variable: relation &lt;chr&gt;\n\nann_token%&gt;%filter(upos==\"ADJ\"|upos==\"NOUN\"|upos==\"VERB\")\n\n# A tibble: 34,232 √ó 11\n   doc_id   sid tid   token    token_with_ws lemma  upos  xpos  feats tid_source\n    &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;         &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     \n 1      1     1 4     super    \"super \"      super  ADJ   &lt;NA&gt;  Gend‚Ä¶ 5         \n 2      1     1 5     service  \"service\"     servi‚Ä¶ NOUN  &lt;NA&gt;  Gend‚Ä¶ 0         \n 3      1     2 2     changez  \"changez \"    chang‚Ä¶ VERB  &lt;NA&gt;  Mood‚Ä¶ 0         \n 4      1     2 5     peut     \"peut \"       pouvo‚Ä¶ VERB  &lt;NA&gt;  Mood‚Ä¶ 2         \n 5      1     2 8     cagnotte \"cagnotte \"   cagnot ADJ   &lt;NA&gt;  Gend‚Ä¶ 9         \n 6      1     2 9     fid√©lit√© \"fid√©lit√© \"   fid√©l‚Ä¶ NOUN  &lt;NA&gt;  Gend‚Ä¶ 5         \n 7      1     2 11    est      \"est\"         √™tre   VERB  &lt;NA&gt;  Mood‚Ä¶ 9         \n 8      1     2 15    dirais   \"dirais \"     dir    VERB  &lt;NA&gt;  Mood‚Ä¶ 11        \n 9      1     2 16    inutile  \"inutile \"    inuti‚Ä¶ ADJ   &lt;NA&gt;  Gend‚Ä¶ 15        \n10      2     1 2     d√©lai    \"d√©lai \"      d√©lai  NOUN  &lt;NA&gt;  Gend‚Ä¶ 0         \n# ‚Ñπ 34,222 more rows\n# ‚Ñπ 1 more variable: relation &lt;chr&gt;\n\ng&lt;-ann_token%&gt;%group_by(upos)%&gt;%\n  summarise(n=n())%&gt;%\n  filter(!is.na(upos))\n\nggplot(g)+\n  geom_col(aes(reorder(upos,n),n, fill=n), show.legend = FALSE)+\n  scale_fill_fermenter(palette = \"PuRd\", direction = 1)+\n  coord_flip()+\n  labs(title = \"Fr√©quence des UPOS\", subtitle = \"Corpus Oiseaux Mania\", caption = \"Data TrustPilot\", x=NULL, y=NULL)+\n  theme_dark()\n\n\n\n\n\n\n\n\nMaintenant, on va s‚Äôint√©resser √† des cat√©gories grammaticales sp√©cifiques :\n\nvocab1&lt;-ann_token%&gt;%\n  filter(upos==\"NOUN\")%&gt;%\n  summarise(freq=n(),.by=lemma)%&gt;%\n  filter(freq&gt;55)\n\nggplot(vocab1,aes(x=reorder(lemma,freq),y=freq))+\n  geom_bar(stat=\"identity\",fill=\"lightgreen\")+\n  coord_flip()+\n  theme_minimal()+\n  labs(title = \"Noms communs les plus fr√©quents\",subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\",x=\"Noms commun\",y=\"Fr√©quence\")\n\n\n\n\n\n\n\nvocab1bis&lt;-ann_token%&gt;%\n  filter(upos==\"ADJ\")%&gt;%\n  summarise(freq=n(),.by=lemma)%&gt;%\n  filter(freq&gt;55)\n\nggplot(vocab1bis,aes(x=reorder(lemma,freq),y=freq))+\n  geom_bar(stat=\"identity\",fill=\"darkblue\")+\n  coord_flip()+\n  theme_minimal()+\n  labs(title = \"Adjectifs les plus fr√©quents\",subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\",x=\"Adjectifs\",y=\"Fr√©quence\")\n\n\n\n\n\n\n\nvocab2&lt;-ann_token%&gt;%\n  filter(upos==\"NOUN\" | upos==\"VERB\" | upos==\"ADJ\")%&gt;%\n  summarise(freq=n(),.by=c(lemma,upos))%&gt;%\n  filter(freq&gt;30)%&gt;%\n  mutate(angle= 90 * sample(c(0, 1), n(), replace = TRUE, prob = c(75, 25)))\n\nlibrary(ggwordcloud)\nggplot(vocab2)+\n  geom_text_wordcloud_area(aes(label=lemma, size=freq, color=freq, angle=angle))+\n  scale_size_area(max_size = 24)+\n  scale_color_fermenter(palette = \"Set2\")+\n  theme_minimal()\n\n\n\n\n\n\n\nggplot(vocab2)+\n  geom_text_wordcloud_area(aes(label=lemma, size=freq, color=upos, angle=angle))+\n  scale_size_area(max_size = 24)+\n  scale_color_manual(values=c(\"ADJ\"=\"orange\",\"NOUN\"=\"lightgreen\",\"VERB\"=\"purple\"))+\n  theme_minimal()",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Annotations et d√©pendances syntaxiques</span>"
    ]
  },
  {
    "objectID": "chapter4.html#les-d√©pendances-syntaxiques",
    "href": "chapter4.html#les-d√©pendances-syntaxiques",
    "title": "4¬† Annotations et d√©pendances syntaxiques",
    "section": "4.5 Les d√©pendances syntaxiques",
    "text": "4.5 Les d√©pendances syntaxiques\nQuels sont les mots associ√©s aux termes cibles ?\n\n#on met √† niveau la racine\nann_racine&lt;- ann_token%&gt;%\n  left_join(ann_token,by= c(\"doc_id\"=\"doc_id\", \"sid\"=\"sid\", \"tid_source\"=\"tid\"), suffix=c(\"\", \"_source\"))\n#on filtre les relation nominales puis celle qui concerne les termes cibles\nfoo&lt;-ann_racine %&gt;%\n  filter(relation == \"amod\"|relation ==\"acl\"|relation ==\"nmod\"|relation ==\"appos\") %&gt;%\n  select(qual = lemma, source = lemma_source)%&gt;%\n  filter(source==\"commande\"|source==\"livraison\"|source==\"produit\"|source==\"prix\")%&gt;% \n  group_by(source,qual)%&gt;%\n  summarise(n=n())\n\n`summarise()` has grouped output by 'source'. You can override using the\n`.groups` argument.\n\n# On remet en forme les donn√©es\nfoo1&lt;-foo%&gt;%\n  pivot_wider(names_from = source, values_from = n)%&gt;%\n  mutate(across(everything(), ~replace_na(.x,0)))%&gt;%\n  mutate(sum=rowSums(.[,2:5]))%&gt;%\n  filter(sum&gt;15)%&gt;%\n  select(-sum)%&gt;%\n  pivot_longer(!qual, names_to = \"source\", values_to = \"n\")\n\n\n\nggplot(foo1,aes(x=reorder(qual,n), y=n, group=source))+\n  geom_bar(stat=\"identity\",aes(fill=source),position=position_dodge())+\n  coord_flip()+\n  scale_fill_brewer(palette=\"Spectral\",direction = -1)+\n  theme_minimal()+ \n  labs( title=\"Analyse des d√©pendances nominales\", subtitle = \"les termes du site et du service\",caption = \"Data : TrustPilot sur Oiseaux Mania\", x=\"tokens d√©pendants\", y=\"Fr√©quence\", fill=\"Termes\")+\n  facet_wrap(~source, ncol = 4)",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Annotations et d√©pendances syntaxiques</span>"
    ]
  },
  {
    "objectID": "chapter5.html",
    "href": "chapter5.html",
    "title": "5¬† Rappel sur les pr√©-traitements",
    "section": "",
    "text": "5.1 Les donn√©es\ndata &lt;- read_csv(\"data/data_trustpilot_oiseaux.csv\")\n\nRows: 4388 Columns: 7\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (4): auteur, date, month, comments\ndbl (3): id, year, note\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# cnlp_init_udpipe(model_name = \"french\")\n# \n# annotate&lt;-cnlp_annotate(data$comments, verbose = 100)\n# ann_token&lt;-annotate$token\n# write_csv2(ann_token, \"annotation_oiseaux.csv\")\n# write_rds(ann_token,\"annotation_oiseaux.rds\")\n\nann_token&lt;-read_rds(\"data/annotation_oiseaux.rds\")",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Rappel sur les pr√©-traitements</span>"
    ]
  },
  {
    "objectID": "chapter5.html#reconstruire-le-texte",
    "href": "chapter5.html#reconstruire-le-texte",
    "title": "5¬† Rappel sur les pr√©-traitements",
    "section": "5.2 Reconstruire le texte",
    "text": "5.2 Reconstruire le texte\n\ndata&lt;-ann_token%&gt;%\n  filter(upos==\"NOUN\"|upos==\"VERB\"|upos==\"ADJ\"|upos==\"ADV\")%&gt;%\n  group_by(doc_id)%&gt;%\n  summarise(text=paste(lemma,collapse = \" \"))%&gt;%\n  inner_join(data, join_by(\"doc_id\"==\"id\"))\n\ncorpus_new&lt;-corpus(data, text_field = \"text\")\n\ntoks&lt;-tokens(corpus_new)%&gt;%\n  tokens_replace(c(\"produire\", \"conformer\",\"colir\", \"tre\", \"livrer\", \"n\"), c(\"produit\", \"conforme\",\"colis\", \"tr√®s\", \"livraison\", \"ne\"))%&gt;%\n  tokens_remove(c(\".\",\",\"))\n\ncolloc&lt;-textstat_collocations(toks, min_count = 10, tolower = TRUE)\n\ntoks&lt;-tokens_compound(toks, pattern = colloc[colloc$z&gt;7,])\n\ndfm&lt;-dfm(toks)\nhead(textstat_frequency(dfm),50)\n\n            feature frequency rank docfreq group\n1           produit       604    1     530   all\n2          commande       571    2     505   all\n3           parfait       481    3     441   all\n4         livraison       462    4     414   all\n5       recommander       430    5     417   all\n6         commander       407    6     362   all\n7              site       395    7     356   all\n8              tr√®s       370    8     321   all\n9             avoir       337    9     293   all\n10           rapide       315   10     303   all\n11 livraison_rapide       311   11     306   all\n12            super       282   12     254   all\n13           oiseau       281   13     251   all\n14              pas       277   14     226   all\n15        tr√®s_bien       272   15     253   all\n16             plus       266   16     231   all\n17             bien       253   17     227   all\n18         recevoir       231   18     210   all\n19               ne       218   19     195   all\n20             dire       201   20     190   all\n21          article       200   21     177   all\n22   tr√®s_satisfait       180   22     174   all\n23          service       179   23     168   all\n24         toujours       176   24     160   all\n25            faire       176   24     152   all\n26        perroquet       173   26     158   all\n27              bon       170   27     158   all\n28             tout       153   28     142   all\n29          qualit√©       150   29     144   all\n30         probl√®me       150   29     137   all\n31           ne_pas       147   31     131   all\n32          trouver       145   32     137   all\n33              top       145   32     138   all\n34            colis       145   32     123   all\n35        excellent       141   35     134   all\n36            petit       137   36     125   all\n37         conforme       132   37     127   all\n38             prix       132   37     125   all\n39          arriver       122   39     117   all\n40          s√©rieux       122   39     116   all\n41    bien_emballer       119   41     115   all\n42       satisfaire       111   42     109   all\n43             m√™me       108   43     100   all\n44         beaucoup       107   44     104   all\n45             jour       104   45     103   all\n46       rapidement       103   46     100   all\n47          pouvoir        98   47      90   all\n48            jouet        96   48      74   all\n49      tr√®s_rapide        94   49      91   all\n50            d√©lai        93   50      86   all\n\ntextplot_wordcloud(dfm, max_size = 4, max_words = 200)",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Rappel sur les pr√©-traitements</span>"
    ]
  },
  {
    "objectID": "chapter5.html#analyse-de-keyness",
    "href": "chapter5.html#analyse-de-keyness",
    "title": "5¬† Rappel sur les pr√©-traitements",
    "section": "5.3 Analyse de keyness",
    "text": "5.3 Analyse de keyness\nDans un premier temps, nous allons regarder le Net Promoter Score (NPS), puis nous √©tudierons les discours des promoteurs, d√©tracteurs et passifs.\nTout d‚Äôabord, nous cr√©ons nos cat√©gories en fonction des notes.\n\ncol&lt;- c(\"red\",\"gold\", \"chartreuse\")\n\n\ndata&lt;-data %&gt;%\n  mutate(NPS=case_when(note&lt;4~\"D√©tracteurs\",\n                       note==4~\"Passifs\",\n                       note&gt;4~\"Promoteurs\"))\n\n\nggplot(data, aes(x=note))+\n  geom_histogram(binwidth = 1, aes(fill=NPS))+\n  labs( title= \" Distribution des scores NPS\", \n        subtitle = paste(\"Moyenne du NPS de l'√©chantillon\",round(mean(data$note),1)), \n        caption = paste(\"Data : TrustPilot, n=\",nrow(data)), \n        y = \"Fr√©quence\")+ \n  scale_fill_manual(values=col)+\n  theme_light()\n\n\n\n\n\n\n\n\nPuis nous r√©alisons un nuage de mots pour chaque groupe, afin d‚Äôavoir une id√©e de ce qui est exprim√©.\n\ndfm$NPS&lt;-data$NPS\n# docvars(dfm)\n\ndfm_gp &lt;-dfm%&gt;%\n    dfm_group(groups = NPS)\n# dfm_gp\n\nstat&lt;- dfm_gp %&gt;% \n  textstat_frequency(n = 30,  groups = NPS)\n# stat\n\nggplot(stat, aes(label = feature)) +\n  geom_text_wordcloud(aes(size=log(frequency), color=group)) +\n  theme_minimal()+\n  facet_wrap(vars(group))+\n  scale_color_manual(values=col)+ \n  labs(title=\"Nuage des 30 mots les plus fr√©quents (Par groupes)\",\n       caption = \"La taille des mots est proportionnelle au log de leurs fr√©quences\")\n\n\n\n\n\n\n\n\nMaintenant, nous nous int√©ressons √† ce qui caract√©rise chacun des groupes par rapport aux autres, gr√¢ce √† la mesure du keyness.\n\ngraph_promoteur&lt;-textstat_keyness(dfm_gp, target = \"Promoteurs\")%&gt;%\n  textplot_keyness(n = 30L, labelsize = 2,   show_legend = FALSE, \n                     show_reference = FALSE,   color = c(\"Darkgreen\", \"gray\"))+\n  labs(x=NULL)\n\n\ngraph_detracteur &lt;- textstat_keyness(dfm_gp, target = \"D√©tracteurs\" )%&gt;%\n  textplot_keyness(n = 30L, labelsize = 2,   show_legend = FALSE,   \n                     show_reference = FALSE,   color = c(\"firebrick\", \"gray\"))+ \n  labs(x=NULL)\n\n\ngraph_passif &lt;- textstat_keyness(dfm_gp, target = \"Passifs\")%&gt;%\n  textplot_keyness(n = 30L, labelsize = 2,   show_legend = FALSE,   show_reference = FALSE,    color = c(\"gold2\", \"gray\"))+\n  labs(x=NULL)\n\n\nlibrary(cowplot)\n\n\nAttachement du package : 'cowplot'\n\n\nL'objet suivant est masqu√© depuis 'package:lubridate':\n\n    stamp\n\np&lt;- plot_grid(graph_detracteur, graph_passif ,graph_promoteur,  labels = c('D√©tracteurs', 'Passifs', 'Promoteurs'), label_size = 10, ncol=3)\n\ntitle &lt;- ggdraw() + draw_label(\"NPS : Les raisons qui conduisent √† la recommandation (keyness)\", fontface='bold')\n\nnote &lt;- ggdraw()+ draw_text(\"Les valeurs repr√©sentent le keyness des termes.\\nIl mesure leur caract√®re distinctif par une statistique du chi¬≤\", size=8,x = 0.5, y = 0.5)\n\n\nplot_grid(title, p,note, ncol=1, rel_heights=c(0.1, 1)) # rel_heights values control title margins",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Rappel sur les pr√©-traitements</span>"
    ]
  },
  {
    "objectID": "chapter5.html#diversit√©-lexicale-et-lisibilit√©",
    "href": "chapter5.html#diversit√©-lexicale-et-lisibilit√©",
    "title": "5¬† Rappel sur les pr√©-traitements",
    "section": "5.4 Diversit√© lexicale et lisibilit√©",
    "text": "5.4 Diversit√© lexicale et lisibilit√©\n\nhead(textstat_lexdiv(dfm))\n\n  document       TTR\n1        1 1.0000000\n2        2 0.8620690\n3        3 0.9000000\n4        4 1.0000000\n5        5 0.9268293\n6        6 1.0000000\n\nhead(textstat_readability(corpus_new))\n\n  document     Flesch\n1        1  24.440000\n2        2  -4.095000\n3        3  -8.278077\n4        4 -10.755000\n5        5  -7.977442\n6        6  56.978462",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Rappel sur les pr√©-traitements</span>"
    ]
  },
  {
    "objectID": "chapter6.html",
    "href": "chapter6.html",
    "title": "6¬† Topic Analysis",
    "section": "",
    "text": "6.1 Introduction\nOn va maintenant s‚Äôint√©resser √† la d√©tection et √† l‚Äôanalyse de topics. Il existe de nombreux algorithmes pour cela. On va commencer avec le mod√®le original : le mod√®le LDA, pour Latent Dirichlet Allocation. Puis on s‚Äôint√©ressera √† un de ses prolongements, le mod√®le STM (Structural Topic Modelling).\nL‚Äôid√©e est la suivante : un corpus est consid√©r√© comme une collection de documents. Chaque document est consid√©r√© comme √©tant compos√© d‚Äôun m√©lange de topics. Chaque topic est consid√©r√© comme √©tant compos√© d‚Äôun m√©lange de tokens. L‚Äôalgorithme calcule par it√©ration les probabilit√©s d‚Äôappartenance des tokens aux topics et des topics aux documents, ce qui nous permet de visualiser la composition des sujets identifi√©s.\nUn sch√©ma explicatif est propos√© par H. Naushan, en 2020.",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>*Topic Analysis*</span>"
    ]
  },
  {
    "objectID": "chapter6.html#introduction",
    "href": "chapter6.html#introduction",
    "title": "6¬† Topic Analysis",
    "section": "",
    "text": "Description du mod√®le LDA :",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>*Topic Analysis*</span>"
    ]
  },
  {
    "objectID": "chapter6.html#les-donn√©es",
    "href": "chapter6.html#les-donn√©es",
    "title": "6¬† Topic Analysis",
    "section": "6.2 Les donn√©es",
    "text": "6.2 Les donn√©es\n\ndata &lt;- read_csv(\"data/data_trustpilot_oiseaux.csv\")\n\nRows: 4388 Columns: 7\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (4): auteur, date, month, comments\ndbl (3): id, year, note\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata&lt;-data%&gt;%mutate(text_id=paste0(\"text_\", row_number(data$id)))",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>*Topic Analysis*</span>"
    ]
  },
  {
    "objectID": "chapter6.html#topics-analysis",
    "href": "chapter6.html#topics-analysis",
    "title": "6¬† Topic Analysis",
    "section": "6.3 Topics Analysis",
    "text": "6.3 Topics Analysis\n\n6.3.1 Le mod√®le LDA avec topicmodels\nOn travaille √† partir du dfm. On doit transformer le format des donn√©es afin de l‚Äôinjecter dans le mod√®le. On r√©duit le nombre de termes consid√©r√©s, ce qui permet de r√©duire les temps de calcul et de trouver une solution convergente.\nLes r√©sultats du mod√®le LDA sont tr√®s d√©pendants de la qualit√© du vocabulaire inject√©. Plus on travaille ce vocabulaire, meilleurs sont les r√©sultats. On va donc reprendre tout ce qu‚Äôon a fait jusqu‚Äô√† pr√©sent pour am√©liorer les r√©sultats de notre mod√®le : on r√©cup√®re les annotations ; on filtre le vocabulaire pour ne garder que les noms, adjectifs et verbes ; on cr√©e les collocations ; on filtre les occurrences trop et pas assez fr√©quentes.\n\nann_token&lt;-read_rds(\"data/annotation_oiseaux.rds\")\n\n\ndata&lt;-ann_token%&gt;%\n  filter(upos==\"NOUN\"|upos==\"VERB\"|upos==\"ADJ\"|upos==\"ADV\")%&gt;%\n  group_by(doc_id)%&gt;%\n  summarise(text=paste(lemma,collapse = \" \"))%&gt;%\n  inner_join(data, join_by(\"doc_id\"==\"id\"))\n\ncorpus_new&lt;-corpus(data, text_field = \"text\")\ntoks&lt;-tokens(corpus_new)%&gt;%\n  tokens_replace(c(\"produire\", \"conformer\",\"colir\", \"tre\", \"livrer\", \"n\"), c(\"produit\", \"conforme\",\"colis\", \"tr√®s\", \"livraison\", \"ne\"))%&gt;%\n  tokens_remove(c(\".\",\",\"))\n\ncolloc&lt;-textstat_collocations(toks, min_count = 10, tolower = TRUE)\n\ntoks&lt;-tokens_compound(toks, pattern = colloc[colloc$z&gt;7,])\n\n\ndfm_new&lt;-dfm(toks)%&gt;%\n  dfm_trim(min_termfreq = 0.6, termfreq_type = \"quantile\",\n           max_docfreq = 0.1, docfreq_type = \"prop\")\ndtm_new &lt;- convert(dfm_new, to = \"topicmodels\")\n\nset.seed(1234)\nlda &lt;- LDA(dtm_new, k = 5)\n\n#On regarde les r√©sultats\nterms(lda,10)\n\n      Topic 1       Topic 2          Topic 3            Topic 4      \n [1,] \"tr√®s\"        \"recommander\"    \"livraison_rapide\" \"livraison\"  \n [2,] \"toujours\"    \"livraison\"      \"site\"             \"tr√®s_bien\"  \n [3,] \"rapide\"      \"avoir\"          \"tr√®s\"             \"commander\"  \n [4,] \"site\"        \"site\"           \"super\"            \"recommander\"\n [5,] \"livraison\"   \"ne\"             \"pas\"              \"recevoir\"   \n [6,] \"recommander\" \"oiseau\"         \"bien\"             \"tout\"       \n [7,] \"recevoir\"    \"commander\"      \"commander\"        \"pas\"        \n [8,] \"bien\"        \"tr√®s\"           \"tr√®s_satisfait\"   \"bon\"        \n [9,] \"excellent\"   \"trouver\"        \"service\"          \"perroquet\"  \n[10,] \"perroquet\"   \"tr√®s_satisfait\" \"qualit√©\"          \"bien\"       \n      Topic 5    \n [1,] \"oiseau\"   \n [2,] \"avoir\"    \n [3,] \"plus\"     \n [4,] \"article\"  \n [5,] \"commander\"\n [6,] \"pas\"      \n [7,] \"ne_pas\"   \n [8,] \"super\"    \n [9,] \"rapide\"   \n[10,] \"bon\"      \n\n# topics(lda)\n\ncorpus_new[\"996\"]\n\nCorpus consisting of 1 document and 7 docvars.\n996 :\n\"second commande satisfaction total exp√©dition tr√®s rapide em...\"\n\ncorpus_new[\"995\"]\n\nCorpus consisting of 1 document and 7 docvars.\n995 :\n\"tr√®s bon produire livraison rapide s√©rieux recommander produ...\"\n\ncorpus_new[\"999\"]\n\nCorpus consisting of 1 document and 7 docvars.\n999 :\n\"tr√®s bon magasin emballage norme trouver besoin conseiller r...\"\n\nterm&lt;-as_tibble(terms(lda,25))%&gt;%\n  mutate(rank=as.numeric(row.names(.)))%&gt;%\n  pivot_longer(-rank, names_to = \"topic\",values_to = \"term\")\n\nggplot(term, aes(x=topic, y= rank, group =  term , label = term)) + \n  scale_y_reverse() +\n  geom_text(aes(color=topic,size=8/log(rank)))+\n  theme_minimal()+\n  scale_color_hue()+\n  guides(color=\"none\",size=\"none\")\n\n\n\n\n\n\n\n\n\n\n6.3.2 D√©terminer le nombre de topics optimal\nLe mod√®le LDA fonctionne √† partir d‚Äôun nombre de topics donn√©. La question est donc de savoir quel est le nombre de topics optimal pour d√©crire notre corpus. Heureusement, des personnes ont cr√©√© des fonctions et des proc√©dures pour nous aider dans cette qu√™te. L‚Äôid√©e est de calculer diff√©rents mod√®les pour diff√©rents nombres de topics, et de comparer la qualit√© des r√©sultats. La proc√©dure ci-dessous est en deux parties :\n\nTout d‚Äôabord, on compare la qualit√© de diff√©rents indicateurs sur un grand nombre de mod√®les, pour aboutir √† une liste de quelques solutions √† comparer plus en d√©tail (de 3 √† 10).\nEnsuite, on compare les r√©sultats de la liste r√©duite de mod√®les, pour d√©terminer lequel a la meilleure distribution des topics entre les documents. La distribution recherch√©e est celle qui distingue le plus les documents en fonction des topics, tout en √©tant √† droite de l‚Äôestimation d‚Äôune r√©partition uniforme des documents entre les topics. Le crit√®re de parcimonie nous invite √† choisir la solution avec le moins grand nombre de topics, en cas de r√©sultats comparables.\n\n\n##Etape 1 : les meilleures solutions\nlibrary(ldatuning)\nlibrary(magrittr)\n\n\nAttachement du package : 'magrittr'\n\n\nL'objet suivant est masqu√© depuis 'package:purrr':\n\n    set_names\n\n\nL'objet suivant est masqu√© depuis 'package:tidyr':\n\n    extract\n\nresult &lt;- FindTopicsNumber(dtm_new,\n                           topics = c(seq(from = 2, to = 9, by = 1), seq(10, 25, 5)),\n                           metrics = c(\"Griffiths2004\", \"CaoJuan2009\", \"Arun2010\", \"Deveaud2014\"),\n                           method = \"Gibbs\",\n                           control = list(seed = 0:4,\n                                          nstart = 5,\n                                          best = TRUE),\n                           mc.cores = 4L,\n                           verbose = TRUE\n                           )\n\nfit models... done.\ncalculate metrics:\n  Griffiths2004... done.\n  CaoJuan2009... done.\n  Arun2010... done.\n  Deveaud2014... done.\n\nFindTopicsNumber_plot(result)\n\nWarning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\n‚Ñπ The deprecated feature was likely used in the ldatuning package.\n  Please report the issue at &lt;https://github.com/nikita-moor/ldatuning/issues&gt;.\n\n\n\n\n\n\n\n\n##Etape 2 : comparaison des solutions\npara &lt;- tibble(k = c(6,7,8,9,10))\nlemma_tm &lt;- para %&gt;%\n  mutate(lda = map(k,\n                   function(k) LDA(\n                     k=k,\n                     x=dtm_new,\n                     method=\"Gibbs\",\n                     control=list(seed = 0:4,\n                                  nstart = 5,\n                                  best = TRUE)\n                     )\n                   )\n         )\n\nlemma_tm &lt;- lemma_tm %&gt;%\n  mutate(lda_gamma = map(.x=lda,\n                         .f=tidytext::tidy,\n                         matrix=\"gamma\"))\nlemma_tm %&gt;%\n  unnest(lda_gamma) %&gt;%\n  group_by(k, document) %&gt;%\n  arrange(desc(gamma)) %&gt;%\n  slice(1) %&gt;%\n  ungroup() %&gt;%\n  ggplot(aes(x=gamma, fill=factor(k))) +\n  geom_histogram(bins = 20) +\n  scale_fill_discrete(name = \"Number of\\nTopics\") +\n  xlab(\"maximum gamma per document\") +\n  facet_wrap(~k) +\n  geom_vline(aes(xintercept = 1/k),\n             tibble(k=lemma_tm %$% unique(k)),\n             color=\"darkred\")\n\n\n\n\n\n\n\n\n\n\n6.3.3 Repr√©sentation graphique\n√Ä partir de la solution retenue aux √©tapes pr√©c√©dentes, on va repr√©senter les diff√©rents topics :\n\nset.seed(1234)     #pour la r√©plicabilit√© des r√©sultats\nlda &lt;- LDA(dtm_new, k = 7)\n\nlda_res&lt;-as.data.frame(terms(lda, 25))%&gt;%\n  rename(nom1='Topic 1',nom2='Topic 2', nom3='Topic 3', nom4='Topic 4', nom5='Topic 5',nom6='Topic 6',nom7='Topic 7')%&gt;%\n  mutate(rank=as.numeric(row.names(.)))%&gt;%\n  pivot_longer(-rank, names_to = \"topic\", values_to = \"term\")\n\nggplot(lda_res, aes(x=topic, y= rank, group =  term , label = term)) + \n  scale_y_reverse() +\n  geom_text(aes(color=topic,size=8/log(rank)))+\n  theme_minimal()+\n  scale_color_hue()+\n  guides(color=FALSE,size=FALSE)",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>*Topic Analysis*</span>"
    ]
  },
  {
    "objectID": "chapter6.html#theory-driven-lda",
    "href": "chapter6.html#theory-driven-lda",
    "title": "6¬† Topic Analysis",
    "section": "6.4 Theory-Driven LDA",
    "text": "6.4 Theory-Driven LDA\nIci, on va forcer les topics gr√¢ce √† la r√©alisation d‚Äôun dictionnaire. C‚Äôest utile quand on cherche √† appliquer une th√©orie qui nous dit ce que l‚Äôon cherche √† trouver. Par exemple, ici on s‚Äôint√©resse aux attributs cl√©s des logements oiseaux. Dans d‚Äôautre cas, on pourra chercher √† expliquer les notes en fonction de topics qui refl√®tent les attributs cl√©s. On peut r√©aliser le dictionnaire a priori ou apr√®s diff√©rentes analyses de topics, de co-occurences, de fr√©quence, etc.\nOn commence par cr√©er un dictionnaire.\n\ndict&lt;-dictionary(list(produit=c(\"produit*\", \"cage\",\"oiseau\",\"graine*\"),\n                      livraison=c(\"livr*\",\"recepti*\",\"d√©lai\"),\n                      commande=c(\"command*\",\"emballage\",\"envoi*\"),\n                      site=\"*site*\",\n                      prix=c(\"*prix*\",\"frais_port\")\n                      ))\ndict\n\nDictionary object with 5 key entries.\n- [produit]:\n  - produit*, cage, oiseau, graine*\n- [livraison]:\n  - livr*, recepti*, d√©lai\n- [commande]:\n  - command*, emballage, envoi*\n- [site]:\n  - *site*\n- [prix]:\n  - *prix*, frais_port\n\nhead(dfm_lookup(dfm_new,dict))\n\nDocument-feature matrix of: 6 documents, 5 features (76.67% sparse) and 7 docvars.\n    features\ndocs produit livraison commande site prix\n   1       0         0        0    0    0\n   2       0         2        1    0    0\n   3       2         1        0    0    0\n   4       0         0        1    0    0\n   5       0         1        0    0    0\n   6       0         0        0    1    0\n\n\nOn utilise ensuite le package ‚Äòseededlda‚Äô pour lancer le mod√®le semi-supervis√©.\n\nlibrary(seededlda)\n\nLe chargement a n√©cessit√© le package : proxyC\n\n\n\nAttachement du package : 'proxyC'\n\n\nL'objet suivant est masqu√© depuis 'package:stats':\n\n    dist\n\n\n\nAttachement du package : 'seededlda'\n\n\nLes objets suivants sont masqu√©s depuis 'package:topicmodels':\n\n    terms, topics\n\n\nL'objet suivant est masqu√© depuis 'package:stats':\n\n    terms\n\nset.seed(1234)\nslda&lt;-textmodel_seededlda(dfm_new, dict, residual = T)\nterms(slda,20)\n\n      produit            livraison                  commande            \n [1,] \"oiseau\"           \"livraison\"                \"commander\"         \n [2,] \"cage\"             \"livraison_rapide\"         \"emballage\"         \n [3,] \"graine\"           \"d√©lai\"                    \"envoi\"             \n [4,] \"perroquet\"        \"conforme\"                 \"envoi_rapide\"      \n [5,] \"tr√®s\"             \"livraison_tr√®s_rapide\"    \"recevoir\"          \n [6,] \"trouver\"          \"dire\"                     \"arriver\"           \n [7,] \"bien\"             \"satisfaire\"               \"bien\"              \n [8,] \"petit\"            \"livraison_temps\"          \"bien_emballer\"     \n [9,] \"plus\"             \"impeccable\"               \"tr√®s_rapidement\"   \n[10,] \"produit_qualit√©\"  \"toujours\"                 \"toujours\"          \n[11,] \"produit_conforme\" \"tr√®s_bien\"                \"recommander\"       \n[12,] \"avoir\"            \"livraison_rapide_produit\" \"tr√®s_satisfait\"    \n[13,] \"jouet\"            \"correspondre\"             \"commande_livraison\"\n[14,] \"acheter\"          \"top\"                      \"envoi_tr√®s_rapide\" \n[15,] \"beaucoup\"         \"service\"                  \"envoie\"            \n[16,] \"harnais\"          \"bien\"                     \"probl√®me\"          \n[17,] \"perruche\"         \"livreur\"                  \"rapidement\"        \n[18,] \"aussi\"            \"livraison_rapidement\"     \"bon_√©tat\"          \n[19,] \"grand\"            \"livraison_d√©lai\"          \"colis\"             \n[20,] \"faire\"            \"attente\"                  \"rapidit√©\"          \n      site                   prix               other     \n [1,] \"site\"                 \"prix\"             \"pas\"     \n [2,] \"tr√®s\"                 \"rapide\"           \"ne\"      \n [3,] \"recommander\"          \"tr√®s_bien\"        \"avoir\"   \n [4,] \"tr√®s_bon_site\"        \"recommander\"      \"plus\"    \n [5,] \"recommander_site\"     \"super\"            \"ne_pas\"  \n [6,] \"super\"                \"bon\"              \"faire\"   \n [7,] \"rapide\"               \"frais_port\"       \"recevoir\"\n [8,] \"tout\"                 \"excellent\"        \"colis\"   \n [9,] \"siter\"                \"tr√®s_rapide\"      \"donc\"    \n[10,] \"site_s√©rieux\"         \"tr√®s_satisfait\"   \"fois\"    \n[11,] \"vraiment\"             \"service\"          \"jour\"    \n[12,] \"recommander_vivement\" \"tr√®s_bon_produit\" \"m√™me\"    \n[13,] \"tr√®s_s√©rieux\"         \"tr√®s_bon\"         \"envoyer\" \n[14,] \"bon_site\"             \"s√©rieux\"          \"attendre\"\n[15,] \"super_site\"           \"bon_produit\"      \"autre\"   \n[16,] \"top\"                  \"rapider\"          \"probl√®me\"\n[17,] \"conseiller\"           \"prix_raisonnable\" \"pouvoir\" \n[18,] \"√©quipe\"               \"dire\"             \"aller\"   \n[19,] \"t√©l√©phone\"            \"exp√©dition\"       \"mettre\"  \n[20,] \"service_client\"       \"qualit√©\"          \"√™tre\"    \n\n\n\n6.4.1 Expliquer les notes en fonction des topics\nMaintenant, nous cherchons √† voir la r√©partition des topics dans les notes, pour comprendre si certains topics contribuent plus ou moins √† la satisfaction.\n\ntheta&lt;-as.data.frame(slda$theta)%&gt;%mutate(doc_id=as.numeric(row.names(.)))\n\ndata&lt;-inner_join(data, theta)\n\nJoining with `by = join_by(doc_id)`\n\nfoo&lt;-data%&gt;%select(note, produit, livraison, commande, site,prix, other)%&gt;%\n  pivot_longer(-note, names_to = \"topic\", values_to = \"value\")\n\nggplot(foo,aes(x=note, y=value, group=topic))+\n  geom_bar(position=\"fill\",stat=\"identity\", aes(fill=topic))+\n  scale_fill_brewer(palette=\"Spectral\")+\n  theme_minimal()\n\n\n\n\n\n\n\n#Pour finir, une petite r√©gression !\nfit&lt;-lm(note~produit+livraison+commande+site+prix, data =data)\nsummary(fit)\n\n\nCall:\nlm(formula = note ~ produit + livraison + commande + site + prix, \n    data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9666 -0.0424  0.1259  0.2685  2.0844 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.27633    0.05989   38.01   &lt;2e-16 ***\nproduit      2.29956    0.09840   23.37   &lt;2e-16 ***\nlivraison    2.81963    0.09325   30.24   &lt;2e-16 ***\ncommande     2.76466    0.09576   28.87   &lt;2e-16 ***\nsite         3.10423    0.09272   33.48   &lt;2e-16 ***\nprix         3.07552    0.08923   34.47   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7103 on 4332 degrees of freedom\nMultiple R-squared:  0.2926,    Adjusted R-squared:  0.2917 \nF-statistic: 358.3 on 5 and 4332 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>*Topic Analysis*</span>"
    ]
  },
  {
    "objectID": "chapter6.html#le-mod√®le-stm",
    "href": "chapter6.html#le-mod√®le-stm",
    "title": "6¬† Topic Analysis",
    "section": "6.5 Le mod√®le STM",
    "text": "6.5 Le mod√®le STM\nLa Mod√©lisation Th√©matique Structurelle est un prolongement du mod√®le LDA d√©velopp√© ci-dessus. Permettant de parvenir aux m√™mes types de r√©sultats de regroupements th√©matiques par plongement lexical, cette derni√®re se distingue dans le sens o√π elle permet d‚Äôassocier d‚Äôautres variables, ou m√©ta-donn√©es, au corpus trait√© afin de prendre en compte les relations de leurs modalit√©s au contenu. Ainsi, elle cr√©e la notion de pr√©valence d‚Äôun topic, qui permet de prendre en compte sa fluctuation en fonction de la propre √©volution de la covariance des √©l√©ments d‚Äôun m√™me m√©lange.\n\n\n\nLe mod√®le STM\n\n\n\n6.5.1 Choisir le nombre de topics : approche par comparaison\nTout d‚Äôabord, on teste plusieurs mod√®les pour d√©couvrir le nombre de topics optimal. 4 m√©triques sont calcul√©s : * held-out likelihood : la probabilit√© d‚Äôapparition des mots au sein d‚Äôun document apr√®s avoir enlev√© ces mots du document lors de l‚Äô√©tape d‚Äôestimation (√† maximiser) ; * residuals : test de la sur-dispersion des r√©sidus ; si les r√©sidus sont sur-dispers√©s, il est possible qu‚Äôil existe une meilleure solution √† plus de topics (√† minimiser) ; * semantic coherence : la coh√©rence s√©mantique est maximis√©e quand les mots les plus probables d‚Äôun topic apparaissent fr√©quemment ensemble (o-occurrences) (√† maximiser) ; * lower bound : permet de suivre la convergence du mod√®le (√† minimiser).\nOn lance l‚Äôestimation sur 3 mod√®les diff√©rents, puis on compare les m√©triques.\n\ndfm_stm &lt;- convert(dfm_new, to = \"stm\")\n\nWarning in dfm2stm(x, docvars, omit_empty = TRUE): Dropped 4,338 empty\ndocument(s)\n\nlibrary(stm)\n\nstm v1.3.7 successfully loaded. See ?stm for help. \n Papers, resources, and other materials at structuraltopicmodel.com\n\n#test de diff√©rents mod√®les\nkresult2 &lt;- searchK(dfm_stm$documents, dfm_stm$vocab, K = c(7,10,25), prevalence =~ s(note), data = dfm_stm$meta,verbose = FALSE)\n\nWarning in splines::bs(x, df, ...): all interior knots match right boundary\nknot\n\n\nWarning in splines::bs(x, df, ...): all interior knots match right boundary\nknot\nWarning in splines::bs(x, df, ...): all interior knots match right boundary\nknot\n\n#afficher les param√®tres calcul√©s\nplot(kresult2)\n\n\n\n\n\n\n\n\n\n\n6.5.2 Le mod√®le final\nUne fois le nombre de topics choisi, il ne reste plus qu‚Äô√† lancer le mod√®le et interpr√©ter ses r√©sultats. L‚Äôinterpr√©tation passe par 3 √©tapes: * interpr√©ter les topics, * regarder la pr√©valence des topics en fonction des variables de pr√©valence retenues, * analyser la corr√©lation entre les topics\nNous testons ici une solution √† 10 topics. Tout d‚Äôabord, nous lan√ßons le mod√®le et nous regardons la distribution des topics en fonction de leur coh√©rence s√©mantique et de leur exclusivit√© (√† partir de la mesure FREX).\n\n#nb de topics retenu\nk=10\n# la sp√©cification du mod√®le\nset.seed(2020)\nmodel.stm &lt;- stm(dfm_stm$documents, \n                 dfm_stm$vocab, \n                 K = k, max.em.its = 25,\n                 data = dfm_stm$meta, \n                 init.type = \"Spectral\", \n                 prevalence =~ note,\n                 interactions = FALSE,\n                 verbose = FALSE) # this is the actual stm call\n\n\n#la qualit√© des topic\n\ntopicQuality(model.stm , dfm_stm$documents, xlab = \"Semantic Coherence\",  ylab = \"Exclusivity\", M = k)\n\n [1] -189.2462 -189.5355 -205.9539 -170.8214 -186.9341 -163.6631 -196.4798\n [8] -249.0770 -211.9196 -194.3200\n [1] 9.835226 9.789158 9.784484 9.802718 9.759080 9.671072 9.790776 9.800144\n [9] 9.668600 9.777661\n\n\n\n\n\n\n\n\n\n\n\n6.5.3 Interpr√©ter les topics\nLa premi√®re √©tape de l‚Äôexploration consiste √† analyser les mots les plus associ√©s √† chaque topics. Il existe 4 m√©triques d‚Äôassociation diff√©rentes : * Highest Prob : les mots avec la plus forte probabilit√© ; * FREX : pond√©ration des mots selon leur fr√©quence d‚Äôapparition globale et leur appartenance √† des documents sp√©cifiques ; * Lift : pond√©ration par l‚Äôoccurrence des mots dans les autres topics, ce qui donne plus d‚Äôimportance aux mots sp√©cifiques au topic examin√© ; * Score : transformation de Lift par le log.\nIci, on affiche les r√©sultats pour les topics 1, 5 et 10 et on repr√©sente les diff√©rentes m√©triques graphiquement :\n\nlabelTopics(model.stm, c(1,5,10))\n\nTopic 1 Top Words:\n     Highest Prob: site, super, top, petit, tout, tr√®s_bon, rapidit√© \n     FREX: site, satisfait, √©quipe, rapidit√©, tr√®s_bon, livraison_rapidement, correct \n     Lift: r√©action, livraison_tr√®s_vite, envoi_rapide_produit_qualit√©, f√©liciter, livraison_rapidement, livraison_rapide_recommander, situation \n     Score: site, super, top, rapidit√©, petit, tr√®s_bon, satisfait \nTopic 5 Top Words:\n     Highest Prob: dire, vraiment, envoyer, recommander_site, magasin, grand, choix \n     FREX: dire, envoyer, vraiment, boutique, choix, envoi_rapide_soign√©, point \n     Lift: amie, √©cossais, hybride, pack, terme, tr√®s_bon_prestation, front \n     Score: dire, envoyer, vraiment, recommander_site, choix, magasin, grand \nTopic 10 Top Words:\n     Highest Prob: oiseau, pas, plus, tr√®s_satisfait, probl√®me, excellent, arriver \n     FREX: tr√®s_satisfait, efficace, excellent, tr√®s_bon_qualit√©, demander, coli, envoie \n     Lift: fou, √©trangler, nid, tr√®s_satisfait, ador, beaucoup_choix_produit, livraison_rapide_colis_bien_emballer \n     Score: oiseau, plus, pas, tr√®s_satisfait, excellent, probl√®me, arriver \n\nplot(model.stm, type = \"summary\", labeltype=\"prob\",text.cex = 0.7,n=7)\n\n\n\n\n\n\n\nplot(model.stm, type = \"summary\", labeltype=\"score\",text.cex = 0.7,n=5)\n\n\n\n\n\n\n\nplot(model.stm, type = \"summary\", labeltype=\"lift\",text.cex = 0.7,n=5)\n\n\n\n\n\n\n\nplot(model.stm, type = \"summary\", labeltype=\"frex\",text.cex = 0.7,n=5)\n\n\n\n\n\n\n\n\nOn peut √©galement repr√©senter les topics sous forme de nuage de mots. La premi√®re proposition, ci-dessous en bleu, repose sur la probabilit√© d‚Äôappartenance des termes aux topics, la seconde, en rouge, repr√©sente les mots en fonction de l‚Äôoccurrence des topics dans les documents.\n\npar(mfrow = c(3,5) , mar = c(0,0,0,0))\nfor (i in seq_along((1:k)))\n{\n  cloud(model.stm,scale=c(2,.25) ,topic = i, type =\"model\", max.words = 50, colors=\"darkblue\", random.order=FALSE, )\n  text(x=0.5, y=1, paste0(\"topic\",i))\n}\n\nWarning in wordcloud::wordcloud(words = vocab, freq = vec, max.words =\nmax.words, : recommander could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud::wordcloud(words = vocab, freq = vec, max.words =\nmax.words, : tr√®s_bien could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud::wordcloud(words = vocab, freq = vec, max.words =\nmax.words, : livraison_rapide could not be fit on page. It will not be plotted.\n\npar(mfrow = c(3,5) , mar = c(0,0,0,0))\n\n\n\n\n\n\n\nfor (i in seq_along((1:k)))\n{\ncloud(model.stm, topic = i,scale=c(2,.25) ,type = c(\"model\",\"documents\"), dfm,thresh = 0.1, max.words = 50, colors=\"firebrick\")\n   text(x=0.5, y=1, paste0(\"topic\",i))\n}\n\nWarning in wordcloud::wordcloud(words = vocab, freq = vec, max.words =\nmax.words, : recommander could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud::wordcloud(words = vocab, freq = vec, max.words =\nmax.words, : service could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud::wordcloud(words = vocab, freq = vec, max.words =\nmax.words, : livraison_rapide could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud::wordcloud(words = vocab, freq = vec, max.words =\nmax.words, : oiseau could not be fit on page. It will not be plotted.\n\n\n\n\n\n\n\n\n\nOn peut aussi s‚Äôint√©resser √† repr√©senter les contrastes dans les topics :\n\nplot(model.stm, type=\"perspectives\", topics=c(5,10))\n\n\n\n\n\n\n\n\nEnfin, on peut s‚Äôint√©resser aux documents qui contribuent aux topics, √† travers la fonction ‚ÄòfindThoughts‚Äô, et √† les repr√©senter graphiquement. Pour cela, on a besoin d‚Äôun vecteur contenant les textes, dans le m√™me ordre que celui fournit au mod√®le STM. Ici, on utilise le vecteur ‚Äòtitle‚Äô cr√©√© dans les premi√®res manipulations des donn√©es.\n\ndata_stm&lt;-data%&gt;%filter(text_id%in%dfm_stm$meta$text_id)\n\n\nthoughts3 &lt;- findThoughts(model.stm,texts=data_stm$comments ,n = 4,\n topics = 3)$docs[[1]]\nthoughts10 &lt;- findThoughts(model.stm, texts = data_stm$comments, n = 3,\ntopics = 10)$docs[[1]]\n\n\n\npar(mfrow = c(1, 2), mar = c(0.5, 0.5, 1, 0.5))\nplotQuote(thoughts3, width = 50,text.cex = 0.8, main = \"Topic 3\")\nplotQuote(thoughts10, width = 50,text.cex = 0.8, main = \"Topic 10\")\n\n\n\n\n\n\n\n\n\n\n6.5.4 Evolution en fonction de la variable de pr√©valence\nIci, on repr√©sente l‚Äô√©volution de la pr√©sence des topics en fonction de notre variable de pr√©valence, l‚Äôann√©e de parution. D‚Äôabord, on calcule l‚Äôeffet de l‚Äôann√©e pour chaque topic, et ensuite on fait notre repr√©sentation graphique.\n\nmodel.stm.labels &lt;- labelTopics(model.stm, 1:k) #on r√©cup√®re la description des topics pour les titres des graphiques\ndfm_stm$meta$datum &lt;- as.numeric(dfm_stm$meta$note) #on transforme l'ann√©e en variable continue\nmodel.stm.ee &lt;- estimateEffect(1:k ~ s(`datum`), model.stm, meta = dfm_stm$meta) #on estime l'effet de l'ann√©e pour chaque topic\n\nWarning in splines::bs(x, df, ...): all interior knots match right boundary\nknot\nWarning in splines::bs(x, df, ...): all interior knots match right boundary\nknot\n\n\nWarning in estimateEffect(1:k ~ s(datum), model.stm, meta = dfm_stm$meta): Covariate matrix is singular.  See the details of ?estimateEffect() for some common causes.\n             Adding a small prior 1e-5 for numerical stability.\n\nsummary(model.stm.ee,topics=3) #r√©sultat pour le topic 13\n\n\nCall:\nestimateEffect(formula = 1:k ~ s(datum), stmobj = model.stm, \n    metadata = dfm_stm$meta)\n\n\nTopic 3:\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.122366   0.008090  15.126  &lt; 2e-16 ***\ns(datum)1    0.001979   0.026834   0.074    0.941    \ns(datum)2   -0.021155   0.014258  -1.484    0.138    \ns(datum)3   -0.032226   0.008235  -3.913 9.25e-05 ***\ns(datum)4   -0.017980  19.940092  -0.001    0.999    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#repr√©sentation graphique\npar(mfrow = c(3,5) , mar = c(1,0,2,0))\nfor (i in seq_along((1:k)))\n{\n  plot(model.stm.ee, \"datum\", method = \"continuous\", topics = i, main = paste0(model.stm.labels$score[i,1:4], collapse = \"-\"), printlegend = T)\n}\n\nWarning in splines::bs(x, df, ...): all interior knots match right boundary\nknot\n\n\nWarning in splines::bs(x, df, ...): all interior knots match right boundary\nknot\nWarning in splines::bs(x, df, ...): all interior knots match right boundary\nknot\nWarning in splines::bs(x, df, ...): all interior knots match right boundary\nknot\nWarning in splines::bs(x, df, ...): all interior knots match right boundary\nknot\nWarning in splines::bs(x, df, ...): all interior knots match right boundary\nknot\nWarning in splines::bs(x, df, ...): all interior knots match right boundary\nknot\nWarning in splines::bs(x, df, ...): all interior knots match right boundary\nknot\nWarning in splines::bs(x, df, ...): all interior knots match right boundary\nknot\nWarning in splines::bs(x, df, ...): all interior knots match right boundary\nknot\n\n\n\n\n\n\n\n\n\n\n\n6.5.5 Corr√©lation entre les topics\nPour finir, on s‚Äôint√©resse √† la corr√©lation entre les topics.\n\nlibrary(reshape2)\n\n\nAttachement du package : 'reshape2'\n\n\nL'objet suivant est masqu√© depuis 'package:tidyr':\n\n    smiths\n\n#On cr√©e les titres/noms des topics\nb&lt;-NULL\nfor (i in seq_along((1:k)))\n{\n  a&lt;-paste0(model.stm.labels$score[i,1:3], collapse = \" \")\n  a&lt;-paste(\"Topic\",i,a)\nb&lt;-rbind(b,a)\n}\nlabel&lt;-as.data.frame(b)\nlabel\n\n                                           V1\na                      Topic 1 site super top\na.1           Topic 2 recommander colis jouet\na.2 Topic 3 livraison perroquet bien_emballer\na.3       Topic 4 commander tr√®s_bien qualit√©\na.4             Topic 5 dire envoyer vraiment\na.5         Topic 6 recevoir faire satisfaire\na.6              Topic 7 bien article service\na.7             Topic 8 avoir s√©rieux trouver\na.8      Topic 9 livraison_rapide rapide tr√®s\na.9                  Topic 10 oiseau plus pas\n\ntopicor&lt;-topicCorr(model.stm, method = \"simple\",verbose = TRUE) # calcul des corr√©lations\nadjmatrix &lt;-topicor[[3]] #on r√©cup√®re les corr√©lations\n\n#calcul des theta moyens par topic\ntheta &lt;-model.stm[[7]]\nthetat&lt;-melt(theta)\nthetat&lt;-thetat %&gt;%group_by(Var2)%&gt;%summarise(mean=mean(value))\n\nlibrary(igraph)\n\n\nAttachement du package : 'igraph'\n\n\nL'objet suivant est masqu√© depuis 'package:seededlda':\n\n    sizes\n\n\nL'objet suivant est masqu√© depuis 'package:quanteda.textplots':\n\n    as.igraph\n\n\nLes objets suivants sont masqu√©s depuis 'package:lubridate':\n\n    %--%, union\n\n\nLes objets suivants sont masqu√©s depuis 'package:dplyr':\n\n    as_data_frame, groups, union\n\n\nLes objets suivants sont masqu√©s depuis 'package:purrr':\n\n    compose, simplify\n\n\nL'objet suivant est masqu√© depuis 'package:tidyr':\n\n    crossing\n\n\nL'objet suivant est masqu√© depuis 'package:tibble':\n\n    as_data_frame\n\n\nLes objets suivants sont masqu√©s depuis 'package:stats':\n\n    decompose, spectrum\n\n\nL'objet suivant est masqu√© depuis 'package:base':\n\n    union\n\n#cr√©ation du graphe des corr√©lations\ng&lt;-graph_from_adjacency_matrix(adjmatrix, mode = \"lower\", weighted = TRUE, diag = FALSE)\ng &lt;- delete.edges(g, E(g)[ abs(weight) &lt; 0.05])\n\nWarning: `delete.edges()` was deprecated in igraph 2.0.0.\n‚Ñπ Please use `delete_edges()` instead.\n\ncurve_multiple(g)\n\n [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[39] 0\n\nset.seed(2021)\nplot(g,\n     # layout=layout_with_fr,  \n     margin = c(0, 0, 0, 0),\n     edge.width=abs(E(g)$weight)*15,\n     edge.color=ifelse(E(g)$weight &gt; 0, \"grey60\",\"red\"),\n     vertex.label=label$V1,\n     vertex.color = adjustcolor(\"pink2\", alpha.f = .2),vertex.label.cex=0.7, vertex.size=400*thetat$mean, vertex.frame.color= \"white\"\n     )\n\nWarning in v(graph): Non-positive edge weight found, ignoring all weights\nduring graph layout.\n\n\n\n\n\n\n\n\n\nEt en bonus, un petit graphique pour repr√©senter les mots les plus contributifs aux topics :\n\nlibrary(broom)\ntd_beta &lt;- tidy(model.stm,log=FALSE)\n\ntd_beta&lt;-td_beta %&gt;% \n  mutate(topic=as.factor(topic))%&gt;%\n  mutate(topic=`levels&lt;-.factor`(td_beta$topic,label$V1))\n\ntd_beta %&gt;%\n    group_by(topic) %&gt;%\n    top_n(15, beta) %&gt;%\n    ungroup() %&gt;%\n    ggplot(aes(reorder(term,beta), beta)) +\n    geom_col(fill=\"firebrick\") +theme_minimal()+\n    facet_wrap(~ topic, scales = \"free\") + labs(x=NULL)+\n    coord_flip()",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>*Topic Analysis*</span>"
    ]
  },
  {
    "objectID": "chapter7.html",
    "href": "chapter7.html",
    "title": "7¬† Vectorisation",
    "section": "",
    "text": "7.1 Introduction\nC‚Äôest sans doute l‚Äôid√©e la plus novatrice que l‚Äôapproche computationnelle du langage a apport√© ces 10 derni√®res ann√©es. Le mod√®le word2vec de Mikolov(2013) en est une premi√®re version, d‚Äôautres ont apport√© des am√©lioration comme le mod√®le Glove.\nL‚Äôid√©e fondamentale est qu‚Äôon peut repr√©senter des mots dans un espace de grande dimension par des vecteurs. Ce qui importe c‚Äôest de conserver la relation entre mots dans cet espace. Deux mots tr√®s corr√©l√©s, au sens de leur cooccurences, doivent l‚Äô√™tre avec la m√™me intensit√© dans cet espace. Admettant que le cosinus de l‚Äôangle entre deux vecteurs est √©quivalent √† leur corr√©lation, on comprend ais√©ment que la vectorisation consiste √† identifier un jeu de coordonn√©es, les param√®tres des vecteurs mots, en connaissant les angles qu‚Äôis forment entre eux.\nPour estimer les coordonn√©e des vecteurs deux m√©thodes peuvent √™tre employ√©e simultan√©ement.\nL‚Äôid√©e de plongement lexical tient alors dans cette dynamique double d‚Äôidentification et de rattachament des √©l√©ments textuels ensembles, selon diff√©rentes m√©thodes de vraisemblance/mesure.\nLe caract√®re remarquable de la m√©thode c‚Äôest qu‚Äôil est posible d‚Äôop√©rer des op√©rations alg√©briques, l‚Äôexemple canonique est celui de : reine = Roi+Homme - Femme\nPour la mise en oeuvre on emploie le package WordVectors de BenJamin Schmidt.",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Vectorisation</span>"
    ]
  },
  {
    "objectID": "chapter7.html#introduction",
    "href": "chapter7.html#introduction",
    "title": "7¬† Vectorisation",
    "section": "",
    "text": "Les mots observ√©s, dont on peut pr√©dire le contexte (Skip-gram)\nLes √©l√©ments du contexte observ√©s, dont on peut pr√©dire le mot (CBOW)\n\n\n\n\n\nWord Embeddings",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Vectorisation</span>"
    ]
  },
  {
    "objectID": "chapter7.html#les-donn√©es",
    "href": "chapter7.html#les-donn√©es",
    "title": "7¬† Vectorisation",
    "section": "7.2 Les donn√©es",
    "text": "7.2 Les donn√©es\nOn repart du vocabulaire pr√©par√© au chapitre 6. On lemmatise, on ne garde que les mots signifiants. On cr√©era les n-gramms directement dans la vectorisation. On sauvegarde le tout en format .txt pour pouvoir ensuite l‚Äôinjecter dans le mod√®le.\n\ndata &lt;- read_csv(\"data/data_trustpilot_oiseaux.csv\")\n\nRows: 4388 Columns: 7\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (4): auteur, date, month, comments\ndbl (3): id, year, note\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata&lt;-data%&gt;%mutate(text_id=paste0(\"text_\", row_number(data$id)))\n\nann_token&lt;-read_rds(\"data/annotation_oiseaux.rds\")\n\n\ndata&lt;-ann_token%&gt;%\n  filter(upos==\"NOUN\"|upos==\"VERB\"|upos==\"ADJ\"|upos==\"ADV\")%&gt;%\n  group_by(doc_id)%&gt;%\n  summarise(text=paste(lemma,collapse = \" \"))%&gt;%\n  inner_join(data, join_by(\"doc_id\"==\"id\"))\n\ncorpus_new&lt;-corpus(data, text_field = \"text\")\ntoks&lt;-tokens(corpus_new)%&gt;%\n  tokens_replace(c(\"produire\", \"conformer\",\"colir\", \"tre\", \"livrer\", \"n\"), c(\"produit\", \"conforme\",\"colis\", \"tr√®s\", \"livraison\", \"ne\"))%&gt;%\n  tokens_remove(c(\".\",\",\"))\n\nfoo1&lt;-data.frame(\n  id = seq_along(toks),\n  text = sapply(toks, paste, collapse = \" \"),\n  row.names = NULL\n)\n\n#on g√©n√®re le fichier de ces textes \"purifi√©s\"\nwrite.table(foo1, file=\"data/textes.txt\")",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Vectorisation</span>"
    ]
  },
  {
    "objectID": "chapter7.html#vectoriser-le-texte",
    "href": "chapter7.html#vectoriser-le-texte",
    "title": "7¬† Vectorisation",
    "section": "7.3 Vectoriser le texte",
    "text": "7.3 Vectoriser le texte\nOn commence par pr√©parer le texte pour l‚Äôalgorithme, avec le package ‚ÄòwordVectors‚Äô.\n\n# install.packages(\"remotes\")\n# remotes::install_github(\"bmschmidt/wordVectors\")\nlibrary(wordVectors)\n\n#Nettoyage des tweets et identification des n-grammes en vue d'entra√Æner le mod√®le\nprep_word2vec(origin=\"data/textes.txt\",destination=\"data/textes_vec.txt\",lowercase=T,bundle_ngrams=3)\n\nBeginning tokenization to text file at data/textes_vec.txt\n\n\nPrepping data/textes.txt\n\n\nStarting training using file data/textes_vec.txt\n\nVocab size (unigrams + bigrams): 21443\nWords in train file: 50975\nStarting training using file data/textes_vec.txt_\nWords processed: 100K     Vocab size: 40K  \nVocab size (unigrams + bigrams): 21490\nWords in train file: 101482\n\n\nOn entra√Æne ensuite le mod√®le.\n\n#Cr√©ation et entra√Ænement du mod√®le vectoriel\n\nmodel = train_word2vec(\"data/textes_vec.txt\",\n                       \"data/textes.bin\",\n                       vectors=200,threads=3,\n                       window=5,\n                       iter=10,negative_samples=0,\n                       force=TRUE, \n                       min_count=30)\n\nStarting training using file D:/NLP_ED/data/textes_vec.txt\nVocab size: 192\nWords in train file: 29198\n\n\nFilename ends with .bin, so reading in binary format\n\n\nReading a word2vec binary file of 192 rows and 200 columns\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |=========                                                             |  14%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |============================================                          |  64%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |===============================================================       |  91%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n\n\nNous avons un vocabulaire de 192 pour 29 198 mots dans le fichier d‚Äôentra√Ænement. Il se pr√©sente sous la forme d‚Äôun tableau de 192 lignes et 200 colonnes.",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Vectorisation</span>"
    ]
  },
  {
    "objectID": "chapter7.html#exploiter-les-r√©sultats",
    "href": "chapter7.html#exploiter-les-r√©sultats",
    "title": "7¬† Vectorisation",
    "section": "7.4 Exploiter les r√©sultats",
    "text": "7.4 Exploiter les r√©sultats\nPour exploiter cette repr√©sentation, une premi√®re mani√®re de faire est de rechercher dans le corpus les termes les plus associ√©s √† un terme cible. Quel est son contexte le plus proche ? Des fonctions pratiques sont propos√©es dans le package. la principale closest_to qui permet de selectionner les termes les plus proches, en termes de cosinus, du vecteur cible.\nDans l‚Äôexemple suivant, on cherche √† mieux saisir le concept de ‚Äúlivraison‚Äù. On examine les trentes termes les plus proches.\n\nfoo&lt;-model %&gt;% \n  closest_to(~\"livraison\",31)%&gt;%\n  filter(word!=\"livraison\")%&gt;%#on choisit les 30 termes les plus proches, sauf livraison\n  rename(similarity=2)\n\nggplot(foo, aes(x=reorder(word,similarity),y=similarity))+\n  geom_point(col=\"black\",size=3)+\n  coord_flip()+\n  ggtitle(\"N-grammes similaires √† livraison\")\n\n\n\n\n\n\n\n\nExercice : r√©p√©ter l‚Äôop√©ration sur ‚Äúperroquet‚Äù\n\n\nSolution\nfoo&lt;-model %&gt;% \n  closest_to(~\"perroquet\",31)%&gt;%\n  filter(word!=\"perroquet\")%&gt;%#on choisit les 30 termes les plus proches, sauf livraison\n  rename(similarity=2)\n\nggplot(foo, aes(x=reorder(word,similarity),y=similarity))+\n  geom_point(col=\"black\",size=3)+\n  coord_flip()+\n  ggtitle(\"N-grammes similaires √† perroquet\")\n\n\n\n\n\n\n\n\n\nOn peut affiner les concepts en faisant la somme de plusieurs mots.\n\nfoo&lt;-model %&gt;% \n  wordVectors::closest_to(~(\"perroquet\"+\"perruche\"),32)%&gt;%\n  filter(word!=\"perroquet\"&word!=\"perruche\")%&gt;%#on choisit les 30 termes les plus proches, sauf livraison\n  rename(similarity=2)\n\nggplot(foo, aes(x=reorder(word,similarity),y=similarity))+\n  geom_point(col=\"black\",size=3)+\n  coord_flip()+\n  ggtitle(\"N-grammes similaires √† perroquet\")\n\n\n\n\n\n\n\n\nEt on peut √©galement soustraire des concepts les uns aux autres.\n\nfoo&lt;-model %&gt;% \n  wordVectors::closest_to(~(\"probl√®me\"-\"livraison\"),32)%&gt;%\n  rename(similarity=2)\n\nggplot(foo, aes(x=reorder(word,similarity),y=similarity))+\n  geom_point(col=\"black\",size=3)+\n  coord_flip()+\n  ggtitle(\"N-grammes similaires √† probl√®me, sans la livraison\")",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Vectorisation</span>"
    ]
  },
  {
    "objectID": "chapter7.html#cr√©er-des-groupes",
    "href": "chapter7.html#cr√©er-des-groupes",
    "title": "7¬† Vectorisation",
    "section": "7.5 Cr√©er des groupes",
    "text": "7.5 Cr√©er des groupes\nOn peut aussi cr√©er des groupes. On verra la m√©thode classique de clustering et la m√©thode t-sne.\n\n7.5.1 Clustering\n\nq_words = c(\"livraison\", \"probl√®me\")\nterm_set = lapply(q_words, \n                  function(q_word) {\n                    nearest_words = model %&gt;% closest_to(model[[q_word]],80)\n                    nearest_words$word\n                  }) %&gt;% unlist\nsubset = model[[term_set,average=F]]\n\nsubset1&lt;-as.data.frame(subset@.Data)\n\n# un calcul de dissimilarit√© sur la base des cosinus\n#la fonction habituel dist ne le permetpas\nMatrix &lt;- as.matrix(subset1)\nsim &lt;- Matrix / sqrt(rowSums(Matrix * Matrix))\nsim &lt;- sim %*% t(sim)\n#on transforme en distance la similarit√© cosinus, celle ci varie de 0 √† 2.\nD_sim &lt;- as.dist(1 - sim)\n\n\n#un clustering hi√©rarchique avec 10 groupes\n\nclus&lt;-hclust(D_sim)\ngroupes&lt;- cutree(clus,k=10)\nlibrary(ggdendro)\nggdendrogram(clus, rotate=TRUE ,type = \"triangle\")\n\n\n\n\n\n\n\nddata &lt;- dendro_data(clus, type = \"triangle\")\nggplot(segment(ddata)) + \n  geom_segment(aes(x = x, y = y, xend = xend, yend = yend)) + \n  coord_flip()+\n    geom_text(data = ddata$labels, \n              aes(x = x, y = y, label = label), size = 2, vjust = 0)\n\n\n\n\n\n\n\n\n\n\n7.5.2 t-sne\n\nlibrary(Rtsne)\nlibrary(RColorBrewer)\n# run Rtsne with default parameters\nset.seed(57)\nrtsne_out &lt;- Rtsne(as.matrix(subset), perplexity=25)\n# plot the output of Rtsne\n#jpeg(\"fig.jpg\", width=2400, height=1800)\ncolor.vec = c(\"#556270\", \"#4ECDC4\", \"#1B676B\", \"#FF6B6B\", \"#C44D58\", \"seagreen1\", \"seagreen4\", \"slateblue4\", \"firebrick\", \"Royalblue\")\n\n#des manip pour associer les groupe du clustering aux termes et √† la leur coordonn√©e dans tsne.\ngroupes&lt;-as.data.frame(groupes)\ngroupes$word&lt;-rownames(groupes)\nterms&lt;-as.data.frame(rownames(subset))\nterms$word&lt;-terms[,1] \nterms&lt;-terms %&gt;% left_join(groupes, by = \"word\")\nplot(rtsne_out$Y, t='n')\n#count(terms, clus)$n[2]\ntext(rtsne_out$Y, labels=rownames(subset),cex=0.8,col=color.vec[terms$groupes])",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Vectorisation</span>"
    ]
  },
  {
    "objectID": "chapter8.html",
    "href": "chapter8.html",
    "title": "8¬† LLM",
    "section": "",
    "text": "8.1 Introduction\nPour finir ce cours, on va s‚Äôint√©resser aux derniers d√©veloppements du NLP, les LLMs (Large Langage Models), fond√©s sur une architecture de Transformers et du m√©canisme de l‚Äôattention.\nL‚Äôid√©e est d‚Äôentra√Æner un mod√®le de langue, sur de tr√®s larges corpus, pour ensuite l‚Äôutiliser dans des t√¢ches sp√©cifiques.\nIl ne s‚Äôagit pas ici de r√©entra√Æner des mod√®les, ce qui est une approche tout √† fait pertinente lorsque l‚Äôon est face √† des corpus au langage sp√©cifique, mais d‚Äôutiliser les outils existants directement disponibles.\nPour trouver le bon mod√®le √† utiliser, il existe Hugging Face. La plupart des √©l√©ments de code sont en python, mais il commence √† exister des impl√©mentations en R.\nLe probl√®me v√©ritable repose sur les temps de calcul et la puissance disponible. On recommande d‚Äôavoir acc√®s √† un GPU, ce qui permet de consid√©rablement raccourcir le temps des traitements. Les outils en R n‚Äôont pas encore impl√©ment√© le recours au GPU, et nos ordis portables n‚Äôen ont pas forc√©ment. On peut avoir recours au cluster de calcul de son universit√©, ou utiliser les service de Google ou OpenAI (souvent, moyennant finance).\nOn va utiliser du code en python et du code en R pour analyser les r√©sultats. On travaille sur un tout petit corpus, pour limiter les temps de calcul. On pourra garder un oeil sur le package R ‚Äòtext‚Äô qui propose beaucoup d‚Äôoutils mais est encore en construction.\n(Pour red√©marrer la session R : command/ctrl + shift + F10)",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>LLM</span>"
    ]
  },
  {
    "objectID": "chapter8.html#introduction",
    "href": "chapter8.html#introduction",
    "title": "8¬† LLM",
    "section": "",
    "text": "transformers\n\n\n\n\n\n\nExemples",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>LLM</span>"
    ]
  },
  {
    "objectID": "chapter8.html#bertopic",
    "href": "chapter8.html#bertopic",
    "title": "8¬† LLM",
    "section": "8.2 BERTopic",
    "text": "8.2 BERTopic\nUn classique du genre : BERTopic\n\nfrom bertopic import BERTopic\n\nWARNING:tensorflow:From C:\\Users\\Super\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n\nfrom bertopic.representation import KeyBERTInspired\n\nimport pandas as pd\n\ndf = r.data\nprint(df)\n\n          id  ...                                           comments\n0        1.0  ...  Comme toujours, super service.\\nNe changez rie...\n1        2.0  ...  Le d√©lai de ma commande super rapide Le d√©lais...\n2        3.0  ...  Produits de qualit√© et √©quipe professionnelle ...\n3        4.0  ...  Envoi rapide, bien emball√© et conforme √† l'ann...\n4        5.0  ...  Exp√©dition internationale ! J'ai r√©cemment d√©m...\n...      ...  ...                                                ...\n4383  4384.0  ...  Tr√®s rapide, et arriv√© en bon √©tat Envois tr√®s...\n4384  4385.0  ...  Satisfaite Tr√®s bon produit et respect des d√©l...\n4385  4386.0  ...  efficacit√© et rapidit√© Tr√®s heureuse d'avoir r...\n4386  4387.0  ...  Oui mais Le produit achet√© est tr√®s bien, la l...\n4387  4388.0  ...  Choix et rapidit√© Du choix propos√© sur le site...\n\n[4388 rows x 7 columns]\n\ntopic_model = BERTopic(language=\"multilingual\")\ntopics, probs = topic_model.fit_transform(df['comments'])\ntopic_model.get_topic_info()\n\n    Topic  ...                                Representative_Docs\n0      -1  ...  [Tr√®s tr√®s bien tr√®s satisfaite des produits e...\n1       0  ...  [Tr√®s bon site pour oiseaux, Tr√®s satisfait me...\n2       1  ...  [Je recommande  ce site, Je recommande ce site...\n3       2  ...   [Service rapide, Service rapide, Service rapide]\n4       3  ...  [Ma commande n a pas pu √™tre honor√©e dans les ...\n..    ...  ...                                                ...\n56     55  ...  [Produit conforme et livraison rapide, Utilis√©...\n57     56  ...  [Je suis jamais d√©√ßue la livraison se passe to...\n58     57  ...  [Excellent - tout s'est tr√®s bien pass√©, je re...\n59     58  ...  [D√©lais court, emballage correct. Commande re√ß...\n60     59  ...  [Livraison rapide! Tout bon! Je recommande., J...\n\n[61 rows x 5 columns]\n\ntopic_model.get_topic(8)\n\n[('articles', 0.10211028465762363), ('article', 0.060884242655030674), ('description', 0.021620093578803835), ('correspondant', 0.019784195046463393), ('des', 0.018246064088917685), ('rapidement', 0.018099480469657134), ('conformes', 0.017285211053008398), ('mes', 0.015389113267492663), ('les', 0.014979100322801916), ('et', 0.014849319284039132)]\n\ntopic_model.get_topic_freq().head()\n\n    Topic  Count\n5      -1   1248\n2       0    432\n4       1    358\n12      2    191\n1       3    116\n\ntopic_model.get_document_info(df['comments'])\n\n                                               Document  ...  Representative_document\n0     Comme toujours, super service.\\nNe changez rie...  ...                    False\n1     Le d√©lai de ma commande super rapide Le d√©lais...  ...                    False\n2     Produits de qualit√© et √©quipe professionnelle ...  ...                    False\n3     Envoi rapide, bien emball√© et conforme √† l'ann...  ...                    False\n4     Exp√©dition internationale ! J'ai r√©cemment d√©m...  ...                    False\n...                                                 ...  ...                      ...\n4383  Tr√®s rapide, et arriv√© en bon √©tat Envois tr√®s...  ...                    False\n4384  Satisfaite Tr√®s bon produit et respect des d√©l...  ...                    False\n4385  efficacit√© et rapidit√© Tr√®s heureuse d'avoir r...  ...                    False\n4386  Oui mais Le produit achet√© est tr√®s bien, la l...  ...                    False\n4387  Choix et rapidit√© Du choix propos√© sur le site...  ...                    False\n\n[4388 rows x 8 columns]\n\ntopic_model.find_topics(\"r√©clamation\")\n\n([16, 42, 43, 22, 9], [0.5475961, 0.50152946, 0.4991401, 0.4750504, 0.47463918])\n\ntopic_model.generate_topic_labels()\n\n['-1_et_tr√®s_de', '0_oiseaux_mania_pour', '1_site_ce_recommande', '2_service_client_bon', '3_ai_le_pas', '4_livraison_rapide_tr√®s', '5_livraison_produit_rapide', '6_ma_commande_merci', '7_rien_dire_redire', '8_articles_article_description', '9_merci_vos_bien', '10_recommande_je_recommander', '11_frais_port_peu', '12_jouets_pour_les', '13_rapidement_commande_re√ßue', '14_envoi_envoie_rapide', '15_qualit√©_livraison_produits', '16_tres_qualite_bon', '17_satisfait_satisfaite_tr√®s', '18_graines_les_berries', '19_livraison_rapide_conforme', '20_ma_commande_client', '21_√©tait_le_sac', '22_parfait_rapide_tres', '23_t√©l√©phone_t√©l√©phonique_au', '24_efficace_efficacit√©_rapide', '25_commande_rapidement_arriv√©e', '26_produit_bon_excellent', '27_produits_bon_bons', '28_parfait_tout_simplement', '29_livraison_rapide_art', '30_prix_choix_produits', '31_exp√©dition_expedition_rapide', '32_achat_transaction_mon', '33_commande_exp√©dition_traitement', '34_produits_bons_titi', '35_magasin_vendeur_prix', '36_d√©lais_temps_dans', '37_nickel_dire_adresse', '38_trop_long_longue', '39_attentes_mes_attente', '40_s√©rieux_r√©actif_rapidit√©', '41_comme_habitude_toujours', '42_tres_produit_bon', '43_prestation_bien_branche', '44_recommande_produit_bon', '45_pas_deux_ampoule', '46_arriv√©_√©tat_rapidement', '47_animaux_v√©t√©rinaire_perruche', '48_ras_respect√©_satisfaction', '49_site_livraison_ce', '50_dans_04_commande', '51_cage_√©l√©gante_une', '52_recommande_soign√©e_bcp', '53_jours_re√ßue_moins', '54_envoi_informations_recommande', '55_conforme_description_produit', '56_satisfait_d√©lai_suis', '57_pass√©_tout_vivement', '58_emballage_serrai_mol', '59_recommande_√©tablissement_int√©ress√©s']\n\ntopic_model.visualize_topics()\n\n                        \n                                            \n\ntopic_model.visualize_heatmap()",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>LLM</span>"
    ]
  },
  {
    "objectID": "chapter8.html#zero-shot-classification",
    "href": "chapter8.html#zero-shot-classification",
    "title": "8¬† LLM",
    "section": "8.3 Zero-Shot Classification",
    "text": "8.3 Zero-Shot Classification\nOn utilise python pour faire la classification (ici, les sentiments) :\n\nimport torch\nprint(torch.__version__)\n\n2.3.0+cu118\n\nprint(torch.cuda.is_available())\n\nTrue\n\ntorch.zeros(1).cuda()\n\ntensor([0.], device='cuda:0')\n\n\nfrom transformers import pipeline\nclassifier = pipeline(\"zero-shot-classification\", model=\"mtheo/camembert-base-xnli\", device=-1)\n\n# load in pandas and numpy for data manipulation\nimport pandas as pd\nimport numpy as np\n\n\ndata = pd.read_csv(\"data/data_trustpilot_oiseaux.csv\")\n\ndf=pd.DataFrame(data.iloc[10:20])\n\n\nlabels = [\"positif\", \"n√©gatif\", \"neutre\"]\n\n# Fonction pour pr√©dire la classification zero-shot pour un texte donn√©\ndef predict_sentiment_for_text(text, text_labels):\n    result = classifier(text, text_labels, multi_label=True)\n    label_scores = {label: score for label, score in zip(result['labels'], result['scores'])}\n    return label_scores\n\n# Appliquer la fonction √† la colonne de texte et stocker les r√©sultats dans une nouvelle colonne\n\ndf['sentiment_results'] = df.apply(lambda row: {'id': row['id'], **predict_sentiment_for_text(row['comments'], labels)}, axis=1)\n# Convertir les r√©sultats de dictionnaire en colonnes s√©par√©es\nresults_df = pd.json_normalize(df['sentiment_results'])\n\nfinal_df = pd.merge(df, results_df, on='id')\n\nfinal_df.to_csv('data/test_ZS_result.csv')\n\nOn r√©cup√®re les r√©sultats dans R et on regarde ce que √ßa donne.\n\ndf&lt;-py$final_df\n\ndf%&gt;%select(positif, n√©gatif, neutre)%&gt;%pivot_longer(everything())%&gt;%\n  ggplot()+\n  geom_violin(aes(x=name, y=value, fill=name), show.legend = F, scale = \"width\", trim=F)+\n  coord_flip()+\n  labs(x=NULL, y=NULL)\n\n\n\n\n\n\n\ndf%&gt;%select(positif, n√©gatif, neutre)%&gt;%pivot_longer(everything())%&gt;%\n  ggplot()+\n  geom_boxplot(aes(name, value, fill=name), show.legend = F)+\n  coord_flip()+\n  labs(x=NULL, y=NULL)\n\n\n\n\n\n\n\ndf2&lt;-df%&gt;%select(note, positif, n√©gatif, neutre)%&gt;%\n  pivot_longer(-note, names_to = \"topic\", values_to = \"value\")\n\nggplot(df2,aes(x=note, y=value, group=topic))+\n  geom_bar(position=\"fill\",stat=\"identity\", aes(fill=topic))+\n  scale_fill_brewer(palette=\"Spectral\")+\n  theme_minimal()",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>LLM</span>"
    ]
  },
  {
    "objectID": "chapter8.html#g√©n√©ration-de-texte",
    "href": "chapter8.html#g√©n√©ration-de-texte",
    "title": "8¬† LLM",
    "section": "8.4 G√©n√©ration de texte",
    "text": "8.4 G√©n√©ration de texte\nUn exemple avec gpt2\n\nimport torch\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\n\n# initialize tokenizer and model from pretrained GPT2 model from Huggingface\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=tokenizer.eos_token_id)\n\n# sentence\nsequence = \"What is AI?\"\n# encoding sentence for model to process\ninputs = tokenizer.encode(sequence, return_tensors='pt')\n\n# generating text\noutputs = model.generate(inputs, max_length=200, do_sample=True, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n\n# decoding text\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n# printing output\nprint(text)\n\nWhat is AI?\n\nAI is a form of artificial intelligence that can be used to perform complex tasks. It is often referred to as machine learning or AI, but it is not a scientific term. Artificial intelligence is being used as a way to improve the way we think and interact with the world around us. For example, it has been shown that humans can learn from other humans in a number of ways, such as how they respond to a situation or what they say to each other. In this way, AI can improve our understanding of our world and our relationships with other people, as well as our ability to make decisions about how to live our lives. AI also has the potential to change how we live and what we do in the future. This is especially true when it comes to health, education, healthcare, and the environment, which can all be affected by the use of AI. However, there are still a lot of unanswered questions about what AI will do for us and how it",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>LLM</span>"
    ]
  },
  {
    "objectID": "webscraping.html",
    "href": "webscraping.html",
    "title": "Appendix A ‚Äî Webscraping",
    "section": "",
    "text": "library(tidyverse)\nlibrary(scales)\nlibrary(quanteda)\nlibrary(quanteda.textstats)\nlibrary(httr)\nlibrary(rvest)\nlibrary(polite)\n\n### Fichier TP_pets_website.csv\n\n##### Scraping sites web\nurl_start&lt;-\"https://fr.trustpilot.com/categories/animals_pets\"\n\ntrustpilot_website&lt;-function(url_start){\n\nsession &lt;- bow(url_start)\nsession$user_agent&lt;-\"Googlebot\"\nmessage(\"Scraping \", url_start)\npage&lt;-nod(session, url_start) %&gt;% \n  scrape(verbose=TRUE)\ni&lt;-page%&gt;%html_elements(\".styles_paginationWrapper__fukEb\")%&gt;%\n  html_element(\"a.button_button__T34Lr:nth-child(5)\")%&gt;%\n  html_text()%&gt;%as.numeric()\nwebsite &lt;- NULL\n\nfor (j in 1:i){\n  url&lt;-paste0(url_start,\"?page=\",j)\n  Sys.sleep(5)\n  session &lt;- bow(url)\n  session$user_agent&lt;-\"Googlebot\"\n  message(\"Scraping \", url)\n  page&lt;-nod(session, url) %&gt;% \n      scrape(verbose=TRUE)\n    \n    company_card &lt;- page %&gt;%\n      html_elements(\"div.styles_wrapper__2JOo2:nth-of-type(n+4)\")\n    \n    website_name &lt;- company_card %&gt;%\n      html_element(\"p.typography_heading-xs__jSwUz\") %&gt;%\n      html_text()\n    \n    nb_avis &lt;- company_card %&gt;%\n      html_element(\"p.typography_body-m__xgxZ_\")%&gt;%\n      html_text()\n    \n    localisation &lt;- company_card%&gt;%\n      html_element(\"span.styles_metadataItem__Qn_Q2\")%&gt;%\n      html_text()\n\n    type &lt;- company_card %&gt;%\n      html_element(\"div.styles_desktop__U5iWw\") %&gt;%\n      html_text()\n    \n    lien &lt;- paste0(\"https://fr.trustpilot.com\", company_card %&gt;%\n               html_element(\"a\")%&gt;%html_attr(\"href\"))\n      \n    \n    website &lt;- rbind(website, data.frame(\n      website_name = website_name, \n      nb_avis = nb_avis,\n      localisation = localisation,\n      type = type,\n      lien = lien\n    ))\n  print(paste(\"page\",j, \"has been scraped\"))\n    \nj&lt;-j+1\n\n}\nreturn(website)\n\n}\n\nwebsite&lt;-trustpilot_website(url_start = url_start)\n\n##### Cr√©ation du fichier TP_pets_website.csv\nwebsite&lt;-website%&gt;%mutate(note=str_split_i(nb_avis,\"\\\\|\", 1)%&gt;%\n                      str_remove_all(., \"[A-z]\")%&gt;%\n                      str_replace(., \",\", \".\")%&gt;%\n                      as.numeric(),\n                    nb_avis=str_split_i(nb_avis,\"\\\\|\",2)%&gt;%\n                      str_remove_all(., \"[A-z]\")%&gt;%\n                      str_remove_all(., \"[:space:]\")%&gt;%\n                      as.numeric()%&gt;%\n                      replace_na(.,0),\n                    url_start=url_start,\n                    nb_page=0,\n                    cat=str_split(type, \"¬∑\"))%&gt;%\n  unnest_wider(cat, names_sep = \"_\")\n\ndata_scrap&lt;-website%&gt;%filter(nb_avis&gt;10)\nwrite_csv(data_scrap, \"TP_pets_website.csv\")\n\n\n### Fichier TP_pets_reviews.rds\n\n##### Scraping reviews\n\ntrustpilot_reviews&lt;-function(data){\n  Sys.sleep(5)\n  \n  for (j in 1:nrow(data)) {\n    i&lt;-1\n    b&lt;-1\n    \n    \n    while (b!=\"TRUE\") {\n      \n      Sys.sleep(5)\n      \n      \n      b&lt;-http_error(paste0(data$lien[j], \"?languages=all&page=\", i))\n      \n      i&lt;-i+1\n      data$nb_page[j]&lt;-i-2\n      \n    }\n    print(paste0(\"nb_page of \", data$website_name[j], \" has been fetched\"))\n    \n  }\n  \n  i&lt;-1\n  reviews &lt;- NULL\n  # cat(\"\\014\")\n  cat(paste0(\"The script will run on \", sum(data$nb_page), \" pages!\\n\"))\n  Sys.sleep(5)\n  \n  \n  for (j in 1: nrow(data)){\n    for (i in 1:data$nb_page[j]){\n      url&lt;-paste0(data$lien[j],\"?languages=all&page=\",i)\n      Sys.sleep(5)\n      session &lt;- bow(url)\n      session$user_agent&lt;-\"Googlebot\"\n      message(\"Scraping \", url)\n      page&lt;-nod(session, url) %&gt;% \n        scrape(verbose=TRUE)\n      \n      review_card &lt;- page %&gt;%\n        html_elements(\"div.styles_reviewCardInner__EwDq2\")\n      \n      name &lt;- review_card %&gt;%\n        html_element(\"span.typography_heading-xxs__QKBS8.typography_appearance-default__AAY17\") %&gt;%\n        html_text()\n      \n      rating &lt;- review_card %&gt;%\n        html_elements(\"div.star-rating_starRating__4rrcf.star-rating_medium__iN6Ty\") %&gt;%\n        html_element(\"img\")%&gt;%\n        html_attr(\"alt\")%&gt;%\n        str_extract(\"[:digit:]\")\n      \n      published &lt;- review_card%&gt;%\n        html_elements(\".styles_reviewContentwrapper__zH_9M\")%&gt;%\n        html_element(\"p.typography_body-m__xgxZ_\")%&gt;%\n        html_text()%&gt;%\n        str_remove(\"Date de l'exp√©rience: \")\n      \n      verified &lt;- review_card %&gt;%\n        html_element(\".styles_detailsIcon__yqwWi\") %&gt;%\n        html_text()\n      \n      title &lt;- review_card %&gt;%\n        html_element(\"h2\")%&gt;%\n        html_text()\n      \n      content &lt;- review_card%&gt;%\n        html_elements(\".styles_reviewContentwrapper__zH_9M\")%&gt;%\n        html_element(\"p.typography_body-l__KUYFJ\") %&gt;%\n        html_text2()\n      \n      \n      reviews &lt;- rbind(reviews, data.frame(\n        website_name = data$website_name[j],\n        name = name, \n        rating = rating,\n        published = published,\n        verified = verified,\n        title = title, \n        content = content\n      ))\n      \n      i&lt;-i+1\n    }\n    print(paste0(data$website_name[j], \" has been scraped\"))\n    \n    j&lt;-j+1\n  }\n\n  return(reviews)\n}\n\nhak&lt;-trustpilot_reviews(data_scrap)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Webscraping</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Balech, Sophie. 2022. ‚ÄúUne Application Du Mod√®le ELM ( Elaboration Likelihood Model ) Au Partage d‚Äôinformation Sur Twitter : √âtude Du R√¥le de La Forme Du Message Et Du Profil de l‚Äô√©metteur:‚Äù Innovations n¬∞ 69 (3): 129‚Äì61. https://doi.org/10.3917/inno.pr2.0135.\n\n\nBalech, Sophie, and Christophe Benavent. 2022. ‚ÄúLe R√¥le Des Dimensions de l‚Äôexp√©rience Dans La Satisfaction Client : Une Application Au Cas de l‚Äôindustrie H√¥teli√®re En Polyn√©sie Fran√ßaise.‚Äù In 38√®me Congr√®s Internation de l‚ÄôAFM, 28‚Äì38. Tunis, Tunisie.\n\n\nBarron, Alexander T. J., Jenny Huang, Rebecca L. Spang, and Simon DeDeo. 2018. ‚ÄúIndividuals, Institutions, and Innovation in the Debates of the French Revolution.‚Äù Proceedings of the National Academy of Sciences 115 (18): 4607‚Äì12. https://doi.org/10.1073/pnas.1717729115.\n\n\nBoegershausen, Johannes, Hannes Datta, Abhishek Borah, and Andrew T. Stephen. 2022. ‚ÄúFields of Gold: Scraping Web Data for Marketing Insights.‚Äù Journal of Marketing 86 (5): 1‚Äì20. https://doi.org/10.1177/00222429221100750.\n\n\nHartmann, Jochen, Mark Heitmann, Christina Schamp, and Oded Netzer. 2021. ‚ÄúThe Power of Brand Selfies.‚Äù Journal of Marketing Research 58 (6): 1159‚Äì77. https://doi.org/10.1177/00222437211037258.\n\n\nKozlowski, Austin C, Matt Taddy, and James A Evans. 2019. ‚ÄúThe Geometry of Culture: Analyzing Meaning Through Word Embeddings.‚Äù American Sociological Review 89 (5): 769‚Äì981. https://doi.org/10.1177/0003122419877135.\n\n\nKumar, Sunil, Arpan Kumar Kar, and P. Vigneswara Ilavarasan. 2021. ‚ÄúApplications of Text Mining in Services Management: A Systematic Literature Review.‚Äù International Journal of Information Management Data Insights 1 (1): 100008. https://doi.org/10.1016/j.jjimei.2021.100008.\n\n\nLefran√ßois, Alicia, Sophie Balech, and Sophie Changeur. 2022. ‚ÄúTransgression Et Consommation: Revue Int√©grative Et Proposition d‚Äôun Agenda de Recherche.‚Äù In. Le Havre, France.\n\n\nLiu, Angela Xia, Yilin Li, and Sean Xin Xu. 2021. ‚ÄúAssessing the Unacquainted: Inferred Reviewer Personality and Review Helpfulness.‚Äù MIS Quarterly 45 (3): 1113‚Äì48. https://doi.org/10.25300/MISQ/2021/14375.\n\n\nTirunillai, Seshadri, and Gerard J. Tellis. 2012. ‚ÄúDoes Chatter Really Matter? Dynamics of User-Generated Content and Stock Performance.‚Äù Marketing Science 31 (2): 198‚Äì215. https://doi.org/10.1287/mksc.1110.0682.\n\n\nVannoni, Matia. 2022. ‚ÄúA Political Economy Approach to the Grammar of Institutions: Theory and Methods.‚Äù Policy Studies Journal 50 (2): 453‚Äì71. https://doi.org/10.1111/psj.12427.",
    "crumbs": [
      "Appendices",
      "References"
    ]
  }
]