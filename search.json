[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NLP avec R pour les sciences sociales",
    "section": "",
    "text": "Introduction\nCeci est le support pour le cours “NLP avec R pour les sciences sociales” réalisé auprès des doctorants de l’école doctorale SHS de l’UPJV.\nL’objectif est de donner aux apprenants des méthodes et outils pour traiter de larges corpus de texte pour répondre à leur problématique de recherche.\nL’environnement de traitement des données est R et Rstudio.\nIci, une présentation en guise d’introduction.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#ressources-en-ligne",
    "href": "index.html#ressources-en-ligne",
    "title": "NLP avec R pour les sciences sociales",
    "section": "Ressources en ligne",
    "text": "Ressources en ligne\nR et RStudio\n\nhttps://www.r-project.org/\nhttps://rstudio.com/products/rstudio/\n\nDes ressources en ligne\n\nIntroduction à R et au tidyverse\nR for data science\nHands-On Programming with R\nText mining with R\nTutoriels Quanteda\nLes techniques du NLP pour la recherche en sciences de gestion\nNLP avec r et en français - un Manuel synthétique\nQuarto pour communiquer\nLe séminaire du Collège de France : Apprendre les langues aux machines - B. Sagot",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "1  Prise en main",
    "section": "",
    "text": "1.1 Introduction\nLe document de travail contient deux types d’éléments : du texte pour expliquer et présenter ce que l’on fait et du code pour réaliser les manipulations de données, les analyses et les graphiques. On commence toujours un document avec un bloc de code de setup, pour lister et charger les packages que l’on va utiliser et les options générales pour l’édition du document.\nLa publication des documents peut se faire en différents formats : html, word, pdf (via latex), présentation html (via revealjs), présentation powerpoint, présentation beamer(via latex), … On se référera au site de Quarto pour plus de détails.\nIci, on commence simplement avec quelques manipulations pour comprendre l’environnement de travail, puis on verra comment charger des données sous différents formats.",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Prise en main</span>"
    ]
  },
  {
    "objectID": "chapter1.html#premières-manipulations",
    "href": "chapter1.html#premières-manipulations",
    "title": "1  Prise en main",
    "section": "1.2 Premières manipulations",
    "text": "1.2 Premières manipulations\n\nCréer un document script (.R) : pour simplement éditer du code\nCréer un document quarto (.qmd) : pour mixer du code et du texte\nCommenter du code\nAfficher de l’aide sur une fonction",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Prise en main</span>"
    ]
  },
  {
    "objectID": "chapter1.html#charger-des-données",
    "href": "chapter1.html#charger-des-données",
    "title": "1  Prise en main",
    "section": "1.3 Charger des données",
    "text": "1.3 Charger des données\n\n1.3.1 Un tableau de données\nFichier .csv, .xlsx, .rds\n\ndata&lt;-read.csv(\"le/chemin/de/mon/fichier.csv\")\n\nlibrary(readxl)\ndata&lt;-read_xlsx(\"le/chemin/de/mon/fichier.xlsx\")\n\nlibrary(readr)\ndata&lt;- read_rds(\"le/chemin/de/mon/fichier.rds\")\n\n\n\n1.3.2 Une collection de fichier textes\nUn dossier avec plusieurs fichier .txt ou .docx ou .pdf\n\nlibrary(readtext)\n#Exemple de nom de document : \"int1_2024_dirigeant.txt\"\ndata&lt;-readtext(\"le/chemin/de/mes/fichiers/*.txt\",\n         docvarsfrom = \"filenames\", \n         docvarnames = c(\"int\", \"année\", \"type\"),\n         dvsep = \"_\")\n\ndata&lt;-readtext(\"le/chemin/de/mes/fichiers/*.docx\")\n\ndata&lt;-readtext(\"le/chemin/de/mes/fichiers/*.pdf\")\n\nPour les problèmes de mise en forme, on consultera la vignette du package readtext.\nUne autre solution pour les fichiers pdf, permettant d’enlever tous les éléments de mise en forme :\n\nlibrary(tm)\n#on récupère les noms des fichiers à lire depuis les dossiers\nfiles &lt;- list.files( pattern = \"pdf$\", recursive = T, include.dirs = T)\n\n#on lit les fichiers, sans la mise en forme\ncorp&lt;-Corpus(URISource(files),\n               readerControl = list(reader = readPDF, text=(\"-layout\")))\n#on enlève les sauts de page et autres mises en forme à partir d'espace\ncorp &lt;- tm_map(corp, stripWhitespace)\n\nUn autre outil pour les pdf : le package pdftools.\n\n\n1.3.3 Reconnaissance Optique des Caractères (OCR)\nPour ça, on utilise le package tesseract :\nExemple avec cette image : \n\nlibrary(tesseract)\ntesseract_download(\"fra\") #pour télécharger le modèle de langage\n\ntext &lt;- tesseract::ocr(\"N1_avril1909b.jpeg\", engine = \"fra\")\n\ncat(text) #pour afficher le texte avec sa mise en page",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Prise en main</span>"
    ]
  },
  {
    "objectID": "chapter2.html",
    "href": "chapter2.html",
    "title": "2  Pré-traitements",
    "section": "",
    "text": "2.1 Introduction\nLe document de travail contient deux types d’éléments : du texte pour expliquer et présenter ce que l’on fait et du code pour réaliser les manipulations de données, les analyses et les graphiques. On commence toujours un document avec un bloc de code de setup, pour lister et charger les packages que l’on va utiliser et les options générales pour l’édition du document.\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(quanteda)\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(RColorBrewer)\nDans un premier temps, nous allons tout simplement charger la base de données de travail puis la décrire. Ensuite, nous créerons un corpus, le visualisons. Puis nous effectueront quelques analyses liminaires, avant de voir les pré-traitements à réaliser sur le corpus.",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Pré-traitements</span>"
    ]
  },
  {
    "objectID": "chapter2.html#les-données",
    "href": "chapter2.html#les-données",
    "title": "2  Pré-traitements",
    "section": "2.2 Les données",
    "text": "2.2 Les données\n\n#On charge les données, stockées dans un fichier csv\n\ndata &lt;- read_csv(\"data/data_trustpilot_oiseaux.csv\")\n\nRows: 4388 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): auteur, date, month, comments\ndbl (3): id, year, note\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnames(data)\n\n[1] \"id\"       \"auteur\"   \"date\"     \"month\"    \"year\"     \"note\"     \"comments\"\n\nview(data)\ndata\n\n# A tibble: 4,388 × 7\n      id auteur                   date            month    year  note comments  \n   &lt;dbl&gt; &lt;chr&gt;                    &lt;chr&gt;           &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;     \n 1     1 MAQUET Cyril             07 août 2023    août     2023     5 \"Comme to…\n 2     2 Mme Laurence  Wolff      08 août 2023    août     2023     5 \"Le délai…\n 3     3 Une nouvelle cliente     07 août 2023    août     2023     5 \"Produits…\n 4     4 Patricia ALLAMAN         15 août 2023    août     2023     5 \"Envoi ra…\n 5     5 VPL                      24 juillet 2023 juillet  2023     5 \"Expéditi…\n 6     6 PHILIPPE GODIN           08 août 2023    août     2023     5 \"site sér…\n 7     7 Mme MARIA ADILIA PEREIRA 15 août 2023    août     2023     1 \"deux sem…\n 8     8 Rachel Mattyssen         31 juillet 2023 juillet  2023     5 \"Très bie…\n 9     9 Estelle Fay              16 juillet 2023 juillet  2023     5 \"Enfin un…\n10    10 Mme T.                   06 août 2023    août     2023     4 \"Satisfai…\n# ℹ 4,378 more rows\n\n#Résumé des données\nsummary(data)\n\n       id          auteur              date              month          \n Min.   :   1   Length:4388        Length:4388        Length:4388       \n 1st Qu.:1098   Class :character   Class :character   Class :character  \n Median :2194   Mode  :character   Mode  :character   Mode  :character  \n Mean   :2194                                                           \n 3rd Qu.:3291                                                           \n Max.   :4388                                                           \n      year           note         comments        \n Min.   :2013   Min.   :1.000   Length:4388       \n 1st Qu.:2018   1st Qu.:5.000   Class :character  \n Median :2020   Median :5.000   Mode  :character  \n Mean   :2019   Mean   :4.641                     \n 3rd Qu.:2021   3rd Qu.:5.000                     \n Max.   :2023   Max.   :5.000",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Pré-traitements</span>"
    ]
  },
  {
    "objectID": "chapter2.html#premières-analyses-des-données",
    "href": "chapter2.html#premières-analyses-des-données",
    "title": "2  Pré-traitements",
    "section": "2.3 Premières analyses des données",
    "text": "2.3 Premières analyses des données\nAvant de s’intéresser au contenu des commentaires, explorons la structure des données. On va regarder la distribution des commentaires et des notes dans le temps, et s’intéresser à la longueur des avis clients.\n\n#Les années \ndata$year&lt;-as.factor(data$year)\nsummary(data$year)\n\n2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 \n 125  209  249  171  324  341  375  735  853  623  383 \n\ndata%&gt;%\n  group_by(year)%&gt;%\n  summarise(n=n(), prop=n/nrow(data))%&gt;%\n  ggplot(aes(year,prop))+\n  geom_col(fill=\"green\",show.legend = TRUE)+\n  scale_y_continuous(labels = scales::percent)+\n  theme_light()+\n  labs(title = \"Répartition des avis dans le temps\", subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\", x=\"années\", y=NULL)\n\n\n\n\n\n\n\n#Les notes\nsummary(data$note)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   5.000   5.000   4.641   5.000   5.000 \n\nsummary(as.factor(data$note))\n\n   1    2    3    4    5 \n 116   62  169  589 3452 \n\ndata%&gt;%\n  group_by(note)%&gt;%\n  summarise(n=n(), prop=n/nrow(data))%&gt;%\n  ggplot(aes(note,prop))+\n  geom_col(fill=c(\"red\",\"pink\",\"orange\",\"gold\",\"lightgreen\"))+\n  annotate(\"text\", x=2, y=0.7, label=paste(\"Note moyenne = \",round(mean(data$note),1)))+\n  scale_y_continuous(labels=scales::percent)+\n  theme_light()+\n  labs(title = \"Répartition des avis en fonction des notes\", subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\", x=\"note\", y=NULL)\n\n\n\n\n\n\n\n#Le nombre de caractère\ndata$nb_caractere&lt;-nchar(data$comments) #on compte le nombre de caractère de chaque commentaire\nsummary(data$nb_caractere)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   12.0    41.0    74.0   104.3   130.0  1571.0 \n\nggplot(data, aes(nb_caractere))+\n  geom_boxplot()+\n  scale_y_continuous(NULL, breaks = NULL)+\n  labs(x=NULL,title = \"Nombre de caractères des avis\", subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\")+\n  theme_light()+\n  coord_flip()\n\n\n\n\n\n\n\n#Le nombre de tokens\n\ndata$nb_token&lt;-ntoken(data$comments) #on compte le nombre de caractère de chaque commentaire\n\nWarning: ntoken.character()/ntype.corpus() was deprecated in quanteda 4.0.0.\nℹ Please use ntoken(tokens(x)) instead.\n\nsummary(data$nb_token)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    2.0     7.0    13.0    19.3    24.0   308.0 \n\nggplot(data, aes(nb_token))+\n  geom_boxplot()+\n  scale_y_continuous(NULL, breaks = NULL)+\n  labs(x=NULL,title = \"Nombre de tokens des avis\", subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\")+\n  theme_light()+\n  coord_flip()\n\n\n\n\n\n\n\n#On va filtrer au-dessus de 100 tokens\ndata_100t&lt;-data%&gt;%filter(nb_token&lt;50)\n  \nggplot(data_100t, aes(nb_token))+\n  geom_boxplot()+\n  scale_y_continuous(NULL, breaks = NULL)+\n  labs(x=NULL,title = \"Nombre de tokens des avis\", subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\")+\n  theme_light()+\n  coord_flip()\n\n\n\n\n\n\n\n#Les notes dans le temps\ndata%&gt;%\n  mutate(note=as.factor(note))%&gt;%\n  group_by(year, note)%&gt;%\n  summarise(n=n() ,prop=n/nrow(data))%&gt;%\n  ggplot(aes(year, prop))+\n  geom_col(aes(fill=note), show.legend = FALSE)+\n  scale_fill_discrete(type=c(\"red\",\"pink\",\"orange\",\"gold\",\"lightgreen\"))+\n  scale_y_continuous(labels=scales::percent)+\n  theme_light()+\n  labs(title = \"Répartition des avis dans le temps\", subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\", x=\"années\", y=NULL)\n\n\n\n\n\n\n\ndata%&gt;%\n  mutate(note=as.factor(note))%&gt;%\n  group_by(year, note)%&gt;%\n  summarise(n=n())%&gt;%\n  ggplot(aes(x=year, y=n, group=note))+\n  geom_bar(position=\"fill\",stat=\"identity\", aes(fill=note))+\n  scale_fill_discrete(type=c(\"red\",\"pink\",\"orange\",\"gold\",\"lightgreen\"))+\n  scale_y_continuous(labels=scales::percent)+\n  theme_minimal()+\n  labs(title = \"Comparaison de la répartition des notes dans le temps\", subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\", x=\"années\", y=NULL)",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Pré-traitements</span>"
    ]
  },
  {
    "objectID": "chapter2.html#pondération-tf-idf",
    "href": "chapter2.html#pondération-tf-idf",
    "title": "2  Pré-traitements",
    "section": "7.1 Pondération tf-idf",
    "text": "7.1 Pondération tf-idf\nOn commence par reprendre nos manipulations précédentes : création de corpus, élimination des stopwords, constitution de bi- ou tri- grammes. On applique ensuite la pondération tf-idf.\n\ndfmtfidf&lt;-dfm_tfidf(dfm)\n\ndfmtfidf\n\nDocument-feature matrix of: 4,388 documents, 5,429 features (99.81% sparse) and 8 docvars.\n       features\ndocs       comme toujours    super  service  changez     rien     sauf     peut\n  text1 1.406738 1.269355 1.173919 1.130383 3.040207 1.166595 2.320047 1.926263\n  text2 0        1.269355 2.347839 0        0        0        0        0       \n  text3 0        0        0        0        0        0        0        0       \n  text4 0        0        0        0        0        0        0        0       \n  text5 0        0        0        0        0        0        0        0       \n  text6 0        0        0        0        0        0        0        0       \n       features\ndocs        être       là\n  text1 1.961025 2.320047\n  text2 0        2.320047\n  text3 0        0       \n  text4 0        0       \n  text5 0        0       \n  text6 0        0       \n[ reached max_ndoc ... 4,382 more documents, reached max_nfeat ... 5,419 more features ]\n\n#Représentations graphiques\ntextplot_wordcloud(dfm, max_words = 200)\n\n\n\n\n\n\n\ntextplot_wordcloud(dfmtfidf, max_words = 200)\n\n\n\n\n\n\n\n#On filtre les mots trop fréquents\n\ndfm_trim&lt;-dfm_trim(dfm, max_termfreq = 500)\n\ndfmtfidf_trim&lt;-dfm_tfidf(dfm_trim)\n\ntextplot_wordcloud(dfm_trim, max_words = 200)\n\n\n\n\n\n\n\ntextplot_wordcloud(dfmtfidf_trim, max_words = 200, color = brewer.pal(6, \"Set2\"))\n\n\n\n\n\n\n\ndisplay.brewer.all()",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Pré-traitements</span>"
    ]
  },
  {
    "objectID": "chapter3.html",
    "href": "chapter3.html",
    "title": "3  Analyse du sentiment",
    "section": "",
    "text": "4 Les données\ndata &lt;- read_csv(\"data/data_trustpilot_oiseaux.csv\")",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analyse du sentiment</span>"
    ]
  },
  {
    "objectID": "chapter3.html#les-sentiments",
    "href": "chapter3.html#les-sentiments",
    "title": "3  Analyse du sentiment",
    "section": "5.1 Les sentiments",
    "text": "5.1 Les sentiments\nIntéressons-nous d’abord aux sentiments :\n\nsent&lt;-d%&gt;%\n  select(positive,negative)%&gt;%\n  pivot_longer(everything(), names_to = \"sentiment\", values_to = \"nb\")%&gt;%\n  summarise(nb=sum(nb), .by = sentiment)%&gt;%\n  mutate(prop=nb/sum(nb))\n\nggplot(data=sent,  aes(x=sentiment, y=prop)) + \n  geom_bar(stat=\"identity\", aes(fill=sentiment), show.legend = FALSE)+\n  scale_y_continuous(labels=scales::percent) +\n  labs(title = \"Répartition des sentiments dans le corpus Oiseaux Mania\",caption = \"Données TrustPilot\",x=\"Sentiments\",y=NULL)+\n  scale_fill_manual(values=c(\"red\", \"lightgreen\"))+\n  theme_light()\n\n\n\n\n\n\n\n\nLe corpus est très largement positif, ce qui n’est pas étonnant. On peut aussi créer d’autres indicateurs, comme la valence (différence positif-négatif) ou l’expressivité (somme de positif+négatif).\nExercice : personnalisez le graphique ci-dessous pour la variable d’expressivité.\n\ndata&lt;-data%&gt;%\n  mutate(nbcar=nchar(comments),\n         valence = positive-negative)\n\nggplot(data = data, aes(x = valence, y= nbcar))+\n  geom_point()+\n  geom_smooth()+\n  labs(title=\"Indicateur de valence\", subtitle = \"en fonction du nombre de caractères\", caption=\"Données TrustPilot\")+\n  theme_minimal()\n\n\n\n\n\n\n\ndata&lt;-data%&gt;%\n  mutate(nbcar=nchar(comments),\n         expressivité = positive+negative)\n\nggplot(data = data, aes(x = expressivité, y= nbcar))+\n  geom_point()+\n  geom_smooth()+\n  labs(title=\"Indicateur d'expressivité\", subtitle = \"en fonction du nombre de caractères\", caption=\"Données TrustPilot\")+\n  theme_minimal()",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analyse du sentiment</span>"
    ]
  },
  {
    "objectID": "chapter3.html#les-émotions",
    "href": "chapter3.html#les-émotions",
    "title": "3  Analyse du sentiment",
    "section": "5.2 Les émotions",
    "text": "5.2 Les émotions\nRegardons maintenant ce qu’il en est de la répartition des émotions :\n\n#On crée d'abord une palette pour les émotions\nemocol&lt;-c(\"yellow\",\"chartreuse\",\"olivedrab3\",\"green4\",\"royalblue3\",\"purple3\",\"red3\",\"orangered1\") \n\n\nemo&lt;-d%&gt;%\n  select(-positive, -negative)%&gt;% #On récupère les émotions\n  pivot_longer(everything(), names_to = \"emotion\", values_to = \"nb\")#On transforme le tableau\n \nemo2&lt;-emo%&gt;%\n  summarise(nb=sum(nb), .by = emotion)%&gt;%\n  mutate(prop=nb/sum(nb),\n          emotion=factor(emotion, ordered = TRUE,levels = c(\"joy\",\"trust\",\"fear\",\"surprise\",\"sadness\",\"disgust\",\"anger\",\"anticipation\")))\n\n#On crée un graphique circulaire\nggplot(data=emo2,  aes(x=emotion, y=prop, colour=emotion)) + \n  geom_bar(stat=\"identity\", aes(fill=emotion), show.legend = FALSE)+ \n  scale_y_continuous(labels=scales::percent)+\n  labs(title=\"Distribution des émotions \\n dans le corpus Oiseaux Mania\", caption=\"Données TrustPilot\", x=\"Emotions\", y=NULL) +\n  coord_polar()+ \n  scale_color_manual(values=emocol)+ scale_fill_manual(values=emocol)+\n  theme_minimal()\n\n\n\n\n\n\n\n#On regarde la répartition des émotions dans le corpus :\n\nggplot(emo, aes(x=emotion, y=nb))+\n  geom_violin(aes(fill=emotion), alpha=0.7,adjust = 2)+\n  theme_minimal()+ scale_fill_manual(values=emocol)+\n  scale_x_discrete(labels=NULL)\n\n\n\n\n\n\n\nggplot(emo, aes(x=emotion, y=nb))+\n  geom_boxplot(aes(fill=emotion,), alpha=0.7,adjust = 2, show.legend = FALSE)+\n  theme_minimal()+ scale_fill_manual(values=emocol)",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analyse du sentiment</span>"
    ]
  },
  {
    "objectID": "chapter3.html#en-fonction-des-années",
    "href": "chapter3.html#en-fonction-des-années",
    "title": "3  Analyse du sentiment",
    "section": "7.1 En fonction des années",
    "text": "7.1 En fonction des années\nOn refait les manipulations préliminaires :\n\ndata&lt;-data%&gt;%\n  mutate(year2=case_when(year&lt;2017~\"2013-2016\",\n                         year %in% c(2017:2019)~\"2017-2019\",\n                         .default=as.character(year)))\n\ncorpus_oiseaux&lt;-corpus(data, text_field = \"comments\")\n\ntok&lt;-tokens(corpus_oiseaux, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE)%&gt;%\n  tokens_remove(stopwords(\"fr\"))\n\ndfm&lt;-dfm(tok)\n\nComparons les mots en fonction des années :\n\ndfmgp&lt;-dfm_group(dfm, groups = year2)\ndfmgp\n\nDocument-feature matrix of: 6 documents, 5,392 features (66.69% sparse) and 1 docvar.\n           features\ndocs        comme toujours super service changez rien sauf peut être là\n  2013-2016    38       26    93      58       0   71    6   17    9  8\n  2017-2019    32       61    74      78       0   76    5   13   24  4\n  2020         27       38    42      65       0   51    0    5    6  1\n  2021         35       49    49      64       2   62    8   11   11  2\n  2022         36       49    46      59       0   36    0    4    3  3\n  2023         16       51    22      27       2   21    3    6    3  4\n[ reached max_nfeat ... 5,382 more features ]\n\n#On peut aussi passer la fonction directement en transformant en dfm avec l'option groups : dfm(tok, groups=\"year\")\n\ntextplot_wordcloud(dfmgp, comparison=TRUE, max_words = 200)",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analyse du sentiment</span>"
    ]
  },
  {
    "objectID": "chapter3.html#en-fonction-des-sentiments",
    "href": "chapter3.html#en-fonction-des-sentiments",
    "title": "3  Analyse du sentiment",
    "section": "7.2 En fonction des sentiments",
    "text": "7.2 En fonction des sentiments\nPour comparer en fonction des sentiments, il faut accéder au dictionnaire NRC (en français) (il y a des fonctions simplifiées pour les dictionnaires en anglais) :\n\ndic_nrc&lt;-read_xlsx(\"NRCfr.xlsx\")%&gt;%\n  pivot_longer(-word,names_to = \"sentiment\", values_to=\"value\")%&gt;%\n  filter(value==1, word!=\"NO TRANSLATION\")%&gt;%\n  select(-value)\n\nsent_term&lt;-convert(dfm,to=\"data.frame\")%&gt;%\n  select(-doc_id)%&gt;%\n  pivot_longer(everything(), names_to=\"word\", values_to=\"value\")%&gt;%\n  filter(value!=0)%&gt;%\n  summarise(value=sum(value), .by=word)%&gt;%\n  inner_join(dic_nrc)%&gt;%\n  slice_max(n=10, by=sentiment, order_by = value, with_ties=F)\n\n\nggplot(sent_term, aes(reorder(word, value), value, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~sentiment, scales = \"free_y\") +\n  labs(y = \"Contribution to sentiment\",\n       x = NULL) +\n  coord_flip()+\n  theme_minimal()",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analyse du sentiment</span>"
    ]
  },
  {
    "objectID": "chapter4.html",
    "href": "chapter4.html",
    "title": "4  Annotations et dépendances syntaxiques",
    "section": "",
    "text": "5 Les données\ndata &lt;- read_csv(\"data/data_trustpilot_oiseaux.csv\")\n\nRows: 4388 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): auteur, date, month, comments\ndbl (3): id, year, note\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata$nb_caractere&lt;-nchar(data$comments) #on compte le nombre de caractère de chaque commentaire\nsummary(data$nb_caractere)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   12.0    41.0    74.0   104.3   130.0  1571.0 \n\nmean(data$nb_caractere)\n\n[1] 104.2974\n\nmedian(data$nb_caractere)\n\n[1] 74\n\nround(mean(data$nb_caractere),1)\n\n[1] 104.3\n\nmoy&lt;-round(mean(na.omit(data$nb_caractere)), 1)\n\nggplot(data)+\n  geom_boxplot(aes(nb_caractere))+\n  geom_text(aes(x=500, y=0.2,label=paste(\"Moyenne :\",moy)))+\n  coord_flip()+\n  scale_y_continuous(NULL, breaks = NULL)+\n  theme_minimal()",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Annotations et dépendances syntaxiques</span>"
    ]
  },
  {
    "objectID": "chapter4.html#représentation-en-réseau-des-termes-co-occurents",
    "href": "chapter4.html#représentation-en-réseau-des-termes-co-occurents",
    "title": "4  Annotations et dépendances syntaxiques",
    "section": "7.1 Représentation en réseau des termes co-occurents",
    "text": "7.1 Représentation en réseau des termes co-occurents\n\n# \n# fcm_cooc&lt;-fcm(dfm_cooc2)\n# fcm_cooc\n# topfeatures(fcm_cooc)\n# dim(fcm_cooc)\n# \n# feat&lt;-names(topfeatures(fcm_cooc, 50))\n# fcm_cooc_select&lt;-fcm_select(fcm_cooc, pattern = feat, selection = \"keep\")\n# dim(fcm_cooc_select)\n# \n# textplot_network(fcm_cooc_select, min_freq = 0.8, edge_color = \"red\" , edge_alpha = 0.5, vertex_color = \"blue\",vertex_size = 3, vertex_labelcolor = \"darkblue\", omit_isolated = FALSE)\n# \n# \n# tpfeat&lt;-tibble(feat=names(topfeatures(fcm_cooc,50)),n=topfeatures(fcm_cooc,50))\n# tpfeat&lt;-tpfeat%&gt;%mutate(taille=n/150)\n# \n# textplot_network(fcm_cooc_select, min_freq = 0.8, edge_color = \"red\" , edge_alpha = 0.5, vertex_color = \"blue\",vertex_size = tpfeat$taille, vertex_labelcolor = \"darkblue\", omit_isolated = FALSE)\n# \n# textplot_network(fcm_cooc_select, min_freq = 0.8, edge_color = \"red\" , edge_alpha = 0.5, vertex_color = \"blue\", vertex_labelsize =  tpfeat$taille, vertex_labelcolor = \"darkblue\", omit_isolated = FALSE)\n# \n# \n# \n# textplot_network(fcm_cooc_select, min_freq = 0.8, edge_color = \"red\" , edge_alpha = 0.5, vertex_color = \"blue\",vertex_size = tpfeat$taille, vertex_labelsize =  tpfeat$taille, vertex_labelcolor = \"darkblue\", omit_isolated = FALSE)",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Annotations et dépendances syntaxiques</span>"
    ]
  },
  {
    "objectID": "chapter4.html#détecter-les-langues",
    "href": "chapter4.html#détecter-les-langues",
    "title": "4  Annotations et dépendances syntaxiques",
    "section": "8.1 Détecter les langues",
    "text": "8.1 Détecter les langues\nDans le cas d’un corpus composé de plusieurs langues (par exemple, un corpus extrait de twitter), il peut être intéressant de filtrer le corpus à partir de la langue. On utilise un algorithme, qui peut être long à exécuter selon la taille du corpus, et qui est plutôt performant : cld3. Il repose sur un réseau de neurones développé par Google\n\nlibrary(cld3)\n\ndata$langue&lt;-detect_language(data$comments)\n# data$langue\n\ndata_fr&lt;-data%&gt;%filter(langue==\"fr\")",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Annotations et dépendances syntaxiques</span>"
    ]
  },
  {
    "objectID": "chapter4.html#pos",
    "href": "chapter4.html#pos",
    "title": "4  Annotations et dépendances syntaxiques",
    "section": "8.2 POS",
    "text": "8.2 POS\n\nlibrary(cleanNLP)\n\n# cnlp_init_udpipe(model_name = \"french\")\n# \n# annotate&lt;-cnlp_annotate(data$comments, verbose = 100)\n# ann_token&lt;-annotate$token\n# write_csv2(ann_token, \"annotation_oiseaux.csv\")\n# write_rds(ann_token,\"annotation_oiseaux.rds\")\n\nann_token&lt;-read_rds(\"data/annotation_oiseaux.rds\")\n\nhead(ann_token%&gt;%filter(upos==\"ADJ\"|upos==\"NOUN\"|upos==\"VERB\"),15)\n\n# A tibble: 15 × 11\n   doc_id   sid tid   token    token_with_ws lemma  upos  xpos  feats tid_source\n    &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;         &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     \n 1      1     1 4     super    \"super \"      super  ADJ   &lt;NA&gt;  Gend… 5         \n 2      1     1 5     service  \"service\"     servi… NOUN  &lt;NA&gt;  Gend… 0         \n 3      1     2 2     changez  \"changez \"    chang… VERB  &lt;NA&gt;  Mood… 0         \n 4      1     2 5     peut     \"peut \"       pouvo… VERB  &lt;NA&gt;  Mood… 2         \n 5      1     2 8     cagnotte \"cagnotte \"   cagnot ADJ   &lt;NA&gt;  Gend… 9         \n 6      1     2 9     fidélité \"fidélité \"   fidél… NOUN  &lt;NA&gt;  Gend… 5         \n 7      1     2 11    est      \"est\"         être   VERB  &lt;NA&gt;  Mood… 9         \n 8      1     2 15    dirais   \"dirais \"     dir    VERB  &lt;NA&gt;  Mood… 11        \n 9      1     2 16    inutile  \"inutile \"    inuti… ADJ   &lt;NA&gt;  Gend… 15        \n10      2     1 2     délai    \"délai \"      délai  NOUN  &lt;NA&gt;  Gend… 0         \n11      2     1 5     commande \"commande \"   comma… NOUN  &lt;NA&gt;  Gend… 2         \n12      2     1 7     rapide   \"rapide \"     rapide ADJ   &lt;NA&gt;  Gend… 5         \n13      2     2 2     délais   \"délais \"     délais NOUN  &lt;NA&gt;  Gend… 14        \n14      2     2 6     commande \"commande \"   comma… NOUN  &lt;NA&gt;  Gend… 2         \n15      2     2 8     rapide   \"rapide\"      rapide ADJ   &lt;NA&gt;  Gend… 6         \n# ℹ 1 more variable: relation &lt;chr&gt;\n\nann_token%&gt;%filter(upos==\"ADJ\"|upos==\"NOUN\"|upos==\"VERB\")\n\n# A tibble: 34,232 × 11\n   doc_id   sid tid   token    token_with_ws lemma  upos  xpos  feats tid_source\n    &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;         &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     \n 1      1     1 4     super    \"super \"      super  ADJ   &lt;NA&gt;  Gend… 5         \n 2      1     1 5     service  \"service\"     servi… NOUN  &lt;NA&gt;  Gend… 0         \n 3      1     2 2     changez  \"changez \"    chang… VERB  &lt;NA&gt;  Mood… 0         \n 4      1     2 5     peut     \"peut \"       pouvo… VERB  &lt;NA&gt;  Mood… 2         \n 5      1     2 8     cagnotte \"cagnotte \"   cagnot ADJ   &lt;NA&gt;  Gend… 9         \n 6      1     2 9     fidélité \"fidélité \"   fidél… NOUN  &lt;NA&gt;  Gend… 5         \n 7      1     2 11    est      \"est\"         être   VERB  &lt;NA&gt;  Mood… 9         \n 8      1     2 15    dirais   \"dirais \"     dir    VERB  &lt;NA&gt;  Mood… 11        \n 9      1     2 16    inutile  \"inutile \"    inuti… ADJ   &lt;NA&gt;  Gend… 15        \n10      2     1 2     délai    \"délai \"      délai  NOUN  &lt;NA&gt;  Gend… 0         \n# ℹ 34,222 more rows\n# ℹ 1 more variable: relation &lt;chr&gt;\n\ng&lt;-ann_token%&gt;%group_by(upos)%&gt;%\n  summarise(n=n())%&gt;%\n  filter(!is.na(upos))\n\nggplot(g)+\n  geom_col(aes(reorder(upos,n),n, fill=n), show.legend = FALSE)+\n  scale_fill_fermenter(palette = \"PuRd\", direction = 1)+\n  coord_flip()+\n  labs(title = \"Fréquence des UPOS\", subtitle = \"Corpus Oiseaux Mania\", caption = \"Data TrustPilot\", x=NULL, y=NULL)+\n  theme_dark()\n\n\n\n\n\n\n\n\nMaintenant, on va s’intéresser à des catégories grammaticales spécifiques :\n\nvocab1&lt;-ann_token%&gt;%\n  filter(upos==\"NOUN\")%&gt;%\n  summarise(freq=n(),.by=lemma)%&gt;%\n  filter(freq&gt;55)\n\nggplot(vocab1,aes(x=reorder(lemma,freq),y=freq))+\n  geom_bar(stat=\"identity\",fill=\"lightgreen\")+\n  coord_flip()+\n  theme_minimal()+\n  labs(title = \"Noms communs les plus fréquents\",subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\",x=\"Noms commun\",y=\"Fréquence\")\n\n\n\n\n\n\n\nvocab1bis&lt;-ann_token%&gt;%\n  filter(upos==\"ADJ\")%&gt;%\n  summarise(freq=n(),.by=lemma)%&gt;%\n  filter(freq&gt;55)\n\nggplot(vocab1bis,aes(x=reorder(lemma,freq),y=freq))+\n  geom_bar(stat=\"identity\",fill=\"darkblue\")+\n  coord_flip()+\n  theme_minimal()+\n  labs(title = \"Adjectifs les plus fréquents\",subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\",x=\"Adjectifs\",y=\"Fréquence\")\n\n\n\n\n\n\n\nvocab2&lt;-ann_token%&gt;%\n  filter(upos==\"NOUN\" | upos==\"VERB\" | upos==\"ADJ\")%&gt;%\n  summarise(freq=n(),.by=c(lemma,upos))%&gt;%\n  filter(freq&gt;30)%&gt;%\n  mutate(angle= 90 * sample(c(0, 1), n(), replace = TRUE, prob = c(75, 25)))\n\nlibrary(ggwordcloud)\nggplot(vocab2)+\n  geom_text_wordcloud_area(aes(label=lemma, size=freq, color=freq, angle=angle))+\n  scale_size_area(max_size = 24)+\n  scale_color_fermenter(palette = \"Set2\")+\n  theme_minimal()\n\n\n\n\n\n\n\nggplot(vocab2)+\n  geom_text_wordcloud_area(aes(label=lemma, size=freq, color=upos, angle=angle))+\n  scale_size_area(max_size = 24)+\n  scale_color_manual(values=c(\"ADJ\"=\"orange\",\"NOUN\"=\"lightgreen\",\"VERB\"=\"purple\"))+\n  theme_minimal()",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Annotations et dépendances syntaxiques</span>"
    ]
  },
  {
    "objectID": "chapter5.html",
    "href": "chapter5.html",
    "title": "5  Topic Analysis",
    "section": "",
    "text": "6 Les données\ndata &lt;- read_csv(\"data/data_trustpilot_oiseaux.csv\")\n\nRows: 4388 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): auteur, date, month, comments\ndbl (3): id, year, note\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>*Topic Analysis*</span>"
    ]
  },
  {
    "objectID": "chapter5.html#le-modèle-lda-avec-topicmodels",
    "href": "chapter5.html#le-modèle-lda-avec-topicmodels",
    "title": "5  Topic Analysis",
    "section": "7.1 Le modèle LDA avec topicmodels",
    "text": "7.1 Le modèle LDA avec topicmodels\nOn travaille à partir du dfm. On doit transformer le format des données afin de l’injecter dans le modèle. On réduit le nombre de termes considérés, ce qui permet de réduire les temps de calcul et de trouver une solution convergente.\n\ncorpus_oiseaux&lt;-corpus(data, text_field = \"comments\")\n\ntok&lt;-tokens(corpus_oiseaux, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE)%&gt;%\n  tokens_remove(stopwords(\"fr\"))\n\ndfm&lt;-dfm(tok)\n\n\n#On filtre les mots trop et trop peu fréquents\nrem&lt;-c(\"très\",\"rapide\",\"produit\",\"livraison\", \"commande\", \"bien\", \"site\", \"a\", \"bon\", \"merci\", \"recommande\",\"parfait\")\n\nnews_dfm &lt;- dfm %&gt;%\n  dfm_remove(rem)%&gt;%\n  dfm_trim(min_termfreq = 0.8, termfreq_type = \"quantile\",    # 80% des mots les plus fréquents\n           max_docfreq = 0.2, docfreq_type = \"prop\")          #qui apparaissent dans max 20% des documents\n\n#On supprime les entrées vides\nnews_dfm &lt;- news_dfm[ntoken(news_dfm) &gt; 0,]\n\n#On transforme en dtm, un format compris par le package topicsmodel\ndtm &lt;- convert(news_dfm, to = \"topicmodels\")\n\n#On lance le modèle\nlda &lt;- LDA(dtm, k = 5)\n\n#On regarde les résultats\nterms(lda,10)\n\n      Topic 1      Topic 2    Topic 3      Topic 4      Topic 5  \n [1,] \"bonne\"      \"produits\" \"produits\"   \"colis\"      \"qualité\"\n [2,] \"produits\"   \"qualité\"  \"prix\"       \"qualité\"    \"j'ai\"   \n [3,] \"rien\"       \"tout\"     \"tout\"       \"rapidement\" \"oiseaux\"\n [4,] \"colis\"      \"reçu\"     \"plus\"       \"conforme\"   \"mania\"  \n [5,] \"j'ai\"       \"toujours\" \"conforme\"   \"mania\"      \"service\"\n [6,] \"rapidement\" \"plus\"     \"service\"    \"produits\"   \"c'est\"  \n [7,] \"super\"      \"j'ai\"     \"tres\"       \"oiseaux\"    \"sérieux\"\n [8,] \"oiseaux\"    \"service\"  \"reçu\"       \"top\"        \"choix\"  \n [9,] \"tres\"       \"c'est\"    \"super\"      \"toujours\"   \"peu\"    \n[10,] \"frais\"      \"super\"    \"satisfaite\" \"dire\"       \"chez\"   \n\n# topics(lda)\n\ncorpus_oiseaux[\"text996\"]\n\nCorpus consisting of 1 document and 6 docvars.\ntext996 :\n\"Seconde commande, satisfaction totale, expédition très rapid...\"\n\ncorpus_oiseaux[\"text995\"]\n\nCorpus consisting of 1 document and 6 docvars.\ntext995 :\n\"Très bon produit. Livraison rapide et sérieux je recommande ...\"\n\ncorpus_oiseaux[\"text999\"]\n\nCorpus consisting of 1 document and 6 docvars.\ntext999 :\n\"Très bon magasin emballage dans les normes on y trouve de to...\"",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>*Topic Analysis*</span>"
    ]
  },
  {
    "objectID": "chapter5.html#topic-analysis-à-partir-de-lannotation-des-part-of-speech",
    "href": "chapter5.html#topic-analysis-à-partir-de-lannotation-des-part-of-speech",
    "title": "5  Topic Analysis",
    "section": "7.2 Topic Analysis à partir de l’annotation des part of speech",
    "text": "7.2 Topic Analysis à partir de l’annotation des part of speech\nLes résultats du modèle LDA sont très dépendants de la qualité du vocabulaire injecté. Plus on travaille ce vocabulaire, meilleurs sont les résultats. On va donc reprendre tout ce qu’on a fait jusqu’à présent pour améliorer les résultats de notre modèle : on récupère les annotations ; on filtre le vocabulaire pour ne garder que les noms, adjectifs et verbes ; on crée les collocations ; on filtre les occurrences trop et pas assez fréquentes.\n\nann_token&lt;-read_rds(\"data/annotation_oiseaux.rds\")\n\n\ndata&lt;-ann_token%&gt;%\n  filter(upos==\"NOUN\"|upos==\"VERB\"|upos==\"ADJ\")%&gt;%\n  group_by(doc_id)%&gt;%\n  summarise(text=paste(lemma,collapse = \" \"))%&gt;%\n  inner_join(data, join_by(\"doc_id\"==\"id\"))\n\ncorpus_new&lt;-corpus(data, text_field = \"text\")\ntoks&lt;-tokens(corpus_new)%&gt;%\n  tokens_replace(c(\"produire\", \"conformer\",\"colir\"), c(\"produit\", \"conforme\",\"colis\"))%&gt;%\n  tokens_remove(c(\".\",\",\"))\n\ncolloc&lt;-textstat_collocations(toks, min_count = 10, tolower = TRUE)\n\ntoks&lt;-tokens_compound(toks, pattern = colloc[colloc$z&gt;7,])\n\n\ndfm_new&lt;-dfm(toks)%&gt;%\n  dfm_trim(min_termfreq = 0.6, termfreq_type = \"quantile\",\n           max_docfreq = 0.1, docfreq_type = \"prop\")\ndtm_new &lt;- convert(dfm_new, to = \"topicmodels\")\n\nset.seed(1234)\nlda &lt;- LDA(dtm_new, k = 5)\n\nterm&lt;-as_tibble(terms(lda,25))%&gt;%\n  mutate(rank=as.numeric(row.names(.)))%&gt;%\n  pivot_longer(-rank, names_to = \"topic\",values_to = \"term\")\n\nggplot(term, aes(x=topic, y= rank, group =  term , label = term)) + \n  scale_y_reverse() +\n  geom_text(aes(color=topic,size=8/log(rank)))+\n  theme_minimal()+\n  scale_color_hue()+\n  guides(color=\"none\",size=\"none\")",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>*Topic Analysis*</span>"
    ]
  },
  {
    "objectID": "chapter5.html#déterminer-le-nombre-de-topics-optimal",
    "href": "chapter5.html#déterminer-le-nombre-de-topics-optimal",
    "title": "5  Topic Analysis",
    "section": "7.3 Déterminer le nombre de topics optimal",
    "text": "7.3 Déterminer le nombre de topics optimal\nLe modèle LDA fonctionne à partir d’un nombre de topics donné. La question est donc de savoir quel est le nombre de topics optimal pour décrire notre corpus. Heureusement, des personnes ont créé des fonctions et des procédures pour nous aider dans cette quête. L’idée est de calculer différents modèles pour différents nombres de topics, et de comparer la qualité des résultats. La procédure ci-dessous est en deux parties :\n\nTout d’abord, on compare la qualité de différents indicateurs sur un grand nombre de modèles, pour aboutir à une liste de quelques solutions à comparer plus en détail (de 3 à 10).\nEnsuite, on compare les résultats de la liste réduite de modèles, pour déterminer lequel a la meilleure distribution des topics entre les documents. La distribution recherchée est celle qui distingue le plus les documents en fonction des topics, tout en étant à droite de l’estimation d’une répartition uniforme des documents entre les topics. Le critère de parcimonie nous invite à choisir la solution avec le moins grand nombre de topics, en cas de résultats comparables.\n\n\n##Etape 1 : les meilleures solutions\nlibrary(ldatuning)\nlibrary(magrittr)\n\n\nAttachement du package : 'magrittr'\n\n\nL'objet suivant est masqué depuis 'package:purrr':\n\n    set_names\n\n\nL'objet suivant est masqué depuis 'package:tidyr':\n\n    extract\n\nresult &lt;- FindTopicsNumber(dtm_new,\n                           topics = c(seq(from = 2, to = 9, by = 1), seq(10, 25, 5)),\n                           metrics = c(\"Griffiths2004\", \"CaoJuan2009\", \"Arun2010\", \"Deveaud2014\"),\n                           method = \"Gibbs\",\n                           control = list(seed = 0:4,\n                                          nstart = 5,\n                                          best = TRUE),\n                           mc.cores = 4L,\n                           verbose = TRUE\n                           )\n\nfit models... done.\ncalculate metrics:\n  Griffiths2004... done.\n  CaoJuan2009... done.\n  Arun2010... done.\n  Deveaud2014... done.\n\nFindTopicsNumber_plot(result)\n\nWarning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\nℹ The deprecated feature was likely used in the ldatuning package.\n  Please report the issue at &lt;https://github.com/nikita-moor/ldatuning/issues&gt;.\n\n\n\n\n\n\n\n\n##Etape 2 : comparaison des solutions\npara &lt;- tibble(k = c(6,7,8,9,10))\nlemma_tm &lt;- para %&gt;%\n  mutate(lda = map(k,\n                   function(k) LDA(\n                     k=k,\n                     x=dtm_new,\n                     method=\"Gibbs\",\n                     control=list(seed = 0:4,\n                                  nstart = 5,\n                                  best = TRUE)\n                     )\n                   )\n         )\n\nlemma_tm &lt;- lemma_tm %&gt;%\n  mutate(lda_gamma = map(.x=lda,\n                         .f=tidytext::tidy,\n                         matrix=\"gamma\"))\nlemma_tm %&gt;%\n  unnest(lda_gamma) %&gt;%\n  group_by(k, document) %&gt;%\n  arrange(desc(gamma)) %&gt;%\n  slice(1) %&gt;%\n  ungroup() %&gt;%\n  ggplot(aes(x=gamma, fill=factor(k))) +\n  geom_histogram(bins = 20) +\n  scale_fill_discrete(name = \"Number of\\nTopics\") +\n  xlab(\"maximum gamma per document\") +\n  facet_wrap(~k) +\n  geom_vline(aes(xintercept = 1/k),\n             tibble(k=lemma_tm %$% unique(k)),\n             color=\"darkred\")",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>*Topic Analysis*</span>"
    ]
  },
  {
    "objectID": "chapter5.html#représentation-graphique",
    "href": "chapter5.html#représentation-graphique",
    "title": "5  Topic Analysis",
    "section": "7.4 Représentation graphique",
    "text": "7.4 Représentation graphique\nÀ partir de la solution retenue aux étapes précédentes, on va représenter les différents topics :\n\nset.seed(1234)     #pour la réplicabilité des résultats\nlda &lt;- LDA(dtm_new, k = 9)\n\nlda_res&lt;-as.data.frame(terms(lda, 25))%&gt;%\n  rename(nom1='Topic 1',nom2='Topic 2', nom3='Topic 3', nom4='Topic 4', nom5='Topic 5',nom6='Topic 6',nom7='Topic 7', nom8='Topic 8', nom9='Topic 9')%&gt;%\n  mutate(rank=as.numeric(row.names(.)))%&gt;%\n  pivot_longer(-rank, names_to = \"topic\", values_to = \"term\")\n\nggplot(lda_res, aes(x=topic, y= rank, group =  term , label = term)) + \n  scale_y_reverse() +\n  geom_text(aes(color=topic,size=8/log(rank)))+\n  theme_minimal()+\n  scale_color_hue()+\n  guides(color=FALSE,size=FALSE)",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>*Topic Analysis*</span>"
    ]
  },
  {
    "objectID": "chapter5.html#nps",
    "href": "chapter5.html#nps",
    "title": "5  Topic Analysis",
    "section": "9.1 NPS",
    "text": "9.1 NPS\nDans un premier temps, nous allons regarder le Net Promoter Score (NPS), puis nous étudierons les discours des promoteurs, détracteurs et passifs.\nTout d’abord, nous créons nos catégories en fonction des notes.\n\ncol&lt;- c(\"red\",\"gold\", \"chartreuse\")\n\n\ndata&lt;-data %&gt;%\n  mutate(NPS=case_when(note&lt;4~\"Détracteurs\",\n                       note==4~\"Passifs\",\n                       note&gt;4~\"Promoteurs\"))\n\n\nggplot(data, aes(x=note))+\n  geom_histogram(binwidth = 1, aes(fill=NPS))+\n  labs( title= \" Distribution des scores NPS\", \n        subtitle = paste(\"Moyenne du NPS de l'échantillon\",round(mean(data$note),1)), \n        caption = paste(\"Data : TrustPilot, n=\",nrow(data)), \n        y = \"Fréquence\")+ \n  scale_fill_manual(values=col)+\n  theme_light()\n\n\n\n\n\n\n\n\nPuis nous réalisons un nuage de mots pour chaque groupe, afin d’avoir une idée de ce qui est exprimé.\n\ndfm_new$NPS&lt;-data$NPS\n# docvars(toks)\n\ndfm_gp &lt;-dfm_new %&gt;%\n    dfm_group(groups = NPS)\n# dfm_gp\n\nstat&lt;- dfm_gp %&gt;% \n  textstat_frequency(n = 30,  groups = NPS)\nstat\n\n       feature frequency rank docfreq       group\n1        avoir       103    1       1 Détracteurs\n2    commander        81    2       1 Détracteurs\n3     recevoir        79    3       1 Détracteurs\n4    livraison        77    4       1 Détracteurs\n5        colis        41    5       1 Détracteurs\n6         site        40    6       1 Détracteurs\n7      pouvoir        38    7       1 Détracteurs\n8      article        38    7       1 Détracteurs\n9       livrer        37    9       1 Détracteurs\n10       faire        34   10       1 Détracteurs\n11        fois        31   11       1 Détracteurs\n12    problème        28   12       1 Détracteurs\n13        dire        28   12       1 Détracteurs\n14    attendre        28   12       1 Détracteurs\n15        jour        28   12       1 Détracteurs\n16      devoir        28   12       1 Détracteurs\n17        cher        26   17       1 Détracteurs\n18      graine        26   17       1 Détracteurs\n19       autre        25   19       1 Détracteurs\n20     envoyer        25   19       1 Détracteurs\n21       payer        25   19       1 Détracteurs\n22     acheter        25   19       1 Détracteurs\n23     trouver        23   23       1 Détracteurs\n24        cage        23   23       1 Détracteurs\n25        long        23   23       1 Détracteurs\n26      oiseau        21   26       1 Détracteurs\n27        déçu        21   26       1 Détracteurs\n28        mois        21   26       1 Détracteurs\n29     réponse        20   29       1 Détracteurs\n30     semaine        20   29       1 Détracteurs\n31       avoir        85    1       1     Passifs\n32   livraison        78    2       1     Passifs\n33   commander        56    3       1     Passifs\n34        site        53    4       1     Passifs\n35    recevoir        50    5       1     Passifs\n36      rapide        42    6       1     Passifs\n37   satisfait        41    7       1     Passifs\n38         bon        40    8       1     Passifs\n39       petit        35    9       1     Passifs\n40     pouvoir        34   10       1     Passifs\n41   perroquet        34   10       1     Passifs\n42       colis        33   12       1     Passifs\n43      oiseau        32   13       1     Passifs\n44     trouver        30   14       1     Passifs\n45       faire        29   15       1     Passifs\n46     article        28   16       1     Passifs\n47  frais_port        28   16       1     Passifs\n48        cher        26   18       1     Passifs\n49    problème        23   19       1     Passifs\n50        dire        23   19       1     Passifs\n51 bon_produit        21   21       1     Passifs\n52      livrer        21   21       1     Passifs\n53    emballer        21   21       1     Passifs\n54       délai        20   24       1     Passifs\n55  satisfaire        20   24       1     Passifs\n56        cage        20   24       1     Passifs\n57     arriver        19   27       1     Passifs\n58        prix        19   27       1     Passifs\n59   emballage        19   27       1     Passifs\n60       super        18   30       1     Passifs\n61      rapide       295    1       1  Promoteurs\n62   satisfait       284    2       1  Promoteurs\n63        site       279    3       1  Promoteurs\n64   livraison       275    4       1  Promoteurs\n65   commander       269    5       1  Promoteurs\n66      oiseau       228    6       1  Promoteurs\n67       avoir       222    7       1  Promoteurs\n68    recevoir       186    8       1  Promoteurs\n69         bon       173    9       1  Promoteurs\n70     sérieux       157   10       1  Promoteurs\n71        dire       150   11       1  Promoteurs\n72     qualité       146   12       1  Promoteurs\n73     service       143   13       1  Promoteurs\n74         top       141   14       1  Promoteurs\n75 bon_produit       138   15       1  Promoteurs\n76   excellent       138   15       1  Promoteurs\n77     article       134   17       1  Promoteurs\n78       super       133   18       1  Promoteurs\n79     content       127   19       1  Promoteurs\n80   perroquet       122   20       1  Promoteurs\n81     trouver       119   21       1  Promoteurs\n82    problème       117   22       1  Promoteurs\n83       faire       111   23       1  Promoteurs\n84    emballer       111   23       1  Promoteurs\n85    conforme       108   25       1  Promoteurs\n86        prix        99   26       1  Promoteurs\n87         tre        93   27       1  Promoteurs\n88       délai        92   28       1  Promoteurs\n89       petit        92   28       1  Promoteurs\n90    rapidité        88   30       1  Promoteurs\n\nggplot(stat, aes(label = feature)) +\n  geom_text_wordcloud(aes(size=log(frequency), color=group)) +\n  theme_minimal()+\n  facet_wrap(vars(group))+\n  scale_color_manual(values=col)+ \n  labs(title=\"Nuage des 30 mots les plus fréquents (Par groupes)\",\n       caption = \"La taille des mots est proportionnelle au log de leurs fréquences\")\n\n\n\n\n\n\n\n\nMaintenant, nous nous intéressons à ce qui caractérise chacun des groupes par rapport aux autres, grâce à la mesure du keyness.\n\ngraph_promoteur&lt;-textstat_keyness(dfm_gp, target = \"Promoteurs\")%&gt;%\n  textplot_keyness(n = 30L, labelsize = 2,   show_legend = FALSE, \n                     show_reference = FALSE,   color = c(\"Darkgreen\", \"gray\"))+\n  labs(x=NULL)\n\n\ngraph_detracteur &lt;- textstat_keyness(dfm_gp, target = \"Détracteurs\" )%&gt;%\n  textplot_keyness(n = 30L, labelsize = 2,   show_legend = FALSE,   \n                     show_reference = FALSE,   color = c(\"firebrick\", \"gray\"))+ \n  labs(x=NULL)\n\n\ngraph_passif &lt;- textstat_keyness(dfm_gp, target = \"Passifs\")%&gt;%\n  textplot_keyness(n = 30L, labelsize = 2,   show_legend = FALSE,   show_reference = FALSE,    color = c(\"gold2\", \"gray\"))+\n  labs(x=NULL)\n\n\nlibrary(cowplot)\n\n\nAttachement du package : 'cowplot'\n\n\nL'objet suivant est masqué depuis 'package:lubridate':\n\n    stamp\n\np&lt;- plot_grid(graph_detracteur, graph_passif ,graph_promoteur,  labels = c('Détracteurs', 'Passifs', 'Promoteurs'), label_size = 10, ncol=3)\n\ntitle &lt;- ggdraw() + draw_label(\"NPS : Les raisons qui conduisent à la recommandation (keyness)\", fontface='bold')\n\nnote &lt;- ggdraw()+ draw_text(\"Les valeurs représentent le keyness des termes.\\nIl mesure leur caractère distinctif par une statistique du chi²\", size=8,x = 0.5, y = 0.5)\n\n\nplot_grid(title, p,note, ncol=1, rel_heights=c(0.1, 1)) # rel_heights values control title margins",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>*Topic Analysis*</span>"
    ]
  },
  {
    "objectID": "chapter5.html#en-fonction-des-topics",
    "href": "chapter5.html#en-fonction-des-topics",
    "title": "5  Topic Analysis",
    "section": "9.2 En fonction des topics",
    "text": "9.2 En fonction des topics\nMaintenant, nous cherchons à voir la répartition des topics dans les notes, pour comprendre si certains topics contribuent plus ou moins à la satisfaction.\n\ntheta&lt;-as.data.frame(slda$theta)%&gt;%mutate(doc_id=as.numeric(row.names(.)))\n\ndata&lt;-inner_join(data, theta)\n\nJoining with `by = join_by(doc_id)`\n\nfoo&lt;-data%&gt;%select(note, produit, livraison, commande, site,prix, other)%&gt;%\n  pivot_longer(-note, names_to = \"topic\", values_to = \"value\")\n\nggplot(foo,aes(x=note, y=value, group=topic))+\n  geom_bar(position=\"fill\",stat=\"identity\", aes(fill=topic))+\n  scale_fill_brewer(palette=\"Spectral\")+\n  theme_minimal()\n\n\n\n\n\n\n\n#Pour finir, une petite régression !\nfit&lt;-lm(note~produit+livraison+commande+site+prix, data =data)\nsummary(fit)\n\n\nCall:\nlm(formula = note ~ produit + livraison + commande + site + prix, \n    data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1270 -0.0206  0.1766  0.3033  1.9858 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.51256    0.07178   35.01   &lt;2e-16 ***\nproduit      1.92260    0.11146   17.25   &lt;2e-16 ***\nlivraison    2.45543    0.10907   22.51   &lt;2e-16 ***\ncommande     2.44823    0.10885   22.49   &lt;2e-16 ***\nsite         2.95498    0.10736   27.52   &lt;2e-16 ***\nprix         2.79509    0.10718   26.08   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7591 on 4310 degrees of freedom\nMultiple R-squared:  0.1952,    Adjusted R-squared:  0.1942 \nF-statistic:   209 on 5 and 4310 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>*Topic Analysis*</span>"
    ]
  },
  {
    "objectID": "webscraping.html",
    "href": "webscraping.html",
    "title": "Appendix A — Webscraping",
    "section": "",
    "text": "library(tidyverse)\nlibrary(scales)\nlibrary(quanteda)\nlibrary(quanteda.textstats)\nlibrary(httr)\nlibrary(rvest)\nlibrary(polite)\n\n### Fichier TP_pets_website.csv\n\n##### Scraping sites web\nurl_start&lt;-\"https://fr.trustpilot.com/categories/animals_pets\"\n\ntrustpilot_website&lt;-function(url_start){\n\nsession &lt;- bow(url_start)\nsession$user_agent&lt;-\"Googlebot\"\nmessage(\"Scraping \", url_start)\npage&lt;-nod(session, url_start) %&gt;% \n  scrape(verbose=TRUE)\ni&lt;-page%&gt;%html_elements(\".styles_paginationWrapper__fukEb\")%&gt;%\n  html_element(\"a.button_button__T34Lr:nth-child(5)\")%&gt;%\n  html_text()%&gt;%as.numeric()\nwebsite &lt;- NULL\n\nfor (j in 1:i){\n  url&lt;-paste0(url_start,\"?page=\",j)\n  Sys.sleep(5)\n  session &lt;- bow(url)\n  session$user_agent&lt;-\"Googlebot\"\n  message(\"Scraping \", url)\n  page&lt;-nod(session, url) %&gt;% \n      scrape(verbose=TRUE)\n    \n    company_card &lt;- page %&gt;%\n      html_elements(\"div.styles_wrapper__2JOo2:nth-of-type(n+4)\")\n    \n    website_name &lt;- company_card %&gt;%\n      html_element(\"p.typography_heading-xs__jSwUz\") %&gt;%\n      html_text()\n    \n    nb_avis &lt;- company_card %&gt;%\n      html_element(\"p.typography_body-m__xgxZ_\")%&gt;%\n      html_text()\n    \n    localisation &lt;- company_card%&gt;%\n      html_element(\"span.styles_metadataItem__Qn_Q2\")%&gt;%\n      html_text()\n\n    type &lt;- company_card %&gt;%\n      html_element(\"div.styles_desktop__U5iWw\") %&gt;%\n      html_text()\n    \n    lien &lt;- paste0(\"https://fr.trustpilot.com\", company_card %&gt;%\n               html_element(\"a\")%&gt;%html_attr(\"href\"))\n      \n    \n    website &lt;- rbind(website, data.frame(\n      website_name = website_name, \n      nb_avis = nb_avis,\n      localisation = localisation,\n      type = type,\n      lien = lien\n    ))\n  print(paste(\"page\",j, \"has been scraped\"))\n    \nj&lt;-j+1\n\n}\nreturn(website)\n\n}\n\nwebsite&lt;-trustpilot_website(url_start = url_start)\n\n##### Création du fichier TP_pets_website.csv\nwebsite&lt;-website%&gt;%mutate(note=str_split_i(nb_avis,\"\\\\|\", 1)%&gt;%\n                      str_remove_all(., \"[A-z]\")%&gt;%\n                      str_replace(., \",\", \".\")%&gt;%\n                      as.numeric(),\n                    nb_avis=str_split_i(nb_avis,\"\\\\|\",2)%&gt;%\n                      str_remove_all(., \"[A-z]\")%&gt;%\n                      str_remove_all(., \"[:space:]\")%&gt;%\n                      as.numeric()%&gt;%\n                      replace_na(.,0),\n                    url_start=url_start,\n                    nb_page=0,\n                    cat=str_split(type, \"·\"))%&gt;%\n  unnest_wider(cat, names_sep = \"_\")\n\ndata_scrap&lt;-website%&gt;%filter(nb_avis&gt;10)\nwrite_csv(data_scrap, \"TP_pets_website.csv\")\n\n\n### Fichier TP_pets_reviews.rds\n\n##### Scraping reviews\n\ntrustpilot_reviews&lt;-function(data){\n  Sys.sleep(5)\n  \n  for (j in 1:nrow(data)) {\n    i&lt;-1\n    b&lt;-1\n    \n    \n    while (b!=\"TRUE\") {\n      \n      Sys.sleep(5)\n      \n      \n      b&lt;-http_error(paste0(data$lien[j], \"?languages=all&page=\", i))\n      \n      i&lt;-i+1\n      data$nb_page[j]&lt;-i-2\n      \n    }\n    print(paste0(\"nb_page of \", data$website_name[j], \" has been fetched\"))\n    \n  }\n  \n  i&lt;-1\n  reviews &lt;- NULL\n  # cat(\"\\014\")\n  cat(paste0(\"The script will run on \", sum(data$nb_page), \" pages!\\n\"))\n  Sys.sleep(5)\n  \n  \n  for (j in 1: nrow(data)){\n    for (i in 1:data$nb_page[j]){\n      url&lt;-paste0(data$lien[j],\"?languages=all&page=\",i)\n      Sys.sleep(5)\n      session &lt;- bow(url)\n      session$user_agent&lt;-\"Googlebot\"\n      message(\"Scraping \", url)\n      page&lt;-nod(session, url) %&gt;% \n        scrape(verbose=TRUE)\n      \n      review_card &lt;- page %&gt;%\n        html_elements(\"div.styles_reviewCardInner__EwDq2\")\n      \n      name &lt;- review_card %&gt;%\n        html_element(\"span.typography_heading-xxs__QKBS8.typography_appearance-default__AAY17\") %&gt;%\n        html_text()\n      \n      rating &lt;- review_card %&gt;%\n        html_elements(\"div.star-rating_starRating__4rrcf.star-rating_medium__iN6Ty\") %&gt;%\n        html_element(\"img\")%&gt;%\n        html_attr(\"alt\")%&gt;%\n        str_extract(\"[:digit:]\")\n      \n      published &lt;- review_card%&gt;%\n        html_elements(\".styles_reviewContentwrapper__zH_9M\")%&gt;%\n        html_element(\"p.typography_body-m__xgxZ_\")%&gt;%\n        html_text()%&gt;%\n        str_remove(\"Date de l'expérience: \")\n      \n      verified &lt;- review_card %&gt;%\n        html_element(\".styles_detailsIcon__yqwWi\") %&gt;%\n        html_text()\n      \n      title &lt;- review_card %&gt;%\n        html_element(\"h2\")%&gt;%\n        html_text()\n      \n      content &lt;- review_card%&gt;%\n        html_elements(\".styles_reviewContentwrapper__zH_9M\")%&gt;%\n        html_element(\"p.typography_body-l__KUYFJ\") %&gt;%\n        html_text2()\n      \n      \n      reviews &lt;- rbind(reviews, data.frame(\n        website_name = data$website_name[j],\n        name = name, \n        rating = rating,\n        published = published,\n        verified = verified,\n        title = title, \n        content = content\n      ))\n      \n      i&lt;-i+1\n    }\n    print(paste0(data$website_name[j], \" has been scraped\"))\n    \n    j&lt;-j+1\n  }\n\n  return(reviews)\n}\n\nhak&lt;-trustpilot_reviews(data_scrap)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Webscraping</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Balech, Sophie. 2022. “Une Application Du Modèle ELM ( Elaboration Likelihood Model ) Au Partage d’information Sur Twitter : Étude Du Rôle de La Forme Du Message Et Du Profil de l’émetteur:” Innovations n° 69 (3): 129–61. https://doi.org/10.3917/inno.pr2.0135.\n\n\nBalech, Sophie, and Christophe Benavent. 2022. “Le Rôle Des Dimensions de l’expérience Dans La Satisfaction Client : Une Application Au Cas de l’industrie Hôtelière En Polynésie Française.” In 38ème Congrès Internation de l’AFM, 28–38. Tunis, Tunisie.\n\n\nBarron, Alexander T. J., Jenny Huang, Rebecca L. Spang, and Simon DeDeo. 2018. “Individuals, Institutions, and Innovation in the Debates of the French Revolution.” Proceedings of the National Academy of Sciences 115 (18): 4607–12. https://doi.org/10.1073/pnas.1717729115.\n\n\nBoegershausen, Johannes, Hannes Datta, Abhishek Borah, and Andrew T. Stephen. 2022. “Fields of Gold: Scraping Web Data for Marketing Insights.” Journal of Marketing 86 (5): 1–20. https://doi.org/10.1177/00222429221100750.\n\n\nHartmann, Jochen, Mark Heitmann, Christina Schamp, and Oded Netzer. 2021. “The Power of Brand Selfies.” Journal of Marketing Research 58 (6): 1159–77. https://doi.org/10.1177/00222437211037258.\n\n\nKozlowski, Austin C, Matt Taddy, and James A Evans. 2019. “The Geometry of Culture: Analyzing Meaning Through Word Embeddings.” American Sociological Review 89 (5): 769–981. https://doi.org/10.1177/0003122419877135.\n\n\nKumar, Sunil, Arpan Kumar Kar, and P. Vigneswara Ilavarasan. 2021. “Applications of Text Mining in Services Management: A Systematic Literature Review.” International Journal of Information Management Data Insights 1 (1): 100008. https://doi.org/10.1016/j.jjimei.2021.100008.\n\n\nLefrançois, Alicia, Sophie Balech, and Sophie Changeur. 2022. “Transgression Et Consommation: Revue Intégrative Et Proposition d’un Agenda de Recherche.” In. Le Havre, France.\n\n\nLiu, Angela Xia, Yilin Li, and Sean Xin Xu. 2021. “Assessing the Unacquainted: Inferred Reviewer Personality and Review Helpfulness.” MIS Quarterly 45 (3): 1113–48. https://doi.org/10.25300/MISQ/2021/14375.\n\n\nTirunillai, Seshadri, and Gerard J. Tellis. 2012. “Does Chatter Really Matter? Dynamics of User-Generated Content and Stock Performance.” Marketing Science 31 (2): 198–215. https://doi.org/10.1287/mksc.1110.0682.\n\n\nVannoni, Matia. 2022. “A Political Economy Approach to the Grammar of Institutions: Theory and Methods.” Policy Studies Journal 50 (2): 453–71. https://doi.org/10.1111/psj.12427.",
    "crumbs": [
      "Appendices",
      "References"
    ]
  }
]