[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NLP avec R pour les sciences sociales",
    "section": "",
    "text": "Introduction\nCeci est le support pour le cours “NLP avec R pour les sciences sociales” réalisé auprès des doctorants de l’école doctorale SHS de l’UPJV.\nL’objectif est de donner aux apprenants des méthodes et outils pour traiter de larges corpus de texte pour répondre à leur problématique de recherche.\nL’environnement de traitement des données est R et Rstudio.\nIci, une présentation en guise d’introduction.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#ressources-en-ligne",
    "href": "index.html#ressources-en-ligne",
    "title": "NLP avec R pour les sciences sociales",
    "section": "Ressources en ligne",
    "text": "Ressources en ligne\nR et RStudio\n\nhttps://www.r-project.org/\nhttps://rstudio.com/products/rstudio/\n\nDes ressources en ligne\n\nIntroduction à R et au tidyverse\nR for data science\nHands-On Programming with R\nText mining with R\nTutoriels Quanteda\nLes techniques du NLP pour la recherche en sciences de gestion\nNLP avec r et en français - un Manuel synthétique\nQuarto pour communiquer\nLe séminaire du Collège de France : Apprendre les langues aux machines - B. Sagot",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "1  Prise en main",
    "section": "",
    "text": "1.1 Introduction\nLe document de travail contient deux types d’éléments : du texte pour expliquer et présenter ce que l’on fait et du code pour réaliser les manipulations de données, les analyses et les graphiques. On commence toujours un document avec un bloc de code de setup, pour lister et charger les packages que l’on va utiliser et les options générales pour l’édition du document.\nLa publication des documents peut se faire en différents formats : html, word, pdf (via latex), présentation html (via revealjs), présentation powerpoint, présentation beamer(via latex), … On se référera au site de Quarto pour plus de détails.\nIci, on commence simplement avec quelques manipulations pour comprendre l’environnement de travail, puis on verra comment charger des données sous différents formats.",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Prise en main</span>"
    ]
  },
  {
    "objectID": "chapter1.html#premières-manipulations",
    "href": "chapter1.html#premières-manipulations",
    "title": "1  Prise en main",
    "section": "1.2 Premières manipulations",
    "text": "1.2 Premières manipulations\n\nCréer un document script (.R) : pour simplement éditer du code\nCréer un document quarto (.qmd) : pour mixer du code et du texte\nCommenter du code : #vous permet d’écrire un commentaire dans le code\nAfficher de l’aide sur une fonction : F1 ou lancer la ligne ?‘nom de la fonction’",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Prise en main</span>"
    ]
  },
  {
    "objectID": "chapter1.html#charger-des-données",
    "href": "chapter1.html#charger-des-données",
    "title": "1  Prise en main",
    "section": "1.3 Charger des données",
    "text": "1.3 Charger des données\n\n1.3.1 Un tableau de données\nFichier .csv, .xlsx, .rds\n\ndata&lt;-read.csv(\"le/chemin/de/mon/fichier.csv\")\n\nlibrary(readxl)\ndata&lt;-read_xlsx(\"le/chemin/de/mon/fichier.xlsx\")\n\nlibrary(readr)\ndata&lt;- read_rds(\"le/chemin/de/mon/fichier.rds\")\n\n\n\n1.3.2 Une collection de fichier textes\nUn dossier avec plusieurs fichier .txt ou .docx ou .pdf\n\nlibrary(readtext)\n#Exemple de nom de document : \"int1_2024_dirigeant.txt\"\ndata&lt;-readtext(\"le/chemin/de/mes/fichiers/*.txt\",\n         docvarsfrom = \"filenames\", \n         docvarnames = c(\"int\", \"année\", \"type\"),\n         dvsep = \"_\")\n\ndata&lt;-readtext(\"le/chemin/de/mes/fichiers/*.docx\")\n\ndata&lt;-readtext(\"le/chemin/de/mes/fichiers/*.pdf\")\n\nPour les problèmes de mise en forme, on consultera la vignette du package readtext.\nUne autre solution pour les fichiers pdf, permettant d’enlever tous les éléments de mise en forme :\n\nlibrary(tm)\n#on récupère les noms des fichiers à lire depuis les dossiers\nfiles &lt;- list.files( pattern = \"pdf$\", recursive = T, include.dirs = T)\n\n#on lit les fichiers, sans la mise en forme\ncorp&lt;-Corpus(URISource(files),\n               readerControl = list(reader = readPDF, text=(\"-layout\")))\n#on enlève les sauts de page et autres mises en forme à partir d'espace\ncorp &lt;- tm_map(corp, stripWhitespace)\n\nUn autre outil pour les pdf : le package pdftools.\n\n\n1.3.3 Reconnaissance Optique des Caractères (OCR)\nPour ça, on utilise le package tesseract :\nExemple avec cette image : \n\nlibrary(tesseract)\ntesseract_download(\"fra\") #pour télécharger le modèle de langage\n\ntext &lt;- tesseract::ocr(\"N1_avril1909b.jpeg\", engine = \"fra\")\n\ncat(text) #pour afficher le texte avec sa mise en page",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Prise en main</span>"
    ]
  },
  {
    "objectID": "chapter2.html",
    "href": "chapter2.html",
    "title": "2  Pré-traitements",
    "section": "",
    "text": "2.1 Introduction\nLe document de travail contient deux types d’éléments : du texte pour expliquer et présenter ce que l’on fait et du code pour réaliser les manipulations de données, les analyses et les graphiques. On commence toujours un document avec un bloc de code de setup, pour lister et charger les packages que l’on va utiliser et les options générales pour l’édition du document.\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(quanteda)\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(RColorBrewer)\nDans un premier temps, nous allons tout simplement charger la base de données de travail puis la décrire. Ensuite, nous créerons un corpus, le visualisons. Puis nous effectueront quelques analyses liminaires, avant de voir les pré-traitements à réaliser sur le corpus.",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Pré-traitements</span>"
    ]
  },
  {
    "objectID": "chapter2.html#introduction",
    "href": "chapter2.html#introduction",
    "title": "2  Pré-traitements",
    "section": "",
    "text": "2.1.1 Les données\n\n#On charge les données, stockées dans un fichier csv\n\ndata &lt;- read_csv(\"data/data_trustpilot_oiseaux.csv\")\n\nRows: 4388 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): auteur, date, month, comments\ndbl (3): id, year, note\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnames(data)\n\n[1] \"id\"       \"auteur\"   \"date\"     \"month\"    \"year\"     \"note\"     \"comments\"\n\nview(data)\ndata\n\n# A tibble: 4,388 × 7\n      id auteur                   date            month    year  note comments  \n   &lt;dbl&gt; &lt;chr&gt;                    &lt;chr&gt;           &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;     \n 1     1 MAQUET Cyril             07 août 2023    août     2023     5 \"Comme to…\n 2     2 Mme Laurence  Wolff      08 août 2023    août     2023     5 \"Le délai…\n 3     3 Une nouvelle cliente     07 août 2023    août     2023     5 \"Produits…\n 4     4 Patricia ALLAMAN         15 août 2023    août     2023     5 \"Envoi ra…\n 5     5 VPL                      24 juillet 2023 juillet  2023     5 \"Expéditi…\n 6     6 PHILIPPE GODIN           08 août 2023    août     2023     5 \"site sér…\n 7     7 Mme MARIA ADILIA PEREIRA 15 août 2023    août     2023     1 \"deux sem…\n 8     8 Rachel Mattyssen         31 juillet 2023 juillet  2023     5 \"Très bie…\n 9     9 Estelle Fay              16 juillet 2023 juillet  2023     5 \"Enfin un…\n10    10 Mme T.                   06 août 2023    août     2023     4 \"Satisfai…\n# ℹ 4,378 more rows\n\n#Résumé des données\nsummary(data)\n\n       id          auteur              date              month          \n Min.   :   1   Length:4388        Length:4388        Length:4388       \n 1st Qu.:1098   Class :character   Class :character   Class :character  \n Median :2194   Mode  :character   Mode  :character   Mode  :character  \n Mean   :2194                                                           \n 3rd Qu.:3291                                                           \n Max.   :4388                                                           \n      year           note         comments        \n Min.   :2013   Min.   :1.000   Length:4388       \n 1st Qu.:2018   1st Qu.:5.000   Class :character  \n Median :2020   Median :5.000   Mode  :character  \n Mean   :2019   Mean   :4.641                     \n 3rd Qu.:2021   3rd Qu.:5.000                     \n Max.   :2023   Max.   :5.000                     \n\n\n\n\n2.1.2 Premières analyses/visualisations des données\nAvant de s’intéresser au contenu des commentaires, explorons la structure des données. On va regarder la distribution des commentaires et des notes dans le temps, et s’intéresser à la longueur des avis clients.\n\n#Les années \ndata$year&lt;-as.factor(data$year)\nsummary(data$year)\n\n2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 \n 125  209  249  171  324  341  375  735  853  623  383 \n\ndata%&gt;%\n  group_by(year)%&gt;%\n  summarise(n=n(), prop=n/nrow(data))%&gt;%\n  ggplot(aes(year,prop))+\n  geom_col(fill=\"green\",show.legend = TRUE)+\n  scale_y_continuous(labels = scales::percent)+\n  theme_light()+\n  labs(title = \"Répartition des avis dans le temps\", subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\", x=\"années\", y=NULL)\n\n\n\n\n\n\n\n#Les notes\nsummary(data$note)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   5.000   5.000   4.641   5.000   5.000 \n\nsummary(as.factor(data$note))\n\n   1    2    3    4    5 \n 116   62  169  589 3452 \n\ndata%&gt;%\n  group_by(note)%&gt;%\n  summarise(n=n(), prop=n/nrow(data))%&gt;%\n  ggplot(aes(note,prop))+\n  geom_col(fill=c(\"red\",\"pink\",\"orange\",\"gold\",\"lightgreen\"))+\n  annotate(\"text\", x=2, y=0.7, label=paste(\"Note moyenne = \",round(mean(data$note),1)))+\n  scale_y_continuous(labels=scales::percent)+\n  theme_light()+\n  labs(title = \"Répartition des avis en fonction des notes\", subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\", x=\"note\", y=NULL)\n\n\n\n\n\n\n\n#Le nombre de caractère\ndata$nb_caractere&lt;-nchar(data$comments) #on compte le nombre de caractère de chaque commentaire\nsummary(data$nb_caractere)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   12.0    41.0    74.0   104.3   130.0  1571.0 \n\nggplot(data, aes(nb_caractere))+\n  geom_boxplot()+\n  scale_y_continuous(NULL, breaks = NULL)+\n  labs(x=NULL,title = \"Nombre de caractères des avis\", subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\")+\n  theme_light()+\n  coord_flip()\n\n\n\n\n\n\n\n#Le nombre de tokens\n\ndata$nb_token&lt;-ntoken(data$comments) #on compte le nombre de caractère de chaque commentaire\n\nWarning: ntoken.character()/ntype.corpus() was deprecated in quanteda 4.0.0.\nℹ Please use ntoken(tokens(x)) instead.\n\nsummary(data$nb_token)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    2.0     7.0    13.0    19.3    24.0   308.0 \n\nggplot(data, aes(nb_token))+\n  geom_boxplot()+\n  scale_y_continuous(NULL, breaks = NULL)+\n  labs(x=NULL,title = \"Nombre de tokens des avis\", subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\")+\n  theme_light()+\n  coord_flip()\n\n\n\n\n\n\n\n#On va filtrer au-dessus de 100 tokens\ndata_100t&lt;-data%&gt;%filter(nb_token&lt;50)\n  \nggplot(data_100t, aes(nb_token))+\n  geom_boxplot()+\n  scale_y_continuous(NULL, breaks = NULL)+\n  labs(x=NULL,title = \"Nombre de tokens des avis\", subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\")+\n  theme_light()+\n  coord_flip()\n\n\n\n\n\n\n\n#Les notes dans le temps\ndata%&gt;%\n  mutate(note=as.factor(note))%&gt;%\n  group_by(year, note)%&gt;%\n  summarise(n=n() ,prop=n/nrow(data))%&gt;%\n  ggplot(aes(year, prop))+\n  geom_col(aes(fill=note), show.legend = FALSE)+\n  scale_fill_discrete(type=c(\"red\",\"pink\",\"orange\",\"gold\",\"lightgreen\"))+\n  scale_y_continuous(labels=scales::percent)+\n  theme_light()+\n  labs(title = \"Répartition des avis dans le temps\", subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\", x=\"années\", y=NULL)\n\n\n\n\n\n\n\ndata%&gt;%\n  mutate(note=as.factor(note))%&gt;%\n  group_by(year, note)%&gt;%\n  summarise(n=n())%&gt;%\n  ggplot(aes(x=year, y=n, group=note))+\n  geom_bar(position=\"fill\",stat=\"identity\", aes(fill=note))+\n  scale_fill_discrete(type=c(\"red\",\"pink\",\"orange\",\"gold\",\"lightgreen\"))+\n  scale_y_continuous(labels=scales::percent)+\n  theme_minimal()+\n  labs(title = \"Comparaison de la répartition des notes dans le temps\", subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\", x=\"années\", y=NULL)",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Pré-traitements</span>"
    ]
  },
  {
    "objectID": "chapter2.html#création-du-corpus-et-premières-observations-du-corpus",
    "href": "chapter2.html#création-du-corpus-et-premières-observations-du-corpus",
    "title": "2  Pré-traitements",
    "section": "2.2 Création du corpus et premières observations du corpus",
    "text": "2.2 Création du corpus et premières observations du corpus\nTout d’abord, nous transformons le jeu de données en corpus. La variable qui contient le texte est “comments”, les autres variables vont devenir des métadonnées du corpus, c’est-à-dire des variables associées à chaque texte. Cela sera utile par le suite pour faire des analyses comparatives entre les textes suivant différentes variables (le temps en particulier, mais pas seulement).\n\n#Création du corpus\ncorpus_oiseaux&lt;-corpus(data, text_field = \"comments\")\ncorpus_oiseaux\n\nCorpus consisting of 4,388 documents and 8 docvars.\ntext1 :\n\"Comme toujours, super service. Ne changez rien!(sauf peut êt...\"\n\ntext2 :\n\"Le délai de ma commande super rapide Le délais des ma comman...\"\n\ntext3 :\n\"Produits de qualité et équipe professionnelle … Très bon acc...\"\n\ntext4 :\n\"Envoi rapide, bien emballé et conforme à l'annonce\"\n\ntext5 :\n\"Expédition internationale ! J'ai récemment déménagé en Espag...\"\n\ntext6 :\n\"site sérieux bon produit livraisons plus que correct mais pe...\"\n\n[ reached max_ndoc ... 4,382 more documents ]\n\ncorpus_oiseaux[\"text600\"] #pour visualiser un texte précis\n\nCorpus consisting of 1 document and 8 docvars.\ntext600 :\n\"Commande reçu dans les temps pas de surprise. Très bon site\"\n\na&lt;-corpus_oiseaux[\"text30\"]\nrm(a)\n\nEnsuite, nous allons extraire de chaque texte les termes qui les composent. Ces termes sont nommés “token” (jeton), et comme vous pouvez le voir, ce ne sont pas uniquement des mots, mais tout caractère ou suite de caractères séparés des autres par un espace.\n\n#Extraction des tokens\ntok&lt;-tokens(corpus_oiseaux)\ntok[\"text600\"]\n\nTokens consisting of 1 document and 8 docvars.\ntext600 :\n [1] \"Commande\" \"reçu\"     \"dans\"     \"les\"      \"temps\"    \"pas\"     \n [7] \"de\"       \"surprise\" \".\"        \"Très\"     \"bon\"      \"site\"    \n\n\nChaque texte est maintenant décomposé en une suite de tokens. Pour voir les termes les plus fréquents dans le corpus, ainsi que leur co-occurrences (apparition de deux termes en même temps), il convient de transformer l’objet tok en une matrice termes-documents. En ligne, tous les tokens identifiés, en ligne, tous les textes du corpus, et les valeurs correspondent au nombre d’occurrences (d’apparitions) de chaque token dans chaque document. Une particularité de cette matrice est qu’elle contient énormément de zéro.\n\n#Transformation en document-term frequency matrix\ndfm&lt;-dfm(tok)\ndfm\n\nDocument-feature matrix of: 4,388 documents, 5,658 features (99.72% sparse) and 8 docvars.\n       features\ndocs    comme toujours , super service . ne changez rien !\n  text1     1        1 1     1       1 4  1       1    1 1\n  text2     0        1 1     2       0 3  0       0    0 0\n  text3     0        0 0     0       0 2  0       0    0 0\n  text4     0        0 1     0       0 0  0       0    0 0\n  text5     0        0 4     0       0 2  1       0    0 4\n  text6     0        0 0     0       0 0  0       0    0 0\n[ reached max_ndoc ... 4,382 more documents, reached max_nfeat ... 5,648 more features ]\n\n\nEnfin, nous pouvons avoir un aperçu des termes les plus fréquents. Nous les visualisons d’abord sous forme de tableau (les 20 tokens les plus fréquents), puis sous la forme d’un nuage de mots, où la taille des mots correspond à leur fréquence dans le corpus.\n\n#Visualisation des termes les plus fréquents\ntextstat_frequency(dfm, n=20) #les 20 premiers termes les plus fréquents\n\n     feature frequency rank docfreq group\n1          .      4777    1    2094   all\n2          ,      2788    2    1576   all\n3         de      2685    3    1646   all\n4         et      2530    4    1915   all\n5       très      1996    5    1529   all\n6     rapide      1659    6    1569   all\n7         je      1622    7    1221   all\n8  livraison      1412    8    1299   all\n9          !      1349    9     649   all\n10         à      1312   10    1022   all\n11  commande      1238   11    1030   all\n12        le      1215   12     812   all\n13        la      1215   12     868   all\n14      pour      1049   14     782   all\n15       les      1013   15     754   all\n16      bien       920   16     803   all\n17        en       801   17     618   all\n18      site       793   18     686   all\n19  produits       738   19     661   all\n20        un       729   20     565   all\n\ntextplot_wordcloud(dfm) #nuage de mots\n\n\n\n\n\n\n\n\nPour conclure sur cette première approche du corpus, nous voyons que nos analyses sont gếnées par la présence de la ponctuation et de plein de petits mots “vides de sens” (les articles par exemple). C’est pourquoi nous allons nettoyer le corpus pour avoir une meilleure vision de ce qu’il contient.",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Pré-traitements</span>"
    ]
  },
  {
    "objectID": "chapter2.html#nettoyage-du-corpus",
    "href": "chapter2.html#nettoyage-du-corpus",
    "title": "2  Pré-traitements",
    "section": "2.3 Nettoyage du corpus",
    "text": "2.3 Nettoyage du corpus\nLe nettoyage du corpus pour les analyses se fait lors de la transformation en tokens. Nous allons ajouter des options pour supprimer la ponctuation, les chiffres et les stopwords (les mots qui n’apportent pas de sens sémantique mais permettent l’articulation du discours).\n\nstopwords(\"fr\")\n\n  [1] \"au\"       \"aux\"      \"avec\"     \"ce\"       \"ces\"      \"dans\"    \n  [7] \"de\"       \"des\"      \"du\"       \"elle\"     \"en\"       \"et\"      \n [13] \"eux\"      \"il\"       \"je\"       \"la\"       \"le\"       \"leur\"    \n [19] \"lui\"      \"ma\"       \"mais\"     \"me\"       \"même\"     \"mes\"     \n [25] \"moi\"      \"mon\"      \"ne\"       \"nos\"      \"notre\"    \"nous\"    \n [31] \"on\"       \"ou\"       \"par\"      \"pas\"      \"pour\"     \"qu\"      \n [37] \"que\"      \"qui\"      \"sa\"       \"se\"       \"ses\"      \"son\"     \n [43] \"sur\"      \"ta\"       \"te\"       \"tes\"      \"toi\"      \"ton\"     \n [49] \"tu\"       \"un\"       \"une\"      \"vos\"      \"votre\"    \"vous\"    \n [55] \"c\"        \"d\"        \"j\"        \"l\"        \"à\"        \"m\"       \n [61] \"n\"        \"s\"        \"t\"        \"y\"        \"été\"      \"étée\"    \n [67] \"étées\"    \"étés\"     \"étant\"    \"suis\"     \"es\"       \"est\"     \n [73] \"sommes\"   \"êtes\"     \"sont\"     \"serai\"    \"seras\"    \"sera\"    \n [79] \"serons\"   \"serez\"    \"seront\"   \"serais\"   \"serait\"   \"serions\" \n [85] \"seriez\"   \"seraient\" \"étais\"    \"était\"    \"étions\"   \"étiez\"   \n [91] \"étaient\"  \"fus\"      \"fut\"      \"fûmes\"    \"fûtes\"    \"furent\"  \n [97] \"sois\"     \"soit\"     \"soyons\"   \"soyez\"    \"soient\"   \"fusse\"   \n[103] \"fusses\"   \"fût\"      \"fussions\" \"fussiez\"  \"fussent\"  \"ayant\"   \n[109] \"eu\"       \"eue\"      \"eues\"     \"eus\"      \"ai\"       \"as\"      \n[115] \"avons\"    \"avez\"     \"ont\"      \"aurai\"    \"auras\"    \"aura\"    \n[121] \"aurons\"   \"aurez\"    \"auront\"   \"aurais\"   \"aurait\"   \"aurions\" \n[127] \"auriez\"   \"auraient\" \"avais\"    \"avait\"    \"avions\"   \"aviez\"   \n[133] \"avaient\"  \"eut\"      \"eûmes\"    \"eûtes\"    \"eurent\"   \"aie\"     \n[139] \"aies\"     \"ait\"      \"ayons\"    \"ayez\"     \"aient\"    \"eusse\"   \n[145] \"eusses\"   \"eût\"      \"eussions\" \"eussiez\"  \"eussent\"  \"ceci\"    \n[151] \"cela\"     \"celà\"     \"cet\"      \"cette\"    \"ici\"      \"ils\"     \n[157] \"les\"      \"leurs\"    \"quel\"     \"quels\"    \"quelle\"   \"quelles\" \n[163] \"sans\"     \"soi\"     \n\ntok&lt;-tokens(corpus_oiseaux, remove_punct = TRUE, remove_numbers = TRUE)%&gt;%\n  tokens_remove(stopwords(\"fr\"))\ncorpus_oiseaux[\"text600\"]\n\nCorpus consisting of 1 document and 8 docvars.\ntext600 :\n\"Commande reçu dans les temps pas de surprise. Très bon site\"\n\ntok[\"text600\"]\n\nTokens consisting of 1 document and 8 docvars.\ntext600 :\n[1] \"Commande\" \"reçu\"     \"temps\"    \"surprise\" \"Très\"     \"bon\"      \"site\"    \n\n\nEnsuite, on transforme en dfm et on visualise ce que ça donne.\n\ndfm&lt;-dfm(tok)\ndfm\n\nDocument-feature matrix of: 4,388 documents, 5,430 features (99.81% sparse) and 8 docvars.\n       features\ndocs    comme toujours super service changez rien sauf peut être là\n  text1     1        1     1       1       1    1    1    1    1  1\n  text2     0        1     2       0       0    0    0    0    0  1\n  text3     0        0     0       0       0    0    0    0    0  0\n  text4     0        0     0       0       0    0    0    0    0  0\n  text5     0        0     0       0       0    0    0    0    0  0\n  text6     0        0     0       0       0    0    0    0    0  0\n[ reached max_ndoc ... 4,382 more documents, reached max_nfeat ... 5,420 more features ]\n\ntextstat_frequency(dfm, n=20)\n\n      feature frequency rank docfreq group\n1        très      1996    1    1529   all\n2      rapide      1659    2    1569   all\n3   livraison      1412    3    1299   all\n4    commande      1238    4    1030   all\n5        bien       920    5     803   all\n6        site       793    6     686   all\n7    produits       738    7     661   all\n8           a       706    8     566   all\n9     produit       676    9     609   all\n10        bon       639   10     575   all\n11      merci       553   11     529   all\n12 recommande       553   11     534   all\n13    parfait       522   13     474   all\n14    qualité       487   14     447   all\n15       j'ai       382   15     289   all\n16    oiseaux       379   16     319   all\n17       tout       375   17     347   all\n18       reçu       362   18     332   all\n19      colis       360   19     313   all\n20 rapidement       356   20     343   all\n\ntextplot_wordcloud(dfm)\n\n\n\n\n\n\n\ng&lt;-textstat_frequency(dfm,n=20)\n\n\nggplot(g, aes(x = feature, y=frequency))+\n  geom_col()+\n  coord_flip()\n\n\n\n\n\n\n\nggplot(g, aes(x = reorder(feature, frequency), y=frequency, fill=frequency))+\n  geom_col(show.legend = TRUE)+\n  coord_flip()+\n  theme_light()+\n  scale_fill_distiller(palette = \"Blues\", direction = 1)+\n  labs(title=\"Les mots les plus fréquents\",subtitle = \"du corpus Oiseaux Mania\",caption = \"Source : Data TrustPilot\")+\n  xlab(NULL)+\n  ylab(\"Fréquence\")\n\n\n\n\n\n\n\ndisplay.brewer.all()\n\n\n\n\n\n\n\n\nGlobalement, la commande et la livraison sont TRÈS rapides et les produits sont bons. La surreprésentation de ces termes dans le corpus nous empêche de voir les thématiques abordées de manière moins évidentes. Nous avons plusieurs solutions qui s’offrent à nous : filtrer les mots trop fréquents du corpus ou nous intéresser à une autre mesure de la fréquence d’apparition. Nous allons d’abord filtrer le corpus.\nOn peut aussi vouloir remplacer des termes par d’autres, comme ici “produits” par “produit”.\n\ntok&lt;-tokens_replace(tok, \"produits\", \"produit\")\ndfm&lt;-dfm(tok)\ntextstat_frequency(dfm, n=20)\n\n      feature frequency rank docfreq group\n1        très      1996    1    1529   all\n2      rapide      1659    2    1569   all\n3     produit      1414    3    1243   all\n4   livraison      1412    4    1299   all\n5    commande      1238    5    1030   all\n6        bien       920    6     803   all\n7        site       793    7     686   all\n8           a       706    8     566   all\n9         bon       639    9     575   all\n10      merci       553   10     529   all\n11 recommande       553   10     534   all\n12    parfait       522   12     474   all\n13    qualité       487   13     447   all\n14       j'ai       382   14     289   all\n15    oiseaux       379   15     319   all\n16       tout       375   16     347   all\n17       reçu       362   17     332   all\n18      colis       360   18     313   all\n19 rapidement       356   19     343   all\n20    service       351   20     325   all",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Pré-traitements</span>"
    ]
  },
  {
    "objectID": "chapter2.html#filtrer-le-corpus-des-termes-trop-fréquents",
    "href": "chapter2.html#filtrer-le-corpus-des-termes-trop-fréquents",
    "title": "2  Pré-traitements",
    "section": "2.4 Filtrer le corpus des termes trop fréquents",
    "text": "2.4 Filtrer le corpus des termes trop fréquents\nNous allons filtrer les mots qui sont présents plus de 500 fois dans le corpus.\n\ndfm_trim&lt;-dfm_trim(dfm, max_termfreq = 500)\n\ntextstat_frequency(dfm_trim, n=20)\n\n      feature frequency rank docfreq group\n1     qualité       487    1     447   all\n2        j'ai       382    2     289   all\n3     oiseaux       379    3     319   all\n4        tout       375    4     347   all\n5        reçu       362    5     332   all\n6       colis       360    6     313   all\n7  rapidement       356    7     343   all\n8     service       351    8     325   all\n9        prix       327    9     307   all\n10 satisfaite       327    9     301   all\n11      super       326   11     294   all\n12   conforme       326   11     307   all\n13       rien       317   13     299   all\n14      bonne       293   14     277   all\n15       plus       292   15     254   all\n16       tres       278   16     216   all\n17   toujours       274   17     236   all\n18      mania       256   18     228   all\n19      envoi       239   19     231   all\n20    sérieux       232   20     220   all\n\ntextplot_wordcloud(dfm_trim, max_words = 100, color = rev(brewer.pal(10, \"Set2\")))\n\nWarning in brewer.pal(10, \"Set2\"): n too large, allowed maximum for palette Set2 is 8\nReturning the palette you asked for with that many colors\n\n\n\n\n\n\n\n\ndisplay.brewer.all()\n\n\n\n\n\n\n\n\nUne autre manière de s’y prendre est d’éliminer directement les termes que l’on ne veut pas voir apparaître.\n\ntextstat_frequency(dfm,n=20)\n\n      feature frequency rank docfreq group\n1        très      1996    1    1529   all\n2      rapide      1659    2    1569   all\n3     produit      1414    3    1243   all\n4   livraison      1412    4    1299   all\n5    commande      1238    5    1030   all\n6        bien       920    6     803   all\n7        site       793    7     686   all\n8           a       706    8     566   all\n9         bon       639    9     575   all\n10      merci       553   10     529   all\n11 recommande       553   10     534   all\n12    parfait       522   12     474   all\n13    qualité       487   13     447   all\n14       j'ai       382   14     289   all\n15    oiseaux       379   15     319   all\n16       tout       375   16     347   all\n17       reçu       362   17     332   all\n18      colis       360   18     313   all\n19 rapidement       356   19     343   all\n20    service       351   20     325   all\n\nrem&lt;-c(\"très\",\"rapide\",\"produit\",\"livraison\", \"commande\", \"bien\", \"site\", \"a\", \"bon\", \"merci\", \"recommande\",\"parfait\", \"j'ai\",\"tres\")\n\ndfm_rem&lt;-dfm_remove(dfm, rem)\ntextstat_frequency(dfm_rem, n=20)\n\n      feature frequency rank docfreq group\n1     qualité       487    1     447   all\n2     oiseaux       379    2     319   all\n3        tout       375    3     347   all\n4        reçu       362    4     332   all\n5       colis       360    5     313   all\n6  rapidement       356    6     343   all\n7     service       351    7     325   all\n8        prix       327    8     307   all\n9  satisfaite       327    8     301   all\n10      super       326   10     294   all\n11   conforme       326   10     307   all\n12       rien       317   12     299   all\n13      bonne       293   13     277   all\n14       plus       292   14     254   all\n15   toujours       274   15     236   all\n16      mania       256   16     228   all\n17      envoi       239   17     231   all\n18    sérieux       232   18     220   all\n19      choix       222   19     205   all\n20  satisfait       220   20     205   all",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Pré-traitements</span>"
    ]
  },
  {
    "objectID": "chapter2.html#la-loi-de-zipf",
    "href": "chapter2.html#la-loi-de-zipf",
    "title": "2  Pré-traitements",
    "section": "2.5 La loi de Zipf",
    "text": "2.5 La loi de Zipf\nVérifions la proposition de la loi de Zipf, selon laquelle la fréquence d’apparition d’un terme est inversement proportionnel à son rang.\n\nzipf&lt;-textstat_frequency(dfm)\nggplot(zipf, aes(rank, frequency))+\n  geom_line(color=\"blue\")+\n  geom_point(color=\"darkgreen\")+\n   scale_x_log10() +\n   scale_y_log10()+\n  theme_light()+\n  labs(title = \"Observation de la loi de Zipf\",x=\"log (rang)\",y=\"log (fréquence)\")",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Pré-traitements</span>"
    ]
  },
  {
    "objectID": "chapter2.html#mesures-de-fréquence",
    "href": "chapter2.html#mesures-de-fréquence",
    "title": "2  Pré-traitements",
    "section": "2.6 Mesures de fréquence",
    "text": "2.6 Mesures de fréquence\nOn s’est pour l’instant intéressé uniquement aux termes les plus fréquents dans un corpus. On a vu comment éliminer les termes trop fréquents pour qu’ils nous apportent de l’information. Pour l’analyse de topics, il nous faut prendre un autre angle d’attaque : afin de détecter les sujets abordés dans un corpus, on ne peut se contenter d’observer les mots les plus fréquents, il faut s’intéresser aux termes dont la fréquence dans l’ensemble du corpus est faible, mais qui contribuent fortement à différencier les éléments du corpus entre eux (les documents). On utilise pour cela une mesure de fréquence pondérée : la tf-idf pour term frequency - inverse document frequency qui permet d’accorder plus de poids aux termes les plus discriminants du corpus. \\(tf-idf= \\frac{occurrence\\ du\\ mot\\ dans\\ le\\ document }{nombre\\ de\\ mots\\ dans \\ le \\ document}* log (\\frac{nombre\\ de\\ documents\\ dans\\ le\\ corpus} {nombre\\ de \\ documents\\ dans\\ lequel\\ le\\ mot\\ apparait})\\)\n\n2.6.1 Pondération tf-idf\nOn commence par reprendre nos manipulations précédentes : création de corpus, élimination des stopwords, constitution de bi- ou tri- grammes. On applique ensuite la pondération tf-idf.\n\ndfmtfidf&lt;-dfm_tfidf(dfm)\n\ndfmtfidf\n\nDocument-feature matrix of: 4,388 documents, 5,429 features (99.81% sparse) and 8 docvars.\n       features\ndocs       comme toujours    super  service  changez     rien     sauf     peut\n  text1 1.406738 1.269355 1.173919 1.130383 3.040207 1.166595 2.320047 1.926263\n  text2 0        1.269355 2.347839 0        0        0        0        0       \n  text3 0        0        0        0        0        0        0        0       \n  text4 0        0        0        0        0        0        0        0       \n  text5 0        0        0        0        0        0        0        0       \n  text6 0        0        0        0        0        0        0        0       \n       features\ndocs        être       là\n  text1 1.961025 2.320047\n  text2 0        2.320047\n  text3 0        0       \n  text4 0        0       \n  text5 0        0       \n  text6 0        0       \n[ reached max_ndoc ... 4,382 more documents, reached max_nfeat ... 5,419 more features ]\n\n#Représentations graphiques\ntextplot_wordcloud(dfm, max_words = 200)\n\n\n\n\n\n\n\ntextplot_wordcloud(dfmtfidf, max_words = 200)\n\n\n\n\n\n\n\n#On filtre les mots trop fréquents\n\ndfm_trim&lt;-dfm_trim(dfm, max_termfreq = 500)\n\ndfmtfidf_trim&lt;-dfm_tfidf(dfm_trim)\n\ntextplot_wordcloud(dfm_trim, max_words = 200)\n\n\n\n\n\n\n\ntextplot_wordcloud(dfmtfidf_trim, max_words = 200, color = brewer.pal(6, \"Set2\"))\n\n\n\n\n\n\n\ndisplay.brewer.all()",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Pré-traitements</span>"
    ]
  },
  {
    "objectID": "chapter2.html#comprendre-le-sens-des-termes",
    "href": "chapter2.html#comprendre-le-sens-des-termes",
    "title": "2  Pré-traitements",
    "section": "2.7 Comprendre le sens des termes",
    "text": "2.7 Comprendre le sens des termes\nOn peut visualiser un ou plusieurs termes dans leur contexte, afin d’avoir une meilleure compréhension de leur sens. Pour cela on utilise la fonction “kwic” pour key word in context, à partir de l’objet tokens :\n\nhead(kwic(tok,\"livraison\",window = 3))\n\nKeyword-in-context with 6 matches.                                                     \n  [text2, 12] revanche n’arrive cliquer | livraison |\n  [text3, 13]    depuis conseil jusqu'à | livraison |\n  [text5, 36]  perfection puisque délai | livraison |\n  [text8, 14]    a entière satisfaction | livraison |\n  [text14, 1]                           | Livraison |\n [text15, 21]         trouve peut frais | livraison |\n                            \n adresse toujours là        \n produit recommande vivement\n Espagne depuis France      \n aussi a rapide             \n rapide soignée Bons        \n élevé                      \n\n# kwic(tok,\"livraison\",window = 3)\n\nhead(kwic(tok, c(\"commande\", \"recommande\"),window = 3))\n\nKeyword-in-context with 6 matches.                                                                            \n  [text2, 2]                     délai |  commande  | super rapide délais   \n  [text2, 6]       super rapide délais |  commande  | super rapide revanche \n [text3, 15] jusqu'à livraison produit | recommande | vivement oiseaux Mania\n [text5, 14]   Food regardant dernière |  commande  | j'avais faite France  \n  [text7, 5]       semaines j'ai passé |  commande  | toujours livré étoile \n  [text8, 9]       j'ai néanmoins fait |  commande  | difficulté a entière  \n\nhead(kwic(tok,\"perroquet\",window = 3))\n\nKeyword-in-context with 6 matches.                                                      \n  [text5, 51]    d'avoir trouvé solution | perroquet |\n  [text18, 8]        compétent sers site | perroquet |\n [text22, 18]         convient tout fait | perroquet |\n  [text34, 6] treats granulés compressés | Perroquet |\n [text37, 10]        choix produit petit | perroquet |\n  [text39, 2]               Alimentation | perroquet |\n                              \n afin qu'il puisse            \n  😊 Excellent rapide          \n très heureux aussi           \n Gris Gabon très              \n n'hésiterai refaire commandes\n commande depuis quelques     \n\nhead(kwic(tok,\"prix\",window=10))\n\nKeyword-in-context with 6 matches.             \n  [text14, 6]\n [text15, 15]\n [text26, 23]\n [text28, 12]\n [text31, 77]\n  [text32, 7]\n                                                                                     \n                                                Livraison rapide soignée Bons produit\n            rapide satisfait commander cher oiseaux mania car Besançon trouve produit\n beaucoup choix graines friandises accessoires commande a livrée rapidement encombres\n      harnais reçus déplore juste l’emballage spartiate arrivé moitié déchiré rapport\n                           flacons 57gr chacun ça fait peut chère Mondial relay offre\n                                           Livraison rapide Bons produit bien emballé\n                                                          \n | prix | raisonnables                                    \n | Prix | très attractif trouve peut frais livraison élevé\n | Prix | intéressant                                     \n | prix | vente c’est peu léger surtout harnais           \n | prix | bien plus raisonnable Bien cordialement         \n | prix | raisonnables",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Pré-traitements</span>"
    ]
  },
  {
    "objectID": "chapter3.html",
    "href": "chapter3.html",
    "title": "3  Analyse du sentiment",
    "section": "",
    "text": "3.1 Les données\ndata &lt;- read_csv(\"data/data_trustpilot_oiseaux.csv\")",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analyse du sentiment</span>"
    ]
  },
  {
    "objectID": "chapter3.html#sentiment-analysis",
    "href": "chapter3.html#sentiment-analysis",
    "title": "3  Analyse du sentiment",
    "section": "3.2 Sentiment analysis",
    "text": "3.2 Sentiment analysis\nOn va réaliser une analyse du sentiment du corpus. Pour cela, on utilise le dictionnaire des sentiments et émotions NRC, car il est disponible dans 40 langues, dont le français. Il existe d’autres dictionnaires de sentiments (positif vs négatif), par exemple AFINN ou BING, ainsi que des dictionnaires thématiques (LIWC par exemple), mais ils sont en anglais ou payants, donc utilisables pour des corpus en anglais ou lorsqu’on dispose d’un budget. Les dernières évolutions du traitement en langage naturel des IA (transformers et autres, dont ChatGPT est un exemple), permettent d’autres approches très pertinentes, en utilisant le machine learning, mais cela va au-delà des objectifs de ce cours.\nIci, on utilise le dictionnaire NRC à travers le package syuzhet. La fonction get_nrc_sentiment prend en entrée un vecteur de type caractère.\n\nlibrary(syuzhet)\n\n# d&lt;-get_nrc_sentiment(data$comments, language = \"french\")\n# write_rds(d, \"sentiment_trustpilot_oiseaux.rds\")\n\nd&lt;-read_rds(\"data/sentiment_trustpilot_oiseaux.rds\")\nsummary(d, digits=1)\n\n     anger      anticipation     disgust         fear          joy     \n Min.   :0.0   Min.   : 0.0   Min.   :0.0   Min.   :0.0   Min.   :0.0  \n 1st Qu.:0.0   1st Qu.: 0.0   1st Qu.:0.0   1st Qu.:0.0   1st Qu.:0.0  \n Median :0.0   Median : 1.0   Median :0.0   Median :0.0   Median :0.0  \n Mean   :0.1   Mean   : 0.9   Mean   :0.1   Mean   :0.1   Mean   :0.6  \n 3rd Qu.:0.0   3rd Qu.: 1.0   3rd Qu.:0.0   3rd Qu.:0.0   3rd Qu.:1.0  \n Max.   :7.0   Max.   :10.0   Max.   :7.0   Max.   :6.0   Max.   :6.0  \n    sadness       surprise       trust        negative       positive \n Min.   :0.0   Min.   :0.0   Min.   :0.0   Min.   : 0.0   Min.   : 0  \n 1st Qu.:0.0   1st Qu.:0.0   1st Qu.:0.0   1st Qu.: 0.0   1st Qu.: 1  \n Median :0.0   Median :1.0   Median :1.0   Median : 0.0   Median : 2  \n Mean   :0.2   Mean   :0.7   Mean   :0.9   Mean   : 0.4   Mean   : 2  \n 3rd Qu.:0.0   3rd Qu.:1.0   3rd Qu.:1.0   3rd Qu.: 0.0   3rd Qu.: 3  \n Max.   :7.0   Max.   :5.0   Max.   :8.0   Max.   :12.0   Max.   :14  \n\ndata&lt;-cbind(data,d)\n\ndata[600,8:17]\n\n    anger anticipation disgust fear joy sadness surprise trust negative\n600     0            1       0    1   1       0        1     1        0\n    positive\n600        1\n\ndata[600,\"comments\"]\n\n[1] \"Commande reçu dans les temps pas de surprise. Très bon site\"\n\n\nLe dictionnaire comprend 10 variables, 8 émotions et 2 sentiments. Pour représenter les données, nous avons besoin de les transformer.\n\ne&lt;-d%&gt;%\n  pivot_longer(everything(),names_to = \"sentiment\", values_to = \"nb\")\n\nggplot(e, aes(sentiment, nb))+\n  geom_col(aes(fill=sentiment),show.legend = FALSE)+\n  theme_minimal()+\n  coord_flip()\n\n\n\n\n\n\n\n\n\n3.2.1 Les sentiments\nIntéressons-nous d’abord aux sentiments :\n\nsent&lt;-d%&gt;%\n  select(positive,negative)%&gt;%\n  pivot_longer(everything(), names_to = \"sentiment\", values_to = \"nb\")%&gt;%\n  summarise(nb=sum(nb), .by = sentiment)%&gt;%\n  mutate(prop=nb/sum(nb))\n\nggplot(data=sent,  aes(x=sentiment, y=prop)) + \n  geom_bar(stat=\"identity\", aes(fill=sentiment), show.legend = FALSE)+\n  scale_y_continuous(labels=scales::percent) +\n  labs(title = \"Répartition des sentiments dans le corpus Oiseaux Mania\",caption = \"Données TrustPilot\",x=\"Sentiments\",y=NULL)+\n  scale_fill_manual(values=c(\"red\", \"lightgreen\"))+\n  theme_light()\n\n\n\n\n\n\n\n\nLe corpus est très largement positif, ce qui n’est pas étonnant. On peut aussi créer d’autres indicateurs, comme la valence (différence positif-négatif) ou l’expressivité (somme de positif+négatif).\nExercice : personnalisez le graphique ci-dessous pour la variable d’expressivité.\n\ndata&lt;-data%&gt;%\n  mutate(nbcar=nchar(comments),\n         valence = positive-negative)\n\nggplot(data = data, aes(x = valence, y= nbcar))+\n  geom_point()+\n  geom_smooth()+\n  labs(title=\"Indicateur de valence\", subtitle = \"en fonction du nombre de caractères\", caption=\"Données TrustPilot\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\nSolution :\n\n\nSolution\ndata&lt;-data%&gt;%\n  mutate(nbcar=nchar(comments),\n         expressivité = positive+negative)\n\nggplot(data = data, aes(x = expressivité, y= nbcar))+\n  geom_point()+\n  geom_smooth()+\n  labs(title=\"Indicateur d'expressivité\", subtitle = \"en fonction du nombre de caractères\", caption=\"Données TrustPilot\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n3.2.2 Les émotions\nRegardons maintenant ce qu’il en est de la répartition des émotions :\n\n#On crée d'abord une palette pour les émotions\nemocol&lt;-c(\"yellow\",\"chartreuse\",\"olivedrab3\",\"green4\",\"royalblue3\",\"purple3\",\"red3\",\"orangered1\") \n\n\nemo&lt;-d%&gt;%\n  select(-positive, -negative)%&gt;% #On récupère les émotions\n  pivot_longer(everything(), names_to = \"emotion\", values_to = \"nb\")#On transforme le tableau\n \nemo2&lt;-emo%&gt;%\n  summarise(nb=sum(nb), .by = emotion)%&gt;%\n  mutate(prop=nb/sum(nb),\n          emotion=factor(emotion, ordered = TRUE,levels = c(\"joy\",\"trust\",\"fear\",\"surprise\",\"sadness\",\"disgust\",\"anger\",\"anticipation\")))\n\n#On crée un graphique circulaire\nggplot(data=emo2,  aes(x=emotion, y=prop, colour=emotion)) + \n  geom_bar(stat=\"identity\", aes(fill=emotion), show.legend = FALSE)+ \n  scale_y_continuous(labels=scales::percent)+\n  labs(title=\"Distribution des émotions \\n dans le corpus Oiseaux Mania\", caption=\"Données TrustPilot\", x=\"Emotions\", y=NULL) +\n  coord_polar()+ \n  scale_color_manual(values=emocol)+ scale_fill_manual(values=emocol)+\n  theme_minimal()\n\n\n\n\n\n\n\n#On regarde la répartition des émotions dans le corpus :\n\nggplot(emo, aes(x=emotion, y=nb))+\n  geom_violin(aes(fill=emotion), alpha=0.7,adjust = 2)+\n  theme_minimal()+ scale_fill_manual(values=emocol)+\n  scale_x_discrete(labels=NULL)\n\n\n\n\n\n\n\nggplot(emo, aes(x=emotion, y=nb))+\n  geom_boxplot(aes(fill=emotion,), alpha=0.7,adjust = 2, show.legend = FALSE)+\n  theme_minimal()+ scale_fill_manual(values=emocol)",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analyse du sentiment</span>"
    ]
  },
  {
    "objectID": "chapter3.html#évolution-du-corpus-dans-le-temps",
    "href": "chapter3.html#évolution-du-corpus-dans-le-temps",
    "title": "3  Analyse du sentiment",
    "section": "3.3 Évolution du corpus dans le temps",
    "text": "3.3 Évolution du corpus dans le temps\nOn va regarder comment les sentiments évoluent dans le temps. On doit tout d’abord créer une variable temporel dans notre jeu de données. Nous en avons déjà une, qui indique la date et l’heure à laquelle le commentaire a été posté. Nous allons la transformer pour regrouper les commentaires en fonction de l’année (on peut le faire pour les jours, les mois, les minutes, …).\n\ndata%&gt;%group_by(year)%&gt;%summarise('nb com'=n())\n\n# A tibble: 11 × 2\n    year `nb com`\n   &lt;dbl&gt;    &lt;int&gt;\n 1  2013      125\n 2  2014      209\n 3  2015      249\n 4  2016      171\n 5  2017      324\n 6  2018      341\n 7  2019      375\n 8  2020      735\n 9  2021      853\n10  2022      623\n11  2023      383\n\n\nRegardons maintenant comment évolue les sentiments dans le temps :\n\n#les émotions\n##mise en forme des données\ngen_sent&lt;-data%&gt;%\n  mutate(year=as.factor(year))%&gt;%\n  group_by(year)%&gt;%\n  summarise(across(7:14,~mean(.x, na.rm = T)))%&gt;%\n  na.exclude()%&gt;%\n  pivot_longer(-year, names_to = \"emotion\", values_to = \"mean\")%&gt;%\n  mutate(emotion=factor(emotion, ordered = TRUE,levels = c(\"joy\",\"trust\",\"fear\",\"surprise\",\"sadness\",\"disgust\",\"anger\",\"anticipation\")))\n\n##graphique\nggplot(gen_sent,aes(x=year, y=mean,group=emotion)) +\n  geom_line(aes(color=emotion), linewidth=0.5) + \n  theme_minimal()+\n  scale_color_manual(values = emocol)\n\n\n\n\n\n\n\n#les sentiments\n##mise en forme des données\ngen_sent2&lt;-data%&gt;%\n  mutate(year=as.factor(year))%&gt;%\n  group_by(year)%&gt;%\n  summarise(across(c(positive, negative),~mean(.x, na.rm = T)))%&gt;%\n  na.exclude()%&gt;%\n  pivot_longer(-year, names_to = \"sentiment\",values_to = \"mean\")\n\n##graphique\nggplot(gen_sent2,aes(x=year, y=mean,group=sentiment)) +\n  geom_line(aes(color=sentiment), linewidth=0.5) + \n  theme_minimal()+\n  scale_color_manual(values = c(\"red\",\"lightgreen\"))\n\n\n\n\n\n\n\n\nMaintenant, on va s’intéresser aux mots.",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analyse du sentiment</span>"
    ]
  },
  {
    "objectID": "chapter3.html#nuage-de-mots-comparés",
    "href": "chapter3.html#nuage-de-mots-comparés",
    "title": "3  Analyse du sentiment",
    "section": "3.4 Nuage de mots comparés",
    "text": "3.4 Nuage de mots comparés\n\n3.4.1 En fonction des années\nOn refait les manipulations préliminaires :\n\ndata&lt;-data%&gt;%\n  mutate(year2=case_when(year&lt;2017~\"2013-2016\",\n                         year %in% c(2017:2019)~\"2017-2019\",\n                         .default=as.character(year)))\n\ncorpus_oiseaux&lt;-corpus(data, text_field = \"comments\")\n\ntok&lt;-tokens(corpus_oiseaux, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE)%&gt;%\n  tokens_remove(stopwords(\"fr\"))\n\ndfm&lt;-dfm(tok)\n\nComparons les mots en fonction des années :\n\ndfmgp&lt;-dfm_group(dfm, groups = year2)\ndfmgp\n\nDocument-feature matrix of: 6 documents, 5,392 features (66.69% sparse) and 1 docvar.\n           features\ndocs        comme toujours super service changez rien sauf peut être là\n  2013-2016    38       26    93      58       0   71    6   17    9  8\n  2017-2019    32       61    74      78       0   76    5   13   24  4\n  2020         27       38    42      65       0   51    0    5    6  1\n  2021         35       49    49      64       2   62    8   11   11  2\n  2022         36       49    46      59       0   36    0    4    3  3\n  2023         16       51    22      27       2   21    3    6    3  4\n[ reached max_nfeat ... 5,382 more features ]\n\n#On peut aussi passer la fonction directement en transformant en dfm avec l'option groups : dfm(tok, groups=\"year\")\n\ntextplot_wordcloud(dfmgp, comparison=TRUE, max_words = 200)\n\n\n\n\n\n\n\n\n\n\n3.4.2 En fonction des sentiments\nPour comparer en fonction des sentiments, il faut accéder au dictionnaire NRC (en français) (il y a des fonctions simplifiées pour les dictionnaires en anglais) :\n\ndic_nrc&lt;-read_xlsx(\"NRCfr.xlsx\")%&gt;%\n  pivot_longer(-word,names_to = \"sentiment\", values_to=\"value\")%&gt;%\n  filter(value==1, word!=\"NO TRANSLATION\")%&gt;%\n  select(-value)\n\nsent_term&lt;-convert(dfm,to=\"data.frame\")%&gt;%\n  select(-doc_id)%&gt;%\n  pivot_longer(everything(), names_to=\"word\", values_to=\"value\")%&gt;%\n  filter(value!=0)%&gt;%\n  summarise(value=sum(value), .by=word)%&gt;%\n  inner_join(dic_nrc)%&gt;%\n  slice_max(n=10, by=sentiment, order_by = value, with_ties=F)\n\n\nggplot(sent_term, aes(reorder(word, value), value, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~sentiment, scales = \"free_y\") +\n  labs(y = \"Contribution to sentiment\",\n       x = NULL) +\n  coord_flip()+\n  theme_minimal()",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analyse du sentiment</span>"
    ]
  },
  {
    "objectID": "chapter4.html",
    "href": "chapter4.html",
    "title": "4  Annotations et dépendances syntaxiques",
    "section": "",
    "text": "4.1 Les données\ndata &lt;- read_csv(\"data/data_trustpilot_oiseaux.csv\")\n\nRows: 4388 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): auteur, date, month, comments\ndbl (3): id, year, note\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata$nb_caractere&lt;-nchar(data$comments) #on compte le nombre de caractère de chaque commentaire\nsummary(data$nb_caractere)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   12.0    41.0    74.0   104.3   130.0  1571.0 \n\nmean(data$nb_caractere)\n\n[1] 104.2974\n\nmedian(data$nb_caractere)\n\n[1] 74\n\nround(mean(data$nb_caractere),1)\n\n[1] 104.3\n\nmoy&lt;-round(mean(na.omit(data$nb_caractere)), 1)\n\nggplot(data)+\n  geom_boxplot(aes(nb_caractere))+\n  geom_text(aes(x=500, y=0.2,label=paste(\"Moyenne :\",moy)))+\n  coord_flip()+\n  scale_y_continuous(NULL, breaks = NULL)+\n  theme_minimal()",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Annotations et dépendances syntaxiques</span>"
    ]
  },
  {
    "objectID": "chapter4.html#les-traitements-préliminaires",
    "href": "chapter4.html#les-traitements-préliminaires",
    "title": "4  Annotations et dépendances syntaxiques",
    "section": "4.2 Les traitements préliminaires",
    "text": "4.2 Les traitements préliminaires\nOn reprend ce qu’on a fait au cours dernier, sans éliminer les termes trop fréquents :\n\ncorpus_oiseaux&lt;-corpus(data, text_field = \"comments\")\n\ntok&lt;-tokens(corpus_oiseaux, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE)%&gt;%\n  tokens_remove(stopwords(\"fr\"))\n\ndfm&lt;-dfm(tok)",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Annotations et dépendances syntaxiques</span>"
    ]
  },
  {
    "objectID": "chapter4.html#co-occurrences",
    "href": "chapter4.html#co-occurrences",
    "title": "4  Annotations et dépendances syntaxiques",
    "section": "4.3 Co-occurrences",
    "text": "4.3 Co-occurrences\nOn va maintenant constituer des bi-grammes basés sur de nombreuses co-occurrences entre les termes :\n\n# textstat_collocations(tok)\n\nhead(textstat_collocations(tok), 5)\n\n       collocation count count_nested length   lambda        z\n1 livraison rapide   612            0      2 3.587755 56.29325\n2    oiseaux mania   194            0      2 7.111407 38.50542\n3         très bon   291            0      2 3.325458 37.28183\n4        très bien   332            0      2 2.763473 37.07693\n5    bonne qualité   116            0      2 4.440036 33.65412\n\ntail(textstat_collocations(tok),10)\n\n            collocation count count_nested length    lambda         z\n4046          bien bien     3            0      2 -1.601302 -2.983435\n4047 livraison produits     4            0      2 -1.539254 -3.248433\n4048        site rapide     7            0      2 -1.205217 -3.274181\n4049  commande commande     8            0      2 -1.154362 -3.338450\n4050        bien rapide    10            0      2 -1.111673 -3.567851\n4051    produits rapide     2            0      2 -2.293296 -3.616176\n4052 livraison commande     9            0      2 -1.233460 -3.770310\n4053             très a     2            0      2 -2.581564 -4.071675\n4054      rapide rapide     4            0      2 -2.401230 -5.077424\n4055          très très    22            0      2 -1.110720 -5.196198\n\ncolloc&lt;-textstat_collocations(tok, min_count = 10, tolower = TRUE)\nhead(colloc,10)\n\n        collocation count count_nested length   lambda        z\n1  livraison rapide   612            0      2 3.587755 56.29325\n2     oiseaux mania   194            0      2 7.111407 38.50542\n3          très bon   291            0      2 3.325458 37.28183\n4         très bien   332            0      2 2.763473 37.07693\n5     bonne qualité   116            0      2 4.440036 33.65412\n6         rien dire    93            0      2 5.576935 32.54402\n7       bon produit   146            0      2 3.412030 32.15574\n8      envoi rapide   137            0      2 3.741715 27.61231\n9  produit conforme    94            0      2 3.613662 27.52989\n10  très satisfaite   154            0      2 3.288031 27.11619\n\ntail(colloc,10)\n\n          collocation count count_nested length     lambda         z\n356   livraison merci    10            0      2 -0.4024193 -1.285159\n357 commande produits    11            0      2 -0.3922655 -1.311659\n358       rapide site    13            0      2 -0.4424632 -1.601325\n359  rapide livraison    23            0      2 -0.3478837 -1.653038\n360     bien commande    11            0      2 -0.5705858 -1.910948\n361          rapide a    10            0      2 -0.6915725 -2.214619\n362          a rapide    10            0      2 -0.8863541 -2.840453\n363   commande rapide    21            0      2 -0.6532274 -2.978690\n364       bien rapide    10            0      2 -1.1116732 -3.567851\n365         très très    22            0      2 -1.1107203 -5.196198\n\ntok_cooc&lt;-tokens_compound(tok, pattern = colloc[colloc$z&gt;6.97,],join = TRUE)\n\ntok[\"text400\"]\n\nTokens consisting of 1 document and 7 docvars.\ntext400 :\n[1] \"Excellent\" \"service\"   \"livraison\" \"rapide\"   \n\ntok_cooc[\"text400\"]\n\nTokens consisting of 1 document and 7 docvars.\ntext400 :\n[1] \"Excellent_service\" \"livraison_rapide\" \n\n\nAnalyse de fréquence et représentation graphique :\n\ndfm_cooc&lt;-dfm(tok_cooc)\n\ndfm_cooc2&lt;-dfm_trim(dfm_cooc, max_termfreq = 170)\n\nhead(textstat_frequency(dfm_cooc2),20)\n\n           feature frequency rank docfreq group\n1             prix       169    1     160   all\n2          service       165    2     154   all\n3        perroquet       165    2     150   all\n4          oiseaux       165    2     145   all\n5              top       163    5     149   all\n6          qualité       160    6     153   all\n7    oiseaux_mania       142    7     129   all\n8       rapidement       139    8     137   all\n9             tres       127    9     107   all\n10        articles       124   10     115   all\n11        rapidité       124   10     117   all\n12            reçu       122   12     114   all\n13         sérieux       121   13     114   all\n14             car       120   14     111   all\n15           temps       120   14     115   all\n16       satisfait       115   16     108   all\n17        problème       114   17     102   all\n18           comme       114   17     108   all\n19 très_satisfaite       111   19     107   all\n20         graines       107   20      87   all\n\ntextplot_wordcloud(dfm_cooc2, max_words = 200, color = brewer.pal(6, \"Set2\"))\n\n\n\n\n\n\n\ntok_cooc&lt;-tokens_replace(tok_cooc, c(\"très_rapidement\",\"très_satisfait\"), c(\"très_rapide\",\"très_satisfaite\"))\n\ndfm_cooc&lt;-dfm(tok_cooc)\ntextstat_frequency(dfm_cooc, n=25)\n\n            feature frequency rank docfreq group\n1          commande       715    1     620   all\n2                 a       502    2     412   all\n3         livraison       474    3     432   all\n4              site       401    4     359   all\n5  livraison_rapide       389    5     381   all\n6             merci       380    6     368   all\n7           parfait       361    7     334   all\n8              très       349    8     298   all\n9           produit       331    9     294   all\n10           rapide       328   10     310   all\n11         produits       321   11     292   all\n12       recommande       320   12     313   all\n13             plus       292   13     254   all\n14             bien       280   14     251   all\n15        très_bien       256   15     236   all\n16            super       252   16     232   all\n17             tout       245   17     224   all\n18             j'ai       234   18     189   all\n19            colis       207   19     176   all\n20  très_satisfaite       202   20     195   all\n21            c'est       196   21     168   all\n22         toujours       188   22     163   all\n23      très_rapide       174   23     168   all\n24             prix       169   24     160   all\n25          service       165   25     154   all\n\ndfm_cooc2&lt;-dfm_trim(dfm_cooc, max_termfreq = 175)\n\ntextplot_wordcloud(dfm_cooc2, max_words = 100, color = brewer.pal(6, \"Set2\"))",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Annotations et dépendances syntaxiques</span>"
    ]
  },
  {
    "objectID": "chapter4.html#annotations",
    "href": "chapter4.html#annotations",
    "title": "4  Annotations et dépendances syntaxiques",
    "section": "4.4 Annotations",
    "text": "4.4 Annotations\nPour cette partie, on repart du jeu de données brut.\n\n4.4.1 Détecter les langues\nDans le cas d’un corpus composé de plusieurs langues (par exemple, un corpus extrait de twitter), il peut être intéressant de filtrer le corpus à partir de la langue. On utilise un algorithme, qui peut être long à exécuter selon la taille du corpus, et qui est plutôt performant : cld3. Il repose sur un réseau de neurones développé par Google\n\nlibrary(cld3)\n\ndata$langue&lt;-detect_language(data$comments)\n# data$langue\n\ndata_fr&lt;-data%&gt;%filter(langue==\"fr\")\n\n\n\n4.4.2 POS\n\nlibrary(cleanNLP)\n\n# cnlp_init_udpipe(model_name = \"french\")\n# \n# annotate&lt;-cnlp_annotate(data$comments, verbose = 100)\n# ann_token&lt;-annotate$token\n# write_csv2(ann_token, \"annotation_oiseaux.csv\")\n# write_rds(ann_token,\"annotation_oiseaux.rds\")\n\nann_token&lt;-read_rds(\"data/annotation_oiseaux.rds\")\n\nhead(ann_token%&gt;%filter(upos==\"ADJ\"|upos==\"NOUN\"|upos==\"VERB\"),15)\n\n# A tibble: 15 × 11\n   doc_id   sid tid   token    token_with_ws lemma  upos  xpos  feats tid_source\n    &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;         &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     \n 1      1     1 4     super    \"super \"      super  ADJ   &lt;NA&gt;  Gend… 5         \n 2      1     1 5     service  \"service\"     servi… NOUN  &lt;NA&gt;  Gend… 0         \n 3      1     2 2     changez  \"changez \"    chang… VERB  &lt;NA&gt;  Mood… 0         \n 4      1     2 5     peut     \"peut \"       pouvo… VERB  &lt;NA&gt;  Mood… 2         \n 5      1     2 8     cagnotte \"cagnotte \"   cagnot ADJ   &lt;NA&gt;  Gend… 9         \n 6      1     2 9     fidélité \"fidélité \"   fidél… NOUN  &lt;NA&gt;  Gend… 5         \n 7      1     2 11    est      \"est\"         être   VERB  &lt;NA&gt;  Mood… 9         \n 8      1     2 15    dirais   \"dirais \"     dir    VERB  &lt;NA&gt;  Mood… 11        \n 9      1     2 16    inutile  \"inutile \"    inuti… ADJ   &lt;NA&gt;  Gend… 15        \n10      2     1 2     délai    \"délai \"      délai  NOUN  &lt;NA&gt;  Gend… 0         \n11      2     1 5     commande \"commande \"   comma… NOUN  &lt;NA&gt;  Gend… 2         \n12      2     1 7     rapide   \"rapide \"     rapide ADJ   &lt;NA&gt;  Gend… 5         \n13      2     2 2     délais   \"délais \"     délais NOUN  &lt;NA&gt;  Gend… 14        \n14      2     2 6     commande \"commande \"   comma… NOUN  &lt;NA&gt;  Gend… 2         \n15      2     2 8     rapide   \"rapide\"      rapide ADJ   &lt;NA&gt;  Gend… 6         \n# ℹ 1 more variable: relation &lt;chr&gt;\n\nann_token%&gt;%filter(upos==\"ADJ\"|upos==\"NOUN\"|upos==\"VERB\")\n\n# A tibble: 34,232 × 11\n   doc_id   sid tid   token    token_with_ws lemma  upos  xpos  feats tid_source\n    &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;         &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     \n 1      1     1 4     super    \"super \"      super  ADJ   &lt;NA&gt;  Gend… 5         \n 2      1     1 5     service  \"service\"     servi… NOUN  &lt;NA&gt;  Gend… 0         \n 3      1     2 2     changez  \"changez \"    chang… VERB  &lt;NA&gt;  Mood… 0         \n 4      1     2 5     peut     \"peut \"       pouvo… VERB  &lt;NA&gt;  Mood… 2         \n 5      1     2 8     cagnotte \"cagnotte \"   cagnot ADJ   &lt;NA&gt;  Gend… 9         \n 6      1     2 9     fidélité \"fidélité \"   fidél… NOUN  &lt;NA&gt;  Gend… 5         \n 7      1     2 11    est      \"est\"         être   VERB  &lt;NA&gt;  Mood… 9         \n 8      1     2 15    dirais   \"dirais \"     dir    VERB  &lt;NA&gt;  Mood… 11        \n 9      1     2 16    inutile  \"inutile \"    inuti… ADJ   &lt;NA&gt;  Gend… 15        \n10      2     1 2     délai    \"délai \"      délai  NOUN  &lt;NA&gt;  Gend… 0         \n# ℹ 34,222 more rows\n# ℹ 1 more variable: relation &lt;chr&gt;\n\ng&lt;-ann_token%&gt;%group_by(upos)%&gt;%\n  summarise(n=n())%&gt;%\n  filter(!is.na(upos))\n\nggplot(g)+\n  geom_col(aes(reorder(upos,n),n, fill=n), show.legend = FALSE)+\n  scale_fill_fermenter(palette = \"PuRd\", direction = 1)+\n  coord_flip()+\n  labs(title = \"Fréquence des UPOS\", subtitle = \"Corpus Oiseaux Mania\", caption = \"Data TrustPilot\", x=NULL, y=NULL)+\n  theme_dark()\n\n\n\n\n\n\n\n\nMaintenant, on va s’intéresser à des catégories grammaticales spécifiques :\n\nvocab1&lt;-ann_token%&gt;%\n  filter(upos==\"NOUN\")%&gt;%\n  summarise(freq=n(),.by=lemma)%&gt;%\n  filter(freq&gt;55)\n\nggplot(vocab1,aes(x=reorder(lemma,freq),y=freq))+\n  geom_bar(stat=\"identity\",fill=\"lightgreen\")+\n  coord_flip()+\n  theme_minimal()+\n  labs(title = \"Noms communs les plus fréquents\",subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\",x=\"Noms commun\",y=\"Fréquence\")\n\n\n\n\n\n\n\nvocab1bis&lt;-ann_token%&gt;%\n  filter(upos==\"ADJ\")%&gt;%\n  summarise(freq=n(),.by=lemma)%&gt;%\n  filter(freq&gt;55)\n\nggplot(vocab1bis,aes(x=reorder(lemma,freq),y=freq))+\n  geom_bar(stat=\"identity\",fill=\"darkblue\")+\n  coord_flip()+\n  theme_minimal()+\n  labs(title = \"Adjectifs les plus fréquents\",subtitle = \"Corpus Oiseaux Mania\", caption=\"Data : TrustPilot\",x=\"Adjectifs\",y=\"Fréquence\")\n\n\n\n\n\n\n\nvocab2&lt;-ann_token%&gt;%\n  filter(upos==\"NOUN\" | upos==\"VERB\" | upos==\"ADJ\")%&gt;%\n  summarise(freq=n(),.by=c(lemma,upos))%&gt;%\n  filter(freq&gt;30)%&gt;%\n  mutate(angle= 90 * sample(c(0, 1), n(), replace = TRUE, prob = c(75, 25)))\n\nlibrary(ggwordcloud)\nggplot(vocab2)+\n  geom_text_wordcloud_area(aes(label=lemma, size=freq, color=freq, angle=angle))+\n  scale_size_area(max_size = 24)+\n  scale_color_fermenter(palette = \"Set2\")+\n  theme_minimal()\n\n\n\n\n\n\n\nggplot(vocab2)+\n  geom_text_wordcloud_area(aes(label=lemma, size=freq, color=upos, angle=angle))+\n  scale_size_area(max_size = 24)+\n  scale_color_manual(values=c(\"ADJ\"=\"orange\",\"NOUN\"=\"lightgreen\",\"VERB\"=\"purple\"))+\n  theme_minimal()",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Annotations et dépendances syntaxiques</span>"
    ]
  },
  {
    "objectID": "chapter4.html#les-dépendances-syntaxiques",
    "href": "chapter4.html#les-dépendances-syntaxiques",
    "title": "4  Annotations et dépendances syntaxiques",
    "section": "4.5 Les dépendances syntaxiques",
    "text": "4.5 Les dépendances syntaxiques\nQuels sont les mots associés aux termes cibles ?\n\n#on met à niveau la racine\nann_racine&lt;- ann_token%&gt;%\n  left_join(ann_token,by= c(\"doc_id\"=\"doc_id\", \"sid\"=\"sid\", \"tid_source\"=\"tid\"), suffix=c(\"\", \"_source\"))\n#on filtre les relation nominales puis celle qui concerne les termes cibles\nfoo&lt;-ann_racine %&gt;%\n  filter(relation == \"amod\"|relation ==\"acl\"|relation ==\"nmod\"|relation ==\"appos\") %&gt;%\n  select(qual = lemma, source = lemma_source)%&gt;%\n  filter(source==\"commande\"|source==\"livraison\"|source==\"produit\"|source==\"prix\")%&gt;% \n  group_by(source,qual)%&gt;%\n  summarise(n=n())\n\n`summarise()` has grouped output by 'source'. You can override using the\n`.groups` argument.\n\n# On remet en forme les données\nfoo1&lt;-foo%&gt;%\n  pivot_wider(names_from = source, values_from = n)%&gt;%\n  mutate(across(everything(), ~replace_na(.x,0)))%&gt;%\n  mutate(sum=rowSums(.[,2:5]))%&gt;%\n  filter(sum&gt;15)%&gt;%\n  select(-sum)%&gt;%\n  pivot_longer(!qual, names_to = \"source\", values_to = \"n\")\n\n\n\nggplot(foo1,aes(x=reorder(qual,n), y=n, group=source))+\n  geom_bar(stat=\"identity\",aes(fill=source),position=position_dodge())+\n  coord_flip()+\n  scale_fill_brewer(palette=\"Spectral\",direction = -1)+\n  theme_minimal()+ \n  labs( title=\"Analyse des dépendances nominales\", subtitle = \"les termes du site et du service\",caption = \"Data : TrustPilot sur Oiseaux Mania\", x=\"tokens dépendants\", y=\"Fréquence\", fill=\"Termes\")+\n  facet_wrap(~source, ncol = 4)",
    "crumbs": [
      "Niveau 1 : Initiation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Annotations et dépendances syntaxiques</span>"
    ]
  },
  {
    "objectID": "chapter5.html",
    "href": "chapter5.html",
    "title": "5  Rappel sur les pré-traitements",
    "section": "",
    "text": "5.1 Les données\ndata &lt;- read_csv(\"data/data_trustpilot_oiseaux.csv\")\n\nRows: 4388 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): auteur, date, month, comments\ndbl (3): id, year, note\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# cnlp_init_udpipe(model_name = \"french\")\n# \n# annotate&lt;-cnlp_annotate(data$comments, verbose = 100)\n# ann_token&lt;-annotate$token\n# write_csv2(ann_token, \"annotation_oiseaux.csv\")\n# write_rds(ann_token,\"annotation_oiseaux.rds\")\n\nann_token&lt;-read_rds(\"data/annotation_oiseaux.rds\")",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Rappel sur les pré-traitements</span>"
    ]
  },
  {
    "objectID": "chapter5.html#reconstruire-le-texte",
    "href": "chapter5.html#reconstruire-le-texte",
    "title": "5  Rappel sur les pré-traitements",
    "section": "5.2 Reconstruire le texte",
    "text": "5.2 Reconstruire le texte\n\ndata&lt;-ann_token%&gt;%\n  filter(upos==\"NOUN\"|upos==\"VERB\"|upos==\"ADJ\"|upos==\"ADV\")%&gt;%\n  group_by(doc_id)%&gt;%\n  summarise(text=paste(lemma,collapse = \" \"))%&gt;%\n  inner_join(data, join_by(\"doc_id\"==\"id\"))\n\ncorpus_new&lt;-corpus(data, text_field = \"text\")\n\ntoks&lt;-tokens(corpus_new)%&gt;%\n  tokens_replace(c(\"produire\", \"conformer\",\"colir\", \"tre\", \"livrer\", \"n\"), c(\"produit\", \"conforme\",\"colis\", \"très\", \"livraison\", \"ne\"))%&gt;%\n  tokens_remove(c(\".\",\",\"))\n\ncolloc&lt;-textstat_collocations(toks, min_count = 10, tolower = TRUE)\n\ntoks&lt;-tokens_compound(toks, pattern = colloc[colloc$z&gt;7,])\n\ndfm&lt;-dfm(toks)\nhead(textstat_frequency(dfm),50)\n\n            feature frequency rank docfreq group\n1           produit       604    1     530   all\n2          commande       571    2     505   all\n3           parfait       481    3     441   all\n4         livraison       462    4     414   all\n5       recommander       430    5     417   all\n6         commander       407    6     362   all\n7              site       395    7     356   all\n8              très       370    8     321   all\n9             avoir       337    9     293   all\n10           rapide       315   10     303   all\n11 livraison_rapide       311   11     306   all\n12            super       282   12     254   all\n13           oiseau       281   13     251   all\n14              pas       277   14     226   all\n15        très_bien       272   15     253   all\n16             plus       266   16     231   all\n17             bien       253   17     227   all\n18         recevoir       231   18     210   all\n19               ne       218   19     195   all\n20             dire       201   20     190   all\n21          article       200   21     177   all\n22   très_satisfait       180   22     174   all\n23          service       179   23     168   all\n24         toujours       176   24     160   all\n25            faire       176   24     152   all\n26        perroquet       173   26     158   all\n27              bon       170   27     158   all\n28             tout       153   28     142   all\n29          qualité       150   29     144   all\n30         problème       150   29     137   all\n31           ne_pas       147   31     131   all\n32          trouver       145   32     137   all\n33              top       145   32     138   all\n34            colis       145   32     123   all\n35        excellent       141   35     134   all\n36            petit       137   36     125   all\n37         conforme       132   37     127   all\n38             prix       132   37     125   all\n39          arriver       122   39     117   all\n40          sérieux       122   39     116   all\n41    bien_emballer       119   41     115   all\n42       satisfaire       111   42     109   all\n43             même       108   43     100   all\n44         beaucoup       107   44     104   all\n45             jour       104   45     103   all\n46       rapidement       103   46     100   all\n47          pouvoir        98   47      90   all\n48            jouet        96   48      74   all\n49      très_rapide        94   49      91   all\n50            délai        93   50      86   all\n\ntextplot_wordcloud(dfm, max_size = 4, max_words = 200)",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Rappel sur les pré-traitements</span>"
    ]
  },
  {
    "objectID": "chapter5.html#analyse-de-keyness",
    "href": "chapter5.html#analyse-de-keyness",
    "title": "5  Rappel sur les pré-traitements",
    "section": "5.3 Analyse de keyness",
    "text": "5.3 Analyse de keyness\nDans un premier temps, nous allons regarder le Net Promoter Score (NPS), puis nous étudierons les discours des promoteurs, détracteurs et passifs.\nTout d’abord, nous créons nos catégories en fonction des notes.\n\ncol&lt;- c(\"red\",\"gold\", \"chartreuse\")\n\n\ndata&lt;-data %&gt;%\n  mutate(NPS=case_when(note&lt;4~\"Détracteurs\",\n                       note==4~\"Passifs\",\n                       note&gt;4~\"Promoteurs\"))\n\n\nggplot(data, aes(x=note))+\n  geom_histogram(binwidth = 1, aes(fill=NPS))+\n  labs( title= \" Distribution des scores NPS\", \n        subtitle = paste(\"Moyenne du NPS de l'échantillon\",round(mean(data$note),1)), \n        caption = paste(\"Data : TrustPilot, n=\",nrow(data)), \n        y = \"Fréquence\")+ \n  scale_fill_manual(values=col)+\n  theme_light()\n\n\n\n\n\n\n\n\nPuis nous réalisons un nuage de mots pour chaque groupe, afin d’avoir une idée de ce qui est exprimé.\n\ndfm$NPS&lt;-data$NPS\n# docvars(dfm)\n\ndfm_gp &lt;-dfm%&gt;%\n    dfm_group(groups = NPS)\n# dfm_gp\n\nstat&lt;- dfm_gp %&gt;% \n  textstat_frequency(n = 30,  groups = NPS)\n# stat\n\nggplot(stat, aes(label = feature)) +\n  geom_text_wordcloud(aes(size=log(frequency), color=group)) +\n  theme_minimal()+\n  facet_wrap(vars(group))+\n  scale_color_manual(values=col)+ \n  labs(title=\"Nuage des 30 mots les plus fréquents (Par groupes)\",\n       caption = \"La taille des mots est proportionnelle au log de leurs fréquences\")\n\n\n\n\n\n\n\n\nMaintenant, nous nous intéressons à ce qui caractérise chacun des groupes par rapport aux autres, grâce à la mesure du keyness.\n\ngraph_promoteur&lt;-textstat_keyness(dfm_gp, target = \"Promoteurs\")%&gt;%\n  textplot_keyness(n = 30L, labelsize = 2,   show_legend = FALSE, \n                     show_reference = FALSE,   color = c(\"Darkgreen\", \"gray\"))+\n  labs(x=NULL)\n\n\ngraph_detracteur &lt;- textstat_keyness(dfm_gp, target = \"Détracteurs\" )%&gt;%\n  textplot_keyness(n = 30L, labelsize = 2,   show_legend = FALSE,   \n                     show_reference = FALSE,   color = c(\"firebrick\", \"gray\"))+ \n  labs(x=NULL)\n\n\ngraph_passif &lt;- textstat_keyness(dfm_gp, target = \"Passifs\")%&gt;%\n  textplot_keyness(n = 30L, labelsize = 2,   show_legend = FALSE,   show_reference = FALSE,    color = c(\"gold2\", \"gray\"))+\n  labs(x=NULL)\n\n\nlibrary(cowplot)\n\n\nAttachement du package : 'cowplot'\n\n\nL'objet suivant est masqué depuis 'package:lubridate':\n\n    stamp\n\np&lt;- plot_grid(graph_detracteur, graph_passif ,graph_promoteur,  labels = c('Détracteurs', 'Passifs', 'Promoteurs'), label_size = 10, ncol=3)\n\ntitle &lt;- ggdraw() + draw_label(\"NPS : Les raisons qui conduisent à la recommandation (keyness)\", fontface='bold')\n\nnote &lt;- ggdraw()+ draw_text(\"Les valeurs représentent le keyness des termes.\\nIl mesure leur caractère distinctif par une statistique du chi²\", size=8,x = 0.5, y = 0.5)\n\n\nplot_grid(title, p,note, ncol=1, rel_heights=c(0.1, 1)) # rel_heights values control title margins",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Rappel sur les pré-traitements</span>"
    ]
  },
  {
    "objectID": "chapter5.html#diversité-lexicale-et-lisibilité",
    "href": "chapter5.html#diversité-lexicale-et-lisibilité",
    "title": "5  Rappel sur les pré-traitements",
    "section": "5.4 Diversité lexicale et lisibilité",
    "text": "5.4 Diversité lexicale et lisibilité\n\nhead(textstat_lexdiv(dfm))\n\n  document       TTR\n1        1 1.0000000\n2        2 0.8620690\n3        3 0.9000000\n4        4 1.0000000\n5        5 0.9268293\n6        6 1.0000000\n\nhead(textstat_readability(corpus_new))\n\n  document     Flesch\n1        1  24.440000\n2        2  -4.095000\n3        3  -8.278077\n4        4 -10.755000\n5        5  -7.977442\n6        6  56.978462",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Rappel sur les pré-traitements</span>"
    ]
  },
  {
    "objectID": "chapter6.html",
    "href": "chapter6.html",
    "title": "6  Topic Analysis",
    "section": "",
    "text": "6.1 Introduction\nOn va maintenant s’intéresser à la détection et à l’analyse de topics. Il existe de nombreux algorithmes pour cela. On va commencer avec le modèle original : le modèle LDA, pour Latent Dirichlet Allocation. Puis on s’intéressera à un de ses prolongements, le modèle STM (Structural Topic Modelling).\nL’idée est la suivante : un corpus est considéré comme une collection de documents. Chaque document est considéré comme étant composé d’un mélange de topics. Chaque topic est considéré comme étant composé d’un mélange de tokens. L’algorithme calcule par itération les probabilités d’appartenance des tokens aux topics et des topics aux documents, ce qui nous permet de visualiser la composition des sujets identifiés.\nUn schéma explicatif est proposé par H. Naushan, en 2020.",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>*Topic Analysis*</span>"
    ]
  },
  {
    "objectID": "chapter6.html#introduction",
    "href": "chapter6.html#introduction",
    "title": "6  Topic Analysis",
    "section": "",
    "text": "Description du modèle LDA :",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>*Topic Analysis*</span>"
    ]
  },
  {
    "objectID": "chapter6.html#les-données",
    "href": "chapter6.html#les-données",
    "title": "6  Topic Analysis",
    "section": "6.2 Les données",
    "text": "6.2 Les données\n\ndata &lt;- read_csv(\"data/data_trustpilot_oiseaux.csv\")\n\nRows: 4388 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): auteur, date, month, comments\ndbl (3): id, year, note\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata&lt;-data%&gt;%mutate(text_id=paste0(\"text_\", row_number(data$id)))",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>*Topic Analysis*</span>"
    ]
  },
  {
    "objectID": "chapter6.html#topics-analysis",
    "href": "chapter6.html#topics-analysis",
    "title": "6  Topic Analysis",
    "section": "6.3 Topics Analysis",
    "text": "6.3 Topics Analysis\n\n6.3.1 Le modèle LDA avec topicmodels\nOn travaille à partir du dfm. On doit transformer le format des données afin de l’injecter dans le modèle. On réduit le nombre de termes considérés, ce qui permet de réduire les temps de calcul et de trouver une solution convergente.\nLes résultats du modèle LDA sont très dépendants de la qualité du vocabulaire injecté. Plus on travaille ce vocabulaire, meilleurs sont les résultats. On va donc reprendre tout ce qu’on a fait jusqu’à présent pour améliorer les résultats de notre modèle : on récupère les annotations ; on filtre le vocabulaire pour ne garder que les noms, adjectifs et verbes ; on crée les collocations ; on filtre les occurrences trop et pas assez fréquentes.\n\nann_token&lt;-read_rds(\"data/annotation_oiseaux.rds\")\n\n\ndata&lt;-ann_token%&gt;%\n  filter(upos==\"NOUN\"|upos==\"VERB\"|upos==\"ADJ\"|upos==\"ADV\")%&gt;%\n  group_by(doc_id)%&gt;%\n  summarise(text=paste(lemma,collapse = \" \"))%&gt;%\n  inner_join(data, join_by(\"doc_id\"==\"id\"))\n\ncorpus_new&lt;-corpus(data, text_field = \"text\")\ntoks&lt;-tokens(corpus_new)%&gt;%\n  tokens_replace(c(\"produire\", \"conformer\",\"colir\", \"tre\", \"livrer\", \"n\"), c(\"produit\", \"conforme\",\"colis\", \"très\", \"livraison\", \"ne\"))%&gt;%\n  tokens_remove(c(\".\",\",\"))\n\ncolloc&lt;-textstat_collocations(toks, min_count = 10, tolower = TRUE)\n\ntoks&lt;-tokens_compound(toks, pattern = colloc[colloc$z&gt;7,])\n\n\ndfm_new&lt;-dfm(toks)%&gt;%\n  dfm_trim(min_termfreq = 0.6, termfreq_type = \"quantile\",\n           max_docfreq = 0.1, docfreq_type = \"prop\")\ndtm_new &lt;- convert(dfm_new, to = \"topicmodels\")\n\nset.seed(1234)\nlda &lt;- LDA(dtm_new, k = 5)\n\n#On regarde les résultats\nterms(lda,10)\n\n      Topic 1       Topic 2          Topic 3            Topic 4      \n [1,] \"très\"        \"recommander\"    \"livraison_rapide\" \"livraison\"  \n [2,] \"toujours\"    \"livraison\"      \"site\"             \"très_bien\"  \n [3,] \"rapide\"      \"avoir\"          \"très\"             \"commander\"  \n [4,] \"site\"        \"site\"           \"super\"            \"recommander\"\n [5,] \"livraison\"   \"ne\"             \"pas\"              \"recevoir\"   \n [6,] \"recommander\" \"oiseau\"         \"bien\"             \"tout\"       \n [7,] \"recevoir\"    \"commander\"      \"commander\"        \"pas\"        \n [8,] \"bien\"        \"très\"           \"très_satisfait\"   \"bon\"        \n [9,] \"excellent\"   \"trouver\"        \"service\"          \"perroquet\"  \n[10,] \"perroquet\"   \"très_satisfait\" \"qualité\"          \"bien\"       \n      Topic 5    \n [1,] \"oiseau\"   \n [2,] \"avoir\"    \n [3,] \"plus\"     \n [4,] \"article\"  \n [5,] \"commander\"\n [6,] \"pas\"      \n [7,] \"ne_pas\"   \n [8,] \"super\"    \n [9,] \"rapide\"   \n[10,] \"bon\"      \n\n# topics(lda)\n\ncorpus_new[\"996\"]\n\nCorpus consisting of 1 document and 7 docvars.\n996 :\n\"second commande satisfaction total expédition très rapide em...\"\n\ncorpus_new[\"995\"]\n\nCorpus consisting of 1 document and 7 docvars.\n995 :\n\"très bon produire livraison rapide sérieux recommander produ...\"\n\ncorpus_new[\"999\"]\n\nCorpus consisting of 1 document and 7 docvars.\n999 :\n\"très bon magasin emballage norme trouver besoin conseiller r...\"\n\nterm&lt;-as_tibble(terms(lda,25))%&gt;%\n  mutate(rank=as.numeric(row.names(.)))%&gt;%\n  pivot_longer(-rank, names_to = \"topic\",values_to = \"term\")\n\nggplot(term, aes(x=topic, y= rank, group =  term , label = term)) + \n  scale_y_reverse() +\n  geom_text(aes(color=topic,size=8/log(rank)))+\n  theme_minimal()+\n  scale_color_hue()+\n  guides(color=\"none\",size=\"none\")\n\n\n\n\n\n\n\n\n\n\n6.3.2 Déterminer le nombre de topics optimal\nLe modèle LDA fonctionne à partir d’un nombre de topics donné. La question est donc de savoir quel est le nombre de topics optimal pour décrire notre corpus. Heureusement, des personnes ont créé des fonctions et des procédures pour nous aider dans cette quête. L’idée est de calculer différents modèles pour différents nombres de topics, et de comparer la qualité des résultats. La procédure ci-dessous est en deux parties :\n\nTout d’abord, on compare la qualité de différents indicateurs sur un grand nombre de modèles, pour aboutir à une liste de quelques solutions à comparer plus en détail (de 3 à 10).\nEnsuite, on compare les résultats de la liste réduite de modèles, pour déterminer lequel a la meilleure distribution des topics entre les documents. La distribution recherchée est celle qui distingue le plus les documents en fonction des topics, tout en étant à droite de l’estimation d’une répartition uniforme des documents entre les topics. Le critère de parcimonie nous invite à choisir la solution avec le moins grand nombre de topics, en cas de résultats comparables.\n\n\n##Etape 1 : les meilleures solutions\nlibrary(ldatuning)\nlibrary(magrittr)\n\n\nAttachement du package : 'magrittr'\n\n\nL'objet suivant est masqué depuis 'package:purrr':\n\n    set_names\n\n\nL'objet suivant est masqué depuis 'package:tidyr':\n\n    extract\n\nresult &lt;- FindTopicsNumber(dtm_new,\n                           topics = c(seq(from = 2, to = 9, by = 1), seq(10, 25, 5)),\n                           metrics = c(\"Griffiths2004\", \"CaoJuan2009\", \"Arun2010\", \"Deveaud2014\"),\n                           method = \"Gibbs\",\n                           control = list(seed = 0:4,\n                                          nstart = 5,\n                                          best = TRUE),\n                           mc.cores = 4L,\n                           verbose = TRUE\n                           )\n\nfit models... done.\ncalculate metrics:\n  Griffiths2004... done.\n  CaoJuan2009... done.\n  Arun2010... done.\n  Deveaud2014... done.\n\nFindTopicsNumber_plot(result)\n\nWarning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\nℹ The deprecated feature was likely used in the ldatuning package.\n  Please report the issue at &lt;https://github.com/nikita-moor/ldatuning/issues&gt;.\n\n\n\n\n\n\n\n\n##Etape 2 : comparaison des solutions\npara &lt;- tibble(k = c(6,7,8,9,10))\nlemma_tm &lt;- para %&gt;%\n  mutate(lda = map(k,\n                   function(k) LDA(\n                     k=k,\n                     x=dtm_new,\n                     method=\"Gibbs\",\n                     control=list(seed = 0:4,\n                                  nstart = 5,\n                                  best = TRUE)\n                     )\n                   )\n         )\n\nlemma_tm &lt;- lemma_tm %&gt;%\n  mutate(lda_gamma = map(.x=lda,\n                         .f=tidytext::tidy,\n                         matrix=\"gamma\"))\nlemma_tm %&gt;%\n  unnest(lda_gamma) %&gt;%\n  group_by(k, document) %&gt;%\n  arrange(desc(gamma)) %&gt;%\n  slice(1) %&gt;%\n  ungroup() %&gt;%\n  ggplot(aes(x=gamma, fill=factor(k))) +\n  geom_histogram(bins = 20) +\n  scale_fill_discrete(name = \"Number of\\nTopics\") +\n  xlab(\"maximum gamma per document\") +\n  facet_wrap(~k) +\n  geom_vline(aes(xintercept = 1/k),\n             tibble(k=lemma_tm %$% unique(k)),\n             color=\"darkred\")\n\n\n\n\n\n\n\n\n\n\n6.3.3 Représentation graphique\nÀ partir de la solution retenue aux étapes précédentes, on va représenter les différents topics :\n\nset.seed(1234)     #pour la réplicabilité des résultats\nlda &lt;- LDA(dtm_new, k = 7)\n\nlda_res&lt;-as.data.frame(terms(lda, 25))%&gt;%\n  rename(nom1='Topic 1',nom2='Topic 2', nom3='Topic 3', nom4='Topic 4', nom5='Topic 5',nom6='Topic 6',nom7='Topic 7')%&gt;%\n  mutate(rank=as.numeric(row.names(.)))%&gt;%\n  pivot_longer(-rank, names_to = \"topic\", values_to = \"term\")\n\nggplot(lda_res, aes(x=topic, y= rank, group =  term , label = term)) + \n  scale_y_reverse() +\n  geom_text(aes(color=topic,size=8/log(rank)))+\n  theme_minimal()+\n  scale_color_hue()+\n  guides(color=FALSE,size=FALSE)",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>*Topic Analysis*</span>"
    ]
  },
  {
    "objectID": "chapter6.html#theory-driven-lda",
    "href": "chapter6.html#theory-driven-lda",
    "title": "6  Topic Analysis",
    "section": "6.4 Theory-Driven LDA",
    "text": "6.4 Theory-Driven LDA\nIci, on va forcer les topics grâce à la réalisation d’un dictionnaire. C’est utile quand on cherche à appliquer une théorie qui nous dit ce que l’on cherche à trouver. Par exemple, ici on s’intéresse aux attributs clés des logements oiseaux. Dans d’autre cas, on pourra chercher à expliquer les notes en fonction de topics qui reflètent les attributs clés. On peut réaliser le dictionnaire a priori ou après différentes analyses de topics, de co-occurences, de fréquence, etc.\nOn commence par créer un dictionnaire.\n\ndict&lt;-dictionary(list(produit=c(\"produit*\", \"cage\",\"oiseau\",\"graine*\"),\n                      livraison=c(\"livr*\",\"recepti*\",\"délai\"),\n                      commande=c(\"command*\",\"emballage\",\"envoi*\"),\n                      site=\"*site*\",\n                      prix=c(\"*prix*\",\"frais_port\")\n                      ))\ndict\n\nDictionary object with 5 key entries.\n- [produit]:\n  - produit*, cage, oiseau, graine*\n- [livraison]:\n  - livr*, recepti*, délai\n- [commande]:\n  - command*, emballage, envoi*\n- [site]:\n  - *site*\n- [prix]:\n  - *prix*, frais_port\n\nhead(dfm_lookup(dfm_new,dict))\n\nDocument-feature matrix of: 6 documents, 5 features (76.67% sparse) and 7 docvars.\n    features\ndocs produit livraison commande site prix\n   1       0         0        0    0    0\n   2       0         2        1    0    0\n   3       2         1        0    0    0\n   4       0         0        1    0    0\n   5       0         1        0    0    0\n   6       0         0        0    1    0\n\n\nOn utilise ensuite le package ‘seededlda’ pour lancer le modèle semi-supervisé.\n\nlibrary(seededlda)\n\nLe chargement a nécessité le package : proxyC\n\n\n\nAttachement du package : 'proxyC'\n\n\nL'objet suivant est masqué depuis 'package:stats':\n\n    dist\n\n\n\nAttachement du package : 'seededlda'\n\n\nLes objets suivants sont masqués depuis 'package:topicmodels':\n\n    terms, topics\n\n\nL'objet suivant est masqué depuis 'package:stats':\n\n    terms\n\nset.seed(1234)\nslda&lt;-textmodel_seededlda(dfm_new, dict, residual = T)\nterms(slda,20)\n\n      produit            livraison                  commande            \n [1,] \"oiseau\"           \"livraison\"                \"commander\"         \n [2,] \"cage\"             \"livraison_rapide\"         \"emballage\"         \n [3,] \"graine\"           \"délai\"                    \"envoi\"             \n [4,] \"perroquet\"        \"conforme\"                 \"envoi_rapide\"      \n [5,] \"très\"             \"livraison_très_rapide\"    \"recevoir\"          \n [6,] \"trouver\"          \"dire\"                     \"arriver\"           \n [7,] \"bien\"             \"satisfaire\"               \"bien\"              \n [8,] \"petit\"            \"livraison_temps\"          \"bien_emballer\"     \n [9,] \"plus\"             \"impeccable\"               \"très_rapidement\"   \n[10,] \"produit_qualité\"  \"toujours\"                 \"toujours\"          \n[11,] \"produit_conforme\" \"très_bien\"                \"recommander\"       \n[12,] \"avoir\"            \"livraison_rapide_produit\" \"très_satisfait\"    \n[13,] \"jouet\"            \"correspondre\"             \"commande_livraison\"\n[14,] \"acheter\"          \"top\"                      \"envoi_très_rapide\" \n[15,] \"beaucoup\"         \"service\"                  \"envoie\"            \n[16,] \"harnais\"          \"bien\"                     \"problème\"          \n[17,] \"perruche\"         \"livreur\"                  \"rapidement\"        \n[18,] \"aussi\"            \"livraison_rapidement\"     \"bon_état\"          \n[19,] \"grand\"            \"livraison_délai\"          \"colis\"             \n[20,] \"faire\"            \"attente\"                  \"rapidité\"          \n      site                   prix               other     \n [1,] \"site\"                 \"prix\"             \"pas\"     \n [2,] \"très\"                 \"rapide\"           \"ne\"      \n [3,] \"recommander\"          \"très_bien\"        \"avoir\"   \n [4,] \"très_bon_site\"        \"recommander\"      \"plus\"    \n [5,] \"recommander_site\"     \"super\"            \"ne_pas\"  \n [6,] \"super\"                \"bon\"              \"faire\"   \n [7,] \"rapide\"               \"frais_port\"       \"recevoir\"\n [8,] \"tout\"                 \"excellent\"        \"colis\"   \n [9,] \"siter\"                \"très_rapide\"      \"donc\"    \n[10,] \"site_sérieux\"         \"très_satisfait\"   \"fois\"    \n[11,] \"vraiment\"             \"service\"          \"jour\"    \n[12,] \"recommander_vivement\" \"très_bon_produit\" \"même\"    \n[13,] \"très_sérieux\"         \"très_bon\"         \"envoyer\" \n[14,] \"bon_site\"             \"sérieux\"          \"attendre\"\n[15,] \"super_site\"           \"bon_produit\"      \"autre\"   \n[16,] \"top\"                  \"rapider\"          \"problème\"\n[17,] \"conseiller\"           \"prix_raisonnable\" \"pouvoir\" \n[18,] \"équipe\"               \"dire\"             \"aller\"   \n[19,] \"téléphone\"            \"expédition\"       \"mettre\"  \n[20,] \"service_client\"       \"qualité\"          \"être\"    \n\n\n\n6.4.1 Expliquer les notes en fonction des topics\nMaintenant, nous cherchons à voir la répartition des topics dans les notes, pour comprendre si certains topics contribuent plus ou moins à la satisfaction.\n\ntheta&lt;-as.data.frame(slda$theta)%&gt;%mutate(doc_id=as.numeric(row.names(.)))\n\ndata&lt;-inner_join(data, theta)\n\nJoining with `by = join_by(doc_id)`\n\nfoo&lt;-data%&gt;%select(note, produit, livraison, commande, site,prix, other)%&gt;%\n  pivot_longer(-note, names_to = \"topic\", values_to = \"value\")\n\nggplot(foo,aes(x=note, y=value, group=topic))+\n  geom_bar(position=\"fill\",stat=\"identity\", aes(fill=topic))+\n  scale_fill_brewer(palette=\"Spectral\")+\n  theme_minimal()\n\n\n\n\n\n\n\n#Pour finir, une petite régression !\nfit&lt;-lm(note~produit+livraison+commande+site+prix, data =data)\nsummary(fit)\n\n\nCall:\nlm(formula = note ~ produit + livraison + commande + site + prix, \n    data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9666 -0.0424  0.1259  0.2685  2.0844 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.27633    0.05989   38.01   &lt;2e-16 ***\nproduit      2.29956    0.09840   23.37   &lt;2e-16 ***\nlivraison    2.81963    0.09325   30.24   &lt;2e-16 ***\ncommande     2.76466    0.09576   28.87   &lt;2e-16 ***\nsite         3.10423    0.09272   33.48   &lt;2e-16 ***\nprix         3.07552    0.08923   34.47   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7103 on 4332 degrees of freedom\nMultiple R-squared:  0.2926,    Adjusted R-squared:  0.2917 \nF-statistic: 358.3 on 5 and 4332 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>*Topic Analysis*</span>"
    ]
  },
  {
    "objectID": "chapter6.html#le-modèle-stm",
    "href": "chapter6.html#le-modèle-stm",
    "title": "6  Topic Analysis",
    "section": "6.5 Le modèle STM",
    "text": "6.5 Le modèle STM\nLa Modélisation Thématique Structurelle est un prolongement du modèle LDA développé ci-dessus. Permettant de parvenir aux mêmes types de résultats de regroupements thématiques par plongement lexical, cette dernière se distingue dans le sens où elle permet d’associer d’autres variables, ou méta-données, au corpus traité afin de prendre en compte les relations de leurs modalités au contenu. Ainsi, elle crée la notion de prévalence d’un topic, qui permet de prendre en compte sa fluctuation en fonction de la propre évolution de la covariance des éléments d’un même mélange.\n\n\n\nLe modèle STM\n\n\n\n6.5.1 Choisir le nombre de topics : approche par comparaison\nTout d’abord, on teste plusieurs modèles pour découvrir le nombre de topics optimal. 4 métriques sont calculés : * held-out likelihood : la probabilité d’apparition des mots au sein d’un document après avoir enlevé ces mots du document lors de l’étape d’estimation (à maximiser) ; * residuals : test de la sur-dispersion des résidus ; si les résidus sont sur-dispersés, il est possible qu’il existe une meilleure solution à plus de topics (à minimiser) ; * semantic coherence : la cohérence sémantique est maximisée quand les mots les plus probables d’un topic apparaissent fréquemment ensemble (o-occurrences) (à maximiser) ; * lower bound : permet de suivre la convergence du modèle (à minimiser).\nOn lance l’estimation sur 3 modèles différents, puis on compare les métriques.\n\ndfm_stm &lt;- convert(dfm_new, to = \"stm\")\n\nWarning in dfm2stm(x, docvars, omit_empty = TRUE): Dropped 4,338 empty\ndocument(s)\n\nlibrary(stm)\n\nstm v1.3.7 successfully loaded. See ?stm for help. \n Papers, resources, and other materials at structuraltopicmodel.com\n\n#test de différents modèles\nkresult2 &lt;- searchK(dfm_stm$documents, dfm_stm$vocab, K = c(7,10,25), prevalence =~ s(note), data = dfm_stm$meta,verbose = FALSE)\n\nWarning in splines::bs(x, df, ...): all interior knots match right boundary\nknot\n\n\nWarning in splines::bs(x, df, ...): all interior knots match right boundary\nknot\nWarning in splines::bs(x, df, ...): all interior knots match right boundary\nknot\n\n#afficher les paramètres calculés\nplot(kresult2)\n\n\n\n\n\n\n\n\n\n\n6.5.2 Le modèle final\nUne fois le nombre de topics choisi, il ne reste plus qu’à lancer le modèle et interpréter ses résultats. L’interprétation passe par 3 étapes: * interpréter les topics, * regarder la prévalence des topics en fonction des variables de prévalence retenues, * analyser la corrélation entre les topics\nNous testons ici une solution à 10 topics. Tout d’abord, nous lançons le modèle et nous regardons la distribution des topics en fonction de leur cohérence sémantique et de leur exclusivité (à partir de la mesure FREX).\n\n#nb de topics retenu\nk=10\n# la spécification du modèle\nset.seed(2020)\nmodel.stm &lt;- stm(dfm_stm$documents, \n                 dfm_stm$vocab, \n                 K = k, max.em.its = 25,\n                 data = dfm_stm$meta, \n                 init.type = \"Spectral\", \n                 prevalence =~ note,\n                 interactions = FALSE,\n                 verbose = FALSE) # this is the actual stm call\n\n\n#la qualité des topic\n\ntopicQuality(model.stm , dfm_stm$documents, xlab = \"Semantic Coherence\",  ylab = \"Exclusivity\", M = k)\n\n [1] -189.2462 -189.5355 -205.9539 -170.8214 -186.9341 -163.6631 -196.4798\n [8] -249.0770 -211.9196 -194.3200\n [1] 9.835226 9.789158 9.784484 9.802718 9.759080 9.671072 9.790776 9.800144\n [9] 9.668600 9.777661\n\n\n\n\n\n\n\n\n\n\n\n6.5.3 Interpréter les topics\nLa première étape de l’exploration consiste à analyser les mots les plus associés à chaque topics. Il existe 4 métriques d’association différentes : * Highest Prob : les mots avec la plus forte probabilité ; * FREX : pondération des mots selon leur fréquence d’apparition globale et leur appartenance à des documents spécifiques ; * Lift : pondération par l’occurrence des mots dans les autres topics, ce qui donne plus d’importance aux mots spécifiques au topic examiné ; * Score : transformation de Lift par le log.\nIci, on affiche les résultats pour les topics 1, 5 et 10 et on représente les différentes métriques graphiquement :\n\nlabelTopics(model.stm, c(1,5,10))\n\nTopic 1 Top Words:\n     Highest Prob: site, super, top, petit, tout, très_bon, rapidité \n     FREX: site, satisfait, équipe, rapidité, très_bon, livraison_rapidement, correct \n     Lift: réaction, livraison_très_vite, envoi_rapide_produit_qualité, féliciter, livraison_rapidement, livraison_rapide_recommander, situation \n     Score: site, super, top, rapidité, petit, très_bon, satisfait \nTopic 5 Top Words:\n     Highest Prob: dire, vraiment, envoyer, recommander_site, magasin, grand, choix \n     FREX: dire, envoyer, vraiment, boutique, choix, envoi_rapide_soigné, point \n     Lift: amie, écossais, hybride, pack, terme, très_bon_prestation, front \n     Score: dire, envoyer, vraiment, recommander_site, choix, magasin, grand \nTopic 10 Top Words:\n     Highest Prob: oiseau, pas, plus, très_satisfait, problème, excellent, arriver \n     FREX: très_satisfait, efficace, excellent, très_bon_qualité, demander, coli, envoie \n     Lift: fou, étrangler, nid, très_satisfait, ador, beaucoup_choix_produit, livraison_rapide_colis_bien_emballer \n     Score: oiseau, plus, pas, très_satisfait, excellent, problème, arriver \n\nplot(model.stm, type = \"summary\", labeltype=\"prob\",text.cex = 0.7,n=7)\n\n\n\n\n\n\n\nplot(model.stm, type = \"summary\", labeltype=\"score\",text.cex = 0.7,n=5)\n\n\n\n\n\n\n\nplot(model.stm, type = \"summary\", labeltype=\"lift\",text.cex = 0.7,n=5)\n\n\n\n\n\n\n\nplot(model.stm, type = \"summary\", labeltype=\"frex\",text.cex = 0.7,n=5)\n\n\n\n\n\n\n\n\nOn peut également représenter les topics sous forme de nuage de mots. La première proposition, ci-dessous en bleu, repose sur la probabilité d’appartenance des termes aux topics, la seconde, en rouge, représente les mots en fonction de l’occurrence des topics dans les documents.\n\npar(mfrow = c(3,5) , mar = c(0,0,0,0))\nfor (i in seq_along((1:k)))\n{\n  cloud(model.stm,scale=c(2,.25) ,topic = i, type =\"model\", max.words = 50, colors=\"darkblue\", random.order=FALSE, )\n  text(x=0.5, y=1, paste0(\"topic\",i))\n}\n\nWarning in wordcloud::wordcloud(words = vocab, freq = vec, max.words =\nmax.words, : recommander could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud::wordcloud(words = vocab, freq = vec, max.words =\nmax.words, : très_bien could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud::wordcloud(words = vocab, freq = vec, max.words =\nmax.words, : livraison_rapide could not be fit on page. It will not be plotted.\n\npar(mfrow = c(3,5) , mar = c(0,0,0,0))\n\n\n\n\n\n\n\nfor (i in seq_along((1:k)))\n{\ncloud(model.stm, topic = i,scale=c(2,.25) ,type = c(\"model\",\"documents\"), dfm,thresh = 0.1, max.words = 50, colors=\"firebrick\")\n   text(x=0.5, y=1, paste0(\"topic\",i))\n}\n\nWarning in wordcloud::wordcloud(words = vocab, freq = vec, max.words =\nmax.words, : recommander could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud::wordcloud(words = vocab, freq = vec, max.words =\nmax.words, : service could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud::wordcloud(words = vocab, freq = vec, max.words =\nmax.words, : livraison_rapide could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud::wordcloud(words = vocab, freq = vec, max.words =\nmax.words, : oiseau could not be fit on page. It will not be plotted.\n\n\n\n\n\n\n\n\n\nOn peut aussi s’intéresser à représenter les contrastes dans les topics :\n\nplot(model.stm, type=\"perspectives\", topics=c(5,10))\n\n\n\n\n\n\n\n\nEnfin, on peut s’intéresser aux documents qui contribuent aux topics, à travers la fonction ‘findThoughts’, et à les représenter graphiquement. Pour cela, on a besoin d’un vecteur contenant les textes, dans le même ordre que celui fournit au modèle STM. Ici, on utilise le vecteur ‘title’ créé dans les premières manipulations des données.\n\ndata_stm&lt;-data%&gt;%filter(text_id%in%dfm_stm$meta$text_id)\n\n\nthoughts3 &lt;- findThoughts(model.stm,texts=data_stm$comments ,n = 4,\n topics = 3)$docs[[1]]\nthoughts10 &lt;- findThoughts(model.stm, texts = data_stm$comments, n = 3,\ntopics = 10)$docs[[1]]\n\n\n\npar(mfrow = c(1, 2), mar = c(0.5, 0.5, 1, 0.5))\nplotQuote(thoughts3, width = 50,text.cex = 0.8, main = \"Topic 3\")\nplotQuote(thoughts10, width = 50,text.cex = 0.8, main = \"Topic 10\")\n\n\n\n\n\n\n\n\n\n\n6.5.4 Evolution en fonction de la variable de prévalence\nIci, on représente l’évolution de la présence des topics en fonction de notre variable de prévalence, l’année de parution. D’abord, on calcule l’effet de l’année pour chaque topic, et ensuite on fait notre représentation graphique.\n\nmodel.stm.labels &lt;- labelTopics(model.stm, 1:k) #on récupère la description des topics pour les titres des graphiques\ndfm_stm$meta$datum &lt;- as.numeric(dfm_stm$meta$note) #on transforme l'année en variable continue\nmodel.stm.ee &lt;- estimateEffect(1:k ~ s(`datum`), model.stm, meta = dfm_stm$meta) #on estime l'effet de l'année pour chaque topic\n\nWarning in splines::bs(x, df, ...): all interior knots match right boundary\nknot\nWarning in splines::bs(x, df, ...): all interior knots match right boundary\nknot\n\n\nWarning in estimateEffect(1:k ~ s(datum), model.stm, meta = dfm_stm$meta): Covariate matrix is singular.  See the details of ?estimateEffect() for some common causes.\n             Adding a small prior 1e-5 for numerical stability.\n\nsummary(model.stm.ee,topics=3) #résultat pour le topic 13\n\n\nCall:\nestimateEffect(formula = 1:k ~ s(datum), stmobj = model.stm, \n    metadata = dfm_stm$meta)\n\n\nTopic 3:\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.122366   0.008090  15.126  &lt; 2e-16 ***\ns(datum)1    0.001979   0.026834   0.074    0.941    \ns(datum)2   -0.021155   0.014258  -1.484    0.138    \ns(datum)3   -0.032226   0.008235  -3.913 9.25e-05 ***\ns(datum)4   -0.017980  19.940092  -0.001    0.999    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#représentation graphique\npar(mfrow = c(3,5) , mar = c(1,0,2,0))\nfor (i in seq_along((1:k)))\n{\n  plot(model.stm.ee, \"datum\", method = \"continuous\", topics = i, main = paste0(model.stm.labels$score[i,1:4], collapse = \"-\"), printlegend = T)\n}\n\nWarning in splines::bs(x, df, ...): all interior knots match right boundary\nknot\n\n\nWarning in splines::bs(x, df, ...): all interior knots match right boundary\nknot\nWarning in splines::bs(x, df, ...): all interior knots match right boundary\nknot\nWarning in splines::bs(x, df, ...): all interior knots match right boundary\nknot\nWarning in splines::bs(x, df, ...): all interior knots match right boundary\nknot\nWarning in splines::bs(x, df, ...): all interior knots match right boundary\nknot\nWarning in splines::bs(x, df, ...): all interior knots match right boundary\nknot\nWarning in splines::bs(x, df, ...): all interior knots match right boundary\nknot\nWarning in splines::bs(x, df, ...): all interior knots match right boundary\nknot\nWarning in splines::bs(x, df, ...): all interior knots match right boundary\nknot\n\n\n\n\n\n\n\n\n\n\n\n6.5.5 Corrélation entre les topics\nPour finir, on s’intéresse à la corrélation entre les topics.\n\nlibrary(reshape2)\n\n\nAttachement du package : 'reshape2'\n\n\nL'objet suivant est masqué depuis 'package:tidyr':\n\n    smiths\n\n#On crée les titres/noms des topics\nb&lt;-NULL\nfor (i in seq_along((1:k)))\n{\n  a&lt;-paste0(model.stm.labels$score[i,1:3], collapse = \" \")\n  a&lt;-paste(\"Topic\",i,a)\nb&lt;-rbind(b,a)\n}\nlabel&lt;-as.data.frame(b)\nlabel\n\n                                           V1\na                      Topic 1 site super top\na.1           Topic 2 recommander colis jouet\na.2 Topic 3 livraison perroquet bien_emballer\na.3       Topic 4 commander très_bien qualité\na.4             Topic 5 dire envoyer vraiment\na.5         Topic 6 recevoir faire satisfaire\na.6              Topic 7 bien article service\na.7             Topic 8 avoir sérieux trouver\na.8      Topic 9 livraison_rapide rapide très\na.9                  Topic 10 oiseau plus pas\n\ntopicor&lt;-topicCorr(model.stm, method = \"simple\",verbose = TRUE) # calcul des corrélations\nadjmatrix &lt;-topicor[[3]] #on récupère les corrélations\n\n#calcul des theta moyens par topic\ntheta &lt;-model.stm[[7]]\nthetat&lt;-melt(theta)\nthetat&lt;-thetat %&gt;%group_by(Var2)%&gt;%summarise(mean=mean(value))\n\nlibrary(igraph)\n\n\nAttachement du package : 'igraph'\n\n\nL'objet suivant est masqué depuis 'package:seededlda':\n\n    sizes\n\n\nL'objet suivant est masqué depuis 'package:quanteda.textplots':\n\n    as.igraph\n\n\nLes objets suivants sont masqués depuis 'package:lubridate':\n\n    %--%, union\n\n\nLes objets suivants sont masqués depuis 'package:dplyr':\n\n    as_data_frame, groups, union\n\n\nLes objets suivants sont masqués depuis 'package:purrr':\n\n    compose, simplify\n\n\nL'objet suivant est masqué depuis 'package:tidyr':\n\n    crossing\n\n\nL'objet suivant est masqué depuis 'package:tibble':\n\n    as_data_frame\n\n\nLes objets suivants sont masqués depuis 'package:stats':\n\n    decompose, spectrum\n\n\nL'objet suivant est masqué depuis 'package:base':\n\n    union\n\n#création du graphe des corrélations\ng&lt;-graph_from_adjacency_matrix(adjmatrix, mode = \"lower\", weighted = TRUE, diag = FALSE)\ng &lt;- delete.edges(g, E(g)[ abs(weight) &lt; 0.05])\n\nWarning: `delete.edges()` was deprecated in igraph 2.0.0.\nℹ Please use `delete_edges()` instead.\n\ncurve_multiple(g)\n\n [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[39] 0\n\nset.seed(2021)\nplot(g,\n     # layout=layout_with_fr,  \n     margin = c(0, 0, 0, 0),\n     edge.width=abs(E(g)$weight)*15,\n     edge.color=ifelse(E(g)$weight &gt; 0, \"grey60\",\"red\"),\n     vertex.label=label$V1,\n     vertex.color = adjustcolor(\"pink2\", alpha.f = .2),vertex.label.cex=0.7, vertex.size=400*thetat$mean, vertex.frame.color= \"white\"\n     )\n\nWarning in v(graph): Non-positive edge weight found, ignoring all weights\nduring graph layout.\n\n\n\n\n\n\n\n\n\nEt en bonus, un petit graphique pour représenter les mots les plus contributifs aux topics :\n\nlibrary(broom)\ntd_beta &lt;- tidy(model.stm,log=FALSE)\n\ntd_beta&lt;-td_beta %&gt;% \n  mutate(topic=as.factor(topic))%&gt;%\n  mutate(topic=`levels&lt;-.factor`(td_beta$topic,label$V1))\n\ntd_beta %&gt;%\n    group_by(topic) %&gt;%\n    top_n(15, beta) %&gt;%\n    ungroup() %&gt;%\n    ggplot(aes(reorder(term,beta), beta)) +\n    geom_col(fill=\"firebrick\") +theme_minimal()+\n    facet_wrap(~ topic, scales = \"free\") + labs(x=NULL)+\n    coord_flip()",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>*Topic Analysis*</span>"
    ]
  },
  {
    "objectID": "chapter7.html",
    "href": "chapter7.html",
    "title": "7  Vectorisation",
    "section": "",
    "text": "7.1 Introduction\nC’est sans doute l’idée la plus novatrice que l’approche computationnelle du langage a apporté ces 10 dernières années. Le modèle word2vec de Mikolov(2013) en est une première version, d’autres ont apporté des amélioration comme le modèle Glove.\nL’idée fondamentale est qu’on peut représenter des mots dans un espace de grande dimension par des vecteurs. Ce qui importe c’est de conserver la relation entre mots dans cet espace. Deux mots très corrélés, au sens de leur cooccurences, doivent l’être avec la même intensité dans cet espace. Admettant que le cosinus de l’angle entre deux vecteurs est équivalent à leur corrélation, on comprend aisément que la vectorisation consiste à identifier un jeu de coordonnées, les paramètres des vecteurs mots, en connaissant les angles qu’is forment entre eux.\nPour estimer les coordonnée des vecteurs deux méthodes peuvent être employée simultanéement.\nL’idée de plongement lexical tient alors dans cette dynamique double d’identification et de rattachament des éléments textuels ensembles, selon différentes méthodes de vraisemblance/mesure.\nLe caractère remarquable de la méthode c’est qu’il est posible d’opérer des opérations algébriques, l’exemple canonique est celui de : reine = Roi+Homme - Femme\nPour la mise en oeuvre on emploie le package WordVectors de BenJamin Schmidt.",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectorisation</span>"
    ]
  },
  {
    "objectID": "chapter7.html#introduction",
    "href": "chapter7.html#introduction",
    "title": "7  Vectorisation",
    "section": "",
    "text": "Les mots observés, dont on peut prédire le contexte (Skip-gram)\nLes éléments du contexte observés, dont on peut prédire le mot (CBOW)\n\n\n\n\n\nWord Embeddings",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectorisation</span>"
    ]
  },
  {
    "objectID": "chapter7.html#les-données",
    "href": "chapter7.html#les-données",
    "title": "7  Vectorisation",
    "section": "7.2 Les données",
    "text": "7.2 Les données\nOn repart du vocabulaire préparé au chapitre 6. On lemmatise, on ne garde que les mots signifiants. On créera les n-gramms directement dans la vectorisation. On sauvegarde le tout en format .txt pour pouvoir ensuite l’injecter dans le modèle.\n\ndata &lt;- read_csv(\"data/data_trustpilot_oiseaux.csv\")\n\nRows: 4388 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): auteur, date, month, comments\ndbl (3): id, year, note\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata&lt;-data%&gt;%mutate(text_id=paste0(\"text_\", row_number(data$id)))\n\nann_token&lt;-read_rds(\"data/annotation_oiseaux.rds\")\n\n\ndata&lt;-ann_token%&gt;%\n  filter(upos==\"NOUN\"|upos==\"VERB\"|upos==\"ADJ\"|upos==\"ADV\")%&gt;%\n  group_by(doc_id)%&gt;%\n  summarise(text=paste(lemma,collapse = \" \"))%&gt;%\n  inner_join(data, join_by(\"doc_id\"==\"id\"))\n\ncorpus_new&lt;-corpus(data, text_field = \"text\")\ntoks&lt;-tokens(corpus_new)%&gt;%\n  tokens_replace(c(\"produire\", \"conformer\",\"colir\", \"tre\", \"livrer\", \"n\"), c(\"produit\", \"conforme\",\"colis\", \"très\", \"livraison\", \"ne\"))%&gt;%\n  tokens_remove(c(\".\",\",\"))\n\nfoo1&lt;-data.frame(\n  id = seq_along(toks),\n  text = sapply(toks, paste, collapse = \" \"),\n  row.names = NULL\n)\n\n#on génère le fichier de ces textes \"purifiés\"\nwrite.table(foo1, file=\"data/textes.txt\")",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectorisation</span>"
    ]
  },
  {
    "objectID": "chapter7.html#vectoriser-le-texte",
    "href": "chapter7.html#vectoriser-le-texte",
    "title": "7  Vectorisation",
    "section": "7.3 Vectoriser le texte",
    "text": "7.3 Vectoriser le texte\nOn commence par préparer le texte pour l’algorithme, avec le package ‘wordVectors’.\n\n# install.packages(\"remotes\")\n# remotes::install_github(\"bmschmidt/wordVectors\")\nlibrary(wordVectors)\n\n#Nettoyage des tweets et identification des n-grammes en vue d'entraîner le modèle\nprep_word2vec(origin=\"data/textes.txt\",destination=\"data/textes_vec.txt\",lowercase=T,bundle_ngrams=3)\n\nBeginning tokenization to text file at data/textes_vec.txt\n\n\nPrepping data/textes.txt\n\n\nStarting training using file data/textes_vec.txt\n\nVocab size (unigrams + bigrams): 21443\nWords in train file: 50975\nStarting training using file data/textes_vec.txt_\nWords processed: 100K     Vocab size: 40K  \nVocab size (unigrams + bigrams): 21490\nWords in train file: 101482\n\n\nOn entraîne ensuite le modèle.\n\n#Création et entraînement du modèle vectoriel\n\nmodel = train_word2vec(\"data/textes_vec.txt\",\n                       \"data/textes.bin\",\n                       vectors=200,threads=3,\n                       window=5,\n                       iter=10,negative_samples=0,\n                       force=TRUE, \n                       min_count=30)\n\nStarting training using file D:/NLP_ED/data/textes_vec.txt\nVocab size: 192\nWords in train file: 29198\n\n\nFilename ends with .bin, so reading in binary format\n\n\nReading a word2vec binary file of 192 rows and 200 columns\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |=========                                                             |  14%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |============================================                          |  64%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |===============================================================       |  91%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n\n\nNous avons un vocabulaire de 192 pour 29 198 mots dans le fichier d’entraînement. Il se présente sous la forme d’un tableau de 192 lignes et 200 colonnes.",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectorisation</span>"
    ]
  },
  {
    "objectID": "chapter7.html#exploiter-les-résultats",
    "href": "chapter7.html#exploiter-les-résultats",
    "title": "7  Vectorisation",
    "section": "7.4 Exploiter les résultats",
    "text": "7.4 Exploiter les résultats\nPour exploiter cette représentation, une première manière de faire est de rechercher dans le corpus les termes les plus associés à un terme cible. Quel est son contexte le plus proche ? Des fonctions pratiques sont proposées dans le package. la principale closest_to qui permet de selectionner les termes les plus proches, en termes de cosinus, du vecteur cible.\nDans l’exemple suivant, on cherche à mieux saisir le concept de “livraison”. On examine les trentes termes les plus proches.\n\nfoo&lt;-model %&gt;% \n  closest_to(~\"livraison\",31)%&gt;%\n  filter(word!=\"livraison\")%&gt;%#on choisit les 30 termes les plus proches, sauf livraison\n  rename(similarity=2)\n\nggplot(foo, aes(x=reorder(word,similarity),y=similarity))+\n  geom_point(col=\"black\",size=3)+\n  coord_flip()+\n  ggtitle(\"N-grammes similaires à livraison\")\n\n\n\n\n\n\n\n\nExercice : répéter l’opération sur “perroquet”\n\n\nSolution\nfoo&lt;-model %&gt;% \n  closest_to(~\"perroquet\",31)%&gt;%\n  filter(word!=\"perroquet\")%&gt;%#on choisit les 30 termes les plus proches, sauf livraison\n  rename(similarity=2)\n\nggplot(foo, aes(x=reorder(word,similarity),y=similarity))+\n  geom_point(col=\"black\",size=3)+\n  coord_flip()+\n  ggtitle(\"N-grammes similaires à perroquet\")\n\n\n\n\n\n\n\n\n\nOn peut affiner les concepts en faisant la somme de plusieurs mots.\n\nfoo&lt;-model %&gt;% \n  wordVectors::closest_to(~(\"perroquet\"+\"perruche\"),32)%&gt;%\n  filter(word!=\"perroquet\"&word!=\"perruche\")%&gt;%#on choisit les 30 termes les plus proches, sauf livraison\n  rename(similarity=2)\n\nggplot(foo, aes(x=reorder(word,similarity),y=similarity))+\n  geom_point(col=\"black\",size=3)+\n  coord_flip()+\n  ggtitle(\"N-grammes similaires à perroquet\")\n\n\n\n\n\n\n\n\nEt on peut également soustraire des concepts les uns aux autres.\n\nfoo&lt;-model %&gt;% \n  wordVectors::closest_to(~(\"problème\"-\"livraison\"),32)%&gt;%\n  rename(similarity=2)\n\nggplot(foo, aes(x=reorder(word,similarity),y=similarity))+\n  geom_point(col=\"black\",size=3)+\n  coord_flip()+\n  ggtitle(\"N-grammes similaires à problème, sans la livraison\")",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectorisation</span>"
    ]
  },
  {
    "objectID": "chapter7.html#créer-des-groupes",
    "href": "chapter7.html#créer-des-groupes",
    "title": "7  Vectorisation",
    "section": "7.5 Créer des groupes",
    "text": "7.5 Créer des groupes\nOn peut aussi créer des groupes. On verra la méthode classique de clustering et la méthode t-sne.\n\n7.5.1 Clustering\n\nq_words = c(\"livraison\", \"problème\")\nterm_set = lapply(q_words, \n                  function(q_word) {\n                    nearest_words = model %&gt;% closest_to(model[[q_word]],80)\n                    nearest_words$word\n                  }) %&gt;% unlist\nsubset = model[[term_set,average=F]]\n\nsubset1&lt;-as.data.frame(subset@.Data)\n\n# un calcul de dissimilarité sur la base des cosinus\n#la fonction habituel dist ne le permetpas\nMatrix &lt;- as.matrix(subset1)\nsim &lt;- Matrix / sqrt(rowSums(Matrix * Matrix))\nsim &lt;- sim %*% t(sim)\n#on transforme en distance la similarité cosinus, celle ci varie de 0 à 2.\nD_sim &lt;- as.dist(1 - sim)\n\n\n#un clustering hiérarchique avec 10 groupes\n\nclus&lt;-hclust(D_sim)\ngroupes&lt;- cutree(clus,k=10)\nlibrary(ggdendro)\nggdendrogram(clus, rotate=TRUE ,type = \"triangle\")\n\n\n\n\n\n\n\nddata &lt;- dendro_data(clus, type = \"triangle\")\nggplot(segment(ddata)) + \n  geom_segment(aes(x = x, y = y, xend = xend, yend = yend)) + \n  coord_flip()+\n    geom_text(data = ddata$labels, \n              aes(x = x, y = y, label = label), size = 2, vjust = 0)\n\n\n\n\n\n\n\n\n\n\n7.5.2 t-sne\n\nlibrary(Rtsne)\nlibrary(RColorBrewer)\n# run Rtsne with default parameters\nset.seed(57)\nrtsne_out &lt;- Rtsne(as.matrix(subset), perplexity=25)\n# plot the output of Rtsne\n#jpeg(\"fig.jpg\", width=2400, height=1800)\ncolor.vec = c(\"#556270\", \"#4ECDC4\", \"#1B676B\", \"#FF6B6B\", \"#C44D58\", \"seagreen1\", \"seagreen4\", \"slateblue4\", \"firebrick\", \"Royalblue\")\n\n#des manip pour associer les groupe du clustering aux termes et à la leur coordonnée dans tsne.\ngroupes&lt;-as.data.frame(groupes)\ngroupes$word&lt;-rownames(groupes)\nterms&lt;-as.data.frame(rownames(subset))\nterms$word&lt;-terms[,1] \nterms&lt;-terms %&gt;% left_join(groupes, by = \"word\")\nplot(rtsne_out$Y, t='n')\n#count(terms, clus)$n[2]\ntext(rtsne_out$Y, labels=rownames(subset),cex=0.8,col=color.vec[terms$groupes])",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectorisation</span>"
    ]
  },
  {
    "objectID": "chapter8.html",
    "href": "chapter8.html",
    "title": "8  LLM",
    "section": "",
    "text": "8.1 Introduction\nPour finir ce cours, on va s’intéresser aux derniers développements du NLP, les LLMs (Large Langage Models), fondés sur une architecture de Transformers et du mécanisme de l’attention.\nL’idée est d’entraîner un modèle de langue, sur de très larges corpus, pour ensuite l’utiliser dans des tâches spécifiques.\nIl ne s’agit pas ici de réentraîner des modèles, ce qui est une approche tout à fait pertinente lorsque l’on est face à des corpus au langage spécifique, mais d’utiliser les outils existants directement disponibles.\nPour trouver le bon modèle à utiliser, il existe Hugging Face. La plupart des éléments de code sont en python, mais il commence à exister des implémentations en R.\nLe problème véritable repose sur les temps de calcul et la puissance disponible. On recommande d’avoir accès à un GPU, ce qui permet de considérablement raccourcir le temps des traitements. Les outils en R n’ont pas encore implémenté le recours au GPU, et nos ordis portables n’en ont pas forcément. On peut avoir recours au cluster de calcul de son université, ou utiliser les service de Google ou OpenAI (souvent, moyennant finance).\nOn va utiliser du code en python et du code en R pour analyser les résultats. On travaille sur un tout petit corpus, pour limiter les temps de calcul. On pourra garder un oeil sur le package R ‘text’ qui propose beaucoup d’outils mais est encore en construction.\n(Pour redémarrer la session R : command/ctrl + shift + F10)",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>LLM</span>"
    ]
  },
  {
    "objectID": "chapter8.html#introduction",
    "href": "chapter8.html#introduction",
    "title": "8  LLM",
    "section": "",
    "text": "transformers\n\n\n\n\n\n\nExemples",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>LLM</span>"
    ]
  },
  {
    "objectID": "chapter8.html#bertopic",
    "href": "chapter8.html#bertopic",
    "title": "8  LLM",
    "section": "8.2 BERTopic",
    "text": "8.2 BERTopic\nUn classique du genre : BERTopic\n\nfrom bertopic import BERTopic\n\nWARNING:tensorflow:From C:\\Users\\Super\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n\nfrom bertopic.representation import KeyBERTInspired\n\nimport pandas as pd\n\ndf = r.data\nprint(df)\n\n          id  ...                                           comments\n0        1.0  ...  Comme toujours, super service.\\nNe changez rie...\n1        2.0  ...  Le délai de ma commande super rapide Le délais...\n2        3.0  ...  Produits de qualité et équipe professionnelle ...\n3        4.0  ...  Envoi rapide, bien emballé et conforme à l'ann...\n4        5.0  ...  Expédition internationale ! J'ai récemment dém...\n...      ...  ...                                                ...\n4383  4384.0  ...  Très rapide, et arrivé en bon état Envois très...\n4384  4385.0  ...  Satisfaite Très bon produit et respect des dél...\n4385  4386.0  ...  efficacité et rapidité Très heureuse d'avoir r...\n4386  4387.0  ...  Oui mais Le produit acheté est très bien, la l...\n4387  4388.0  ...  Choix et rapidité Du choix proposé sur le site...\n\n[4388 rows x 7 columns]\n\ntopic_model = BERTopic(language=\"multilingual\")\ntopics, probs = topic_model.fit_transform(df['comments'])\ntopic_model.get_topic_info()\n\n    Topic  ...                                Representative_Docs\n0      -1  ...  [Très très bien très satisfaite des produits e...\n1       0  ...  [Très bon site pour oiseaux, Très satisfait me...\n2       1  ...  [Je recommande  ce site, Je recommande ce site...\n3       2  ...   [Service rapide, Service rapide, Service rapide]\n4       3  ...  [Ma commande n a pas pu être honorée dans les ...\n..    ...  ...                                                ...\n56     55  ...  [Produit conforme et livraison rapide, Utilisé...\n57     56  ...  [Je suis jamais déçue la livraison se passe to...\n58     57  ...  [Excellent - tout s'est très bien passé, je re...\n59     58  ...  [Délais court, emballage correct. Commande reç...\n60     59  ...  [Livraison rapide! Tout bon! Je recommande., J...\n\n[61 rows x 5 columns]\n\ntopic_model.get_topic(8)\n\n[('articles', 0.10211028465762363), ('article', 0.060884242655030674), ('description', 0.021620093578803835), ('correspondant', 0.019784195046463393), ('des', 0.018246064088917685), ('rapidement', 0.018099480469657134), ('conformes', 0.017285211053008398), ('mes', 0.015389113267492663), ('les', 0.014979100322801916), ('et', 0.014849319284039132)]\n\ntopic_model.get_topic_freq().head()\n\n    Topic  Count\n5      -1   1248\n2       0    432\n4       1    358\n12      2    191\n1       3    116\n\ntopic_model.get_document_info(df['comments'])\n\n                                               Document  ...  Representative_document\n0     Comme toujours, super service.\\nNe changez rie...  ...                    False\n1     Le délai de ma commande super rapide Le délais...  ...                    False\n2     Produits de qualité et équipe professionnelle ...  ...                    False\n3     Envoi rapide, bien emballé et conforme à l'ann...  ...                    False\n4     Expédition internationale ! J'ai récemment dém...  ...                    False\n...                                                 ...  ...                      ...\n4383  Très rapide, et arrivé en bon état Envois très...  ...                    False\n4384  Satisfaite Très bon produit et respect des dél...  ...                    False\n4385  efficacité et rapidité Très heureuse d'avoir r...  ...                    False\n4386  Oui mais Le produit acheté est très bien, la l...  ...                    False\n4387  Choix et rapidité Du choix proposé sur le site...  ...                    False\n\n[4388 rows x 8 columns]\n\ntopic_model.find_topics(\"réclamation\")\n\n([16, 42, 43, 22, 9], [0.5475961, 0.50152946, 0.4991401, 0.4750504, 0.47463918])\n\ntopic_model.generate_topic_labels()\n\n['-1_et_très_de', '0_oiseaux_mania_pour', '1_site_ce_recommande', '2_service_client_bon', '3_ai_le_pas', '4_livraison_rapide_très', '5_livraison_produit_rapide', '6_ma_commande_merci', '7_rien_dire_redire', '8_articles_article_description', '9_merci_vos_bien', '10_recommande_je_recommander', '11_frais_port_peu', '12_jouets_pour_les', '13_rapidement_commande_reçue', '14_envoi_envoie_rapide', '15_qualité_livraison_produits', '16_tres_qualite_bon', '17_satisfait_satisfaite_très', '18_graines_les_berries', '19_livraison_rapide_conforme', '20_ma_commande_client', '21_était_le_sac', '22_parfait_rapide_tres', '23_téléphone_téléphonique_au', '24_efficace_efficacité_rapide', '25_commande_rapidement_arrivée', '26_produit_bon_excellent', '27_produits_bon_bons', '28_parfait_tout_simplement', '29_livraison_rapide_art', '30_prix_choix_produits', '31_expédition_expedition_rapide', '32_achat_transaction_mon', '33_commande_expédition_traitement', '34_produits_bons_titi', '35_magasin_vendeur_prix', '36_délais_temps_dans', '37_nickel_dire_adresse', '38_trop_long_longue', '39_attentes_mes_attente', '40_sérieux_réactif_rapidité', '41_comme_habitude_toujours', '42_tres_produit_bon', '43_prestation_bien_branche', '44_recommande_produit_bon', '45_pas_deux_ampoule', '46_arrivé_état_rapidement', '47_animaux_vétérinaire_perruche', '48_ras_respecté_satisfaction', '49_site_livraison_ce', '50_dans_04_commande', '51_cage_élégante_une', '52_recommande_soignée_bcp', '53_jours_reçue_moins', '54_envoi_informations_recommande', '55_conforme_description_produit', '56_satisfait_délai_suis', '57_passé_tout_vivement', '58_emballage_serrai_mol', '59_recommande_établissement_intéressés']\n\ntopic_model.visualize_topics()\n\n                        \n                                            \n\ntopic_model.visualize_heatmap()",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>LLM</span>"
    ]
  },
  {
    "objectID": "chapter8.html#zero-shot-classification",
    "href": "chapter8.html#zero-shot-classification",
    "title": "8  LLM",
    "section": "8.3 Zero-Shot Classification",
    "text": "8.3 Zero-Shot Classification\nOn utilise python pour faire la classification (ici, les sentiments) :\n\nimport torch\nprint(torch.__version__)\n\n2.3.0+cu118\n\nprint(torch.cuda.is_available())\n\nTrue\n\ntorch.zeros(1).cuda()\n\ntensor([0.], device='cuda:0')\n\n\nfrom transformers import pipeline\nclassifier = pipeline(\"zero-shot-classification\", model=\"mtheo/camembert-base-xnli\", device=-1)\n\n# load in pandas and numpy for data manipulation\nimport pandas as pd\nimport numpy as np\n\n\ndata = pd.read_csv(\"data/data_trustpilot_oiseaux.csv\")\n\ndf=pd.DataFrame(data.iloc[10:20])\n\n\nlabels = [\"positif\", \"négatif\", \"neutre\"]\n\n# Fonction pour prédire la classification zero-shot pour un texte donné\ndef predict_sentiment_for_text(text, text_labels):\n    result = classifier(text, text_labels, multi_label=True)\n    label_scores = {label: score for label, score in zip(result['labels'], result['scores'])}\n    return label_scores\n\n# Appliquer la fonction à la colonne de texte et stocker les résultats dans une nouvelle colonne\n\ndf['sentiment_results'] = df.apply(lambda row: {'id': row['id'], **predict_sentiment_for_text(row['comments'], labels)}, axis=1)\n# Convertir les résultats de dictionnaire en colonnes séparées\nresults_df = pd.json_normalize(df['sentiment_results'])\n\nfinal_df = pd.merge(df, results_df, on='id')\n\nfinal_df.to_csv('data/test_ZS_result.csv')\n\nOn récupère les résultats dans R et on regarde ce que ça donne.\n\ndf&lt;-py$final_df\n\ndf%&gt;%select(positif, négatif, neutre)%&gt;%pivot_longer(everything())%&gt;%\n  ggplot()+\n  geom_violin(aes(x=name, y=value, fill=name), show.legend = F, scale = \"width\", trim=F)+\n  coord_flip()+\n  labs(x=NULL, y=NULL)\n\n\n\n\n\n\n\ndf%&gt;%select(positif, négatif, neutre)%&gt;%pivot_longer(everything())%&gt;%\n  ggplot()+\n  geom_boxplot(aes(name, value, fill=name), show.legend = F)+\n  coord_flip()+\n  labs(x=NULL, y=NULL)\n\n\n\n\n\n\n\ndf2&lt;-df%&gt;%select(note, positif, négatif, neutre)%&gt;%\n  pivot_longer(-note, names_to = \"topic\", values_to = \"value\")\n\nggplot(df2,aes(x=note, y=value, group=topic))+\n  geom_bar(position=\"fill\",stat=\"identity\", aes(fill=topic))+\n  scale_fill_brewer(palette=\"Spectral\")+\n  theme_minimal()",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>LLM</span>"
    ]
  },
  {
    "objectID": "chapter8.html#génération-de-texte",
    "href": "chapter8.html#génération-de-texte",
    "title": "8  LLM",
    "section": "8.4 Génération de texte",
    "text": "8.4 Génération de texte\nUn exemple avec gpt2\n\nimport torch\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\n\n# initialize tokenizer and model from pretrained GPT2 model from Huggingface\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=tokenizer.eos_token_id)\n\n# sentence\nsequence = \"What is AI?\"\n# encoding sentence for model to process\ninputs = tokenizer.encode(sequence, return_tensors='pt')\n\n# generating text\noutputs = model.generate(inputs, max_length=200, do_sample=True, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n\n# decoding text\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n# printing output\nprint(text)\n\nWhat is AI?\n\nAI is a form of artificial intelligence that can be used to perform complex tasks. It is often referred to as machine learning or AI, but it is not a scientific term. Artificial intelligence is being used as a way to improve the way we think and interact with the world around us. For example, it has been shown that humans can learn from other humans in a number of ways, such as how they respond to a situation or what they say to each other. In this way, AI can improve our understanding of our world and our relationships with other people, as well as our ability to make decisions about how to live our lives. AI also has the potential to change how we live and what we do in the future. This is especially true when it comes to health, education, healthcare, and the environment, which can all be affected by the use of AI. However, there are still a lot of unanswered questions about what AI will do for us and how it",
    "crumbs": [
      "Niveau 2 : Approfondissements",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>LLM</span>"
    ]
  },
  {
    "objectID": "webscraping.html",
    "href": "webscraping.html",
    "title": "Appendix A — Webscraping",
    "section": "",
    "text": "library(tidyverse)\nlibrary(scales)\nlibrary(quanteda)\nlibrary(quanteda.textstats)\nlibrary(httr)\nlibrary(rvest)\nlibrary(polite)\n\n### Fichier TP_pets_website.csv\n\n##### Scraping sites web\nurl_start&lt;-\"https://fr.trustpilot.com/categories/animals_pets\"\n\ntrustpilot_website&lt;-function(url_start){\n\nsession &lt;- bow(url_start)\nsession$user_agent&lt;-\"Googlebot\"\nmessage(\"Scraping \", url_start)\npage&lt;-nod(session, url_start) %&gt;% \n  scrape(verbose=TRUE)\ni&lt;-page%&gt;%html_elements(\".styles_paginationWrapper__fukEb\")%&gt;%\n  html_element(\"a.button_button__T34Lr:nth-child(5)\")%&gt;%\n  html_text()%&gt;%as.numeric()\nwebsite &lt;- NULL\n\nfor (j in 1:i){\n  url&lt;-paste0(url_start,\"?page=\",j)\n  Sys.sleep(5)\n  session &lt;- bow(url)\n  session$user_agent&lt;-\"Googlebot\"\n  message(\"Scraping \", url)\n  page&lt;-nod(session, url) %&gt;% \n      scrape(verbose=TRUE)\n    \n    company_card &lt;- page %&gt;%\n      html_elements(\"div.styles_wrapper__2JOo2:nth-of-type(n+4)\")\n    \n    website_name &lt;- company_card %&gt;%\n      html_element(\"p.typography_heading-xs__jSwUz\") %&gt;%\n      html_text()\n    \n    nb_avis &lt;- company_card %&gt;%\n      html_element(\"p.typography_body-m__xgxZ_\")%&gt;%\n      html_text()\n    \n    localisation &lt;- company_card%&gt;%\n      html_element(\"span.styles_metadataItem__Qn_Q2\")%&gt;%\n      html_text()\n\n    type &lt;- company_card %&gt;%\n      html_element(\"div.styles_desktop__U5iWw\") %&gt;%\n      html_text()\n    \n    lien &lt;- paste0(\"https://fr.trustpilot.com\", company_card %&gt;%\n               html_element(\"a\")%&gt;%html_attr(\"href\"))\n      \n    \n    website &lt;- rbind(website, data.frame(\n      website_name = website_name, \n      nb_avis = nb_avis,\n      localisation = localisation,\n      type = type,\n      lien = lien\n    ))\n  print(paste(\"page\",j, \"has been scraped\"))\n    \nj&lt;-j+1\n\n}\nreturn(website)\n\n}\n\nwebsite&lt;-trustpilot_website(url_start = url_start)\n\n##### Création du fichier TP_pets_website.csv\nwebsite&lt;-website%&gt;%mutate(note=str_split_i(nb_avis,\"\\\\|\", 1)%&gt;%\n                      str_remove_all(., \"[A-z]\")%&gt;%\n                      str_replace(., \",\", \".\")%&gt;%\n                      as.numeric(),\n                    nb_avis=str_split_i(nb_avis,\"\\\\|\",2)%&gt;%\n                      str_remove_all(., \"[A-z]\")%&gt;%\n                      str_remove_all(., \"[:space:]\")%&gt;%\n                      as.numeric()%&gt;%\n                      replace_na(.,0),\n                    url_start=url_start,\n                    nb_page=0,\n                    cat=str_split(type, \"·\"))%&gt;%\n  unnest_wider(cat, names_sep = \"_\")\n\ndata_scrap&lt;-website%&gt;%filter(nb_avis&gt;10)\nwrite_csv(data_scrap, \"TP_pets_website.csv\")\n\n\n### Fichier TP_pets_reviews.rds\n\n##### Scraping reviews\n\ntrustpilot_reviews&lt;-function(data){\n  Sys.sleep(5)\n  \n  for (j in 1:nrow(data)) {\n    i&lt;-1\n    b&lt;-1\n    \n    \n    while (b!=\"TRUE\") {\n      \n      Sys.sleep(5)\n      \n      \n      b&lt;-http_error(paste0(data$lien[j], \"?languages=all&page=\", i))\n      \n      i&lt;-i+1\n      data$nb_page[j]&lt;-i-2\n      \n    }\n    print(paste0(\"nb_page of \", data$website_name[j], \" has been fetched\"))\n    \n  }\n  \n  i&lt;-1\n  reviews &lt;- NULL\n  # cat(\"\\014\")\n  cat(paste0(\"The script will run on \", sum(data$nb_page), \" pages!\\n\"))\n  Sys.sleep(5)\n  \n  \n  for (j in 1: nrow(data)){\n    for (i in 1:data$nb_page[j]){\n      url&lt;-paste0(data$lien[j],\"?languages=all&page=\",i)\n      Sys.sleep(5)\n      session &lt;- bow(url)\n      session$user_agent&lt;-\"Googlebot\"\n      message(\"Scraping \", url)\n      page&lt;-nod(session, url) %&gt;% \n        scrape(verbose=TRUE)\n      \n      review_card &lt;- page %&gt;%\n        html_elements(\"div.styles_reviewCardInner__EwDq2\")\n      \n      name &lt;- review_card %&gt;%\n        html_element(\"span.typography_heading-xxs__QKBS8.typography_appearance-default__AAY17\") %&gt;%\n        html_text()\n      \n      rating &lt;- review_card %&gt;%\n        html_elements(\"div.star-rating_starRating__4rrcf.star-rating_medium__iN6Ty\") %&gt;%\n        html_element(\"img\")%&gt;%\n        html_attr(\"alt\")%&gt;%\n        str_extract(\"[:digit:]\")\n      \n      published &lt;- review_card%&gt;%\n        html_elements(\".styles_reviewContentwrapper__zH_9M\")%&gt;%\n        html_element(\"p.typography_body-m__xgxZ_\")%&gt;%\n        html_text()%&gt;%\n        str_remove(\"Date de l'expérience: \")\n      \n      verified &lt;- review_card %&gt;%\n        html_element(\".styles_detailsIcon__yqwWi\") %&gt;%\n        html_text()\n      \n      title &lt;- review_card %&gt;%\n        html_element(\"h2\")%&gt;%\n        html_text()\n      \n      content &lt;- review_card%&gt;%\n        html_elements(\".styles_reviewContentwrapper__zH_9M\")%&gt;%\n        html_element(\"p.typography_body-l__KUYFJ\") %&gt;%\n        html_text2()\n      \n      \n      reviews &lt;- rbind(reviews, data.frame(\n        website_name = data$website_name[j],\n        name = name, \n        rating = rating,\n        published = published,\n        verified = verified,\n        title = title, \n        content = content\n      ))\n      \n      i&lt;-i+1\n    }\n    print(paste0(data$website_name[j], \" has been scraped\"))\n    \n    j&lt;-j+1\n  }\n\n  return(reviews)\n}\n\nhak&lt;-trustpilot_reviews(data_scrap)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Webscraping</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Balech, Sophie. 2022. “Une Application Du Modèle ELM ( Elaboration Likelihood Model ) Au Partage d’information Sur Twitter : Étude Du Rôle de La Forme Du Message Et Du Profil de l’émetteur:” Innovations n° 69 (3): 129–61. https://doi.org/10.3917/inno.pr2.0135.\n\n\nBalech, Sophie, and Christophe Benavent. 2022. “Le Rôle Des Dimensions de l’expérience Dans La Satisfaction Client : Une Application Au Cas de l’industrie Hôtelière En Polynésie Française.” In 38ème Congrès Internation de l’AFM, 28–38. Tunis, Tunisie.\n\n\nBarron, Alexander T. J., Jenny Huang, Rebecca L. Spang, and Simon DeDeo. 2018. “Individuals, Institutions, and Innovation in the Debates of the French Revolution.” Proceedings of the National Academy of Sciences 115 (18): 4607–12. https://doi.org/10.1073/pnas.1717729115.\n\n\nBoegershausen, Johannes, Hannes Datta, Abhishek Borah, and Andrew T. Stephen. 2022. “Fields of Gold: Scraping Web Data for Marketing Insights.” Journal of Marketing 86 (5): 1–20. https://doi.org/10.1177/00222429221100750.\n\n\nHartmann, Jochen, Mark Heitmann, Christina Schamp, and Oded Netzer. 2021. “The Power of Brand Selfies.” Journal of Marketing Research 58 (6): 1159–77. https://doi.org/10.1177/00222437211037258.\n\n\nKozlowski, Austin C, Matt Taddy, and James A Evans. 2019. “The Geometry of Culture: Analyzing Meaning Through Word Embeddings.” American Sociological Review 89 (5): 769–981. https://doi.org/10.1177/0003122419877135.\n\n\nKumar, Sunil, Arpan Kumar Kar, and P. Vigneswara Ilavarasan. 2021. “Applications of Text Mining in Services Management: A Systematic Literature Review.” International Journal of Information Management Data Insights 1 (1): 100008. https://doi.org/10.1016/j.jjimei.2021.100008.\n\n\nLefrançois, Alicia, Sophie Balech, and Sophie Changeur. 2022. “Transgression Et Consommation: Revue Intégrative Et Proposition d’un Agenda de Recherche.” In. Le Havre, France.\n\n\nLiu, Angela Xia, Yilin Li, and Sean Xin Xu. 2021. “Assessing the Unacquainted: Inferred Reviewer Personality and Review Helpfulness.” MIS Quarterly 45 (3): 1113–48. https://doi.org/10.25300/MISQ/2021/14375.\n\n\nTirunillai, Seshadri, and Gerard J. Tellis. 2012. “Does Chatter Really Matter? Dynamics of User-Generated Content and Stock Performance.” Marketing Science 31 (2): 198–215. https://doi.org/10.1287/mksc.1110.0682.\n\n\nVannoni, Matia. 2022. “A Political Economy Approach to the Grammar of Institutions: Theory and Methods.” Policy Studies Journal 50 (2): 453–71. https://doi.org/10.1111/psj.12427.",
    "crumbs": [
      "Appendices",
      "References"
    ]
  }
]